# -*- coding: utf-8 -*-
"""GraphMoleculeImportance_NEW_DB_With_Function_Scomposte.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1umlCTwvLXT9aQtBPpcuO_k0QWOJp4t_0
"""

"""Model creation"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout
from torch_geometric.nn import GCNConv, GINConv, global_mean_pool, global_add_pool, global_max_pool
from torch_geometric.loader import DataLoader
from ogb.graphproppred import PygGraphPropPredDataset
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import roc_curve, auc
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib
import numpy as np
import random
import copy
from rdkit import Chem
import pickle
from torch.nn import Linear
from torch_geometric.nn import GATConv, global_max_pool
from torch.nn import Linear
from torch_geometric.nn import GINConv, global_max_pool

class GCN(torch.nn.Module):
    def __init__(self, input_dim, hidden_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.conv4 = GCNConv(hidden_channels, hidden_channels)
        self.lin = Linear(hidden_channels, 2)  # Output will be 2 classes

        # Register hook on the last layer (conv3)
        self.hook_handle = self.conv4.register_forward_hook(self._store_gradients)

    def _store_gradients(self, module, input, output):
        # Store gradients during forward pass
        self.gradients = output


    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)
        x = x.relu()
        x = self.conv4(x, edge_index)
        x = x.relu()
        x = global_max_pool(x, batch)  # Global pooling
        x = self.lin(x)
        return x



class GAT(torch.nn.Module):
    def __init__(self, input_dim, hidden_channels):
        super(GAT, self).__init__()
        self.conv1 = GATConv(input_dim, hidden_channels, heads=1)
        self.conv2 = GATConv(hidden_channels, hidden_channels, heads=1)
        self.conv3 = GATConv(hidden_channels, hidden_channels, heads=1)
        self.conv4 = GATConv(hidden_channels, hidden_channels, heads=1)
        self.lin = Linear(hidden_channels, 2)  # Output will be 2 classes

        # Register hook on the last layer (conv3)
        self.hook_handle = self.conv4.register_forward_hook(self._store_gradients)

    def _store_gradients(self, module, input, output):
        # Store gradients during forward pass
        self.gradients = output

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)
        x = x.relu()
        x = self.conv4(x, edge_index)
        x = x.relu()
        x = global_max_pool(x, batch)  # Global pooling
        x = self.lin(x)
        return x



class GIN(torch.nn.Module):
    def __init__(self, input_dim, hidden_channels):
        super(GIN, self).__init__()
        self.conv1 = GINConv(Linear(input_dim, hidden_channels))
        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))
        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))
        self.conv4 = GINConv(Linear(hidden_channels, hidden_channels))
        self.lin = Linear(hidden_channels, 2)  # Output will be 2 classes

        # Register hook on the last layer (conv4)
        self.hook_handle = self.conv4.register_forward_hook(self._store_gradients)

    def _store_gradients(self, module, input, output):
        # Store gradients during forward pass
        self.gradients = output

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)
        x = x.relu()
        x = self.conv4(x, edge_index)
        x = x.relu()
        x = global_max_pool(x, batch)  # Global pooling
        x = self.lin(x)
        return x

def train(model, optimizer, train_loader):
    model.train()
    correct = 0
    total = 0
    for data in train_loader:
        optimizer.zero_grad()
        out = model(data.x.float(), data.edge_index, data.batch)
        loss = F.cross_entropy(out, data.y.view(-1).long())
        loss.backward()
        optimizer.step()

        # Calculate training accuracy
        pred = out.argmax(dim=1)
        correct += int((pred == data.y.view(-1)).sum())
        total += data.y.size(0)

    accuracy = correct / total
    return accuracy


def validate(model, loader):
    model.eval()
    correct = 0
    total = 0
    all_predictions = []
    all_labels = []
    with torch.no_grad():
        for data in loader:
            pred = model(data.x.float(), data.edge_index, data.batch)
            pred = pred.argmax(dim=1)
            correct += int((pred == data.y.view(-1)).sum())
            total += data.y.size(0)
            all_predictions.extend(pred.tolist())
            all_labels.extend(data.y.view(-1).tolist())

    accuracy = correct / total
    return accuracy, all_predictions, all_labels



def evaluate_roc_auc(model, loader):
    model.eval()
    all_predictions = []
    all_labels = []
    with torch.no_grad():
        for data in loader:
            pred = model(data.x.float(), data.edge_index, data.batch)
            all_predictions.extend(pred[:, 1].cpu().numpy())
            all_labels.extend(data.y.view(-1).cpu().numpy())

    fpr, tpr, _ = roc_curve(all_labels, all_predictions)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()

    print('AUC:', roc_auc)
    return roc_auc

"""train and validation of the model k-folds models and extraction of the models"""

def k_fold_balanced(data, num_classes, hidden_channels, iteration, k_folds, model_choosed):
    all_best_train_acc = []
    all_best_val_acc = []
    all_auc = []
    label_0_indices = []
    label_1_indices = []
    all_best_models = {'model':[], 'val_ACC': [], 'AUC':[]}

    for i in range(len(data)):
      if data[i].y == 0:
        label_0_indices.append(i)
      elif data[i].y == 1:
        label_1_indices.append(i)
    print(len(label_0_indices))
    print(len(label_1_indices))

    for iter in range(iteration):
      all_data = copy.deepcopy(data)
      print('iter:', iter)
      # Define cross-validation settings
      kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

      best_train_accuracies = []
      best_val_accuracies = []
      best_models = []

      # Collect evaluation metrics for all folds
      all_accuracies = []
      all_precisions = []
      all_recalls = []
      all_f1_scores = []

      min_size_dataset = min(len(label_0_indices), len(label_1_indices))

      random.shuffle(label_0_indices)
      random.shuffle(label_1_indices)
      label_0_indices = label_0_indices[:min_size_dataset]
      label_1_indices = label_1_indices[:min_size_dataset]
      print('size 0 data:' , len(label_0_indices))
      print('size 1 data:', len(label_1_indices))

      balanced_indices = label_0_indices + label_1_indices
      print('size total dataset:' , len(balanced_indices))

      balanced_data = [all_data[i] for i in balanced_indices]
      print(len(balanced_data))

      for btd in balanced_data:
       # Apply one-hot encoding for each column
        one_hot_encoded = [F.one_hot(btd.x[:, i].long(), num_classes[i]) for i in range(btd.x.shape[1])]
        one_hot_encoded_tensor = torch.cat(one_hot_encoded, dim=-1)
        btd.x = one_hot_encoded_tensor

      # Training loop with cross-validation
      for fold, (train_idx, val_idx) in enumerate(kf.split(balanced_indices)):
          print(f"Fold [{fold+1}/{k_folds}]")
          # Subset the data for this fold
          train_data = [balanced_data[i] for i in train_idx]
          val_data = [balanced_data[i] for i in val_idx]

          train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
          val_loader = DataLoader(val_data, batch_size=32, shuffle=False)

          total_features = sum(num_classes)
          # Define the model with the correct input dimension
          if model_choosed == 1:
            model = GCN(total_features, hidden_channels=hidden_channels)
          elif model_choosed == 2:
            model = GIN(total_features, hidden_channels=hidden_channels)
          elif model_choosed == 3:
            model = GAT(total_features, hidden_channels=hidden_channels)

          optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

          # Early stopping variables
          best_val_acc = 0.0
          best_train_acc = 0.0
          patience = 25  # Stop after no improvement for 25 epochs
          counter = 0

          # Collect evaluation metrics for this fold
          fold_accuracies = []
          fold_precisions = []
          fold_recalls = []
          fold_f1_scores = []

          # Training loop with validation and early stopping
          for epoch in range(1, 200):  # Train for maximum 100 epochs
              # Train the model and get train accuracy
              train_acc = train(model, optimizer, train_loader)

              # Validate the model
              val_acc, _, _ = validate(model, val_loader)

              print(f'Fold [{fold+1}/{k_folds}] Epoch: {epoch}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

              # Check for improvement in validation accuracy
              if val_acc > best_val_acc:
                  best_val_acc = val_acc
                  best_train_acc = train_acc
                  counter = 0  # Reset counter
              else:
                  counter += 1  # No improvement, increase counter

              # If no improvement for "patience" number of epochs, stop training
              if counter >= patience:
                  print(f'Fold [{fold+1}/{k_folds}] Early stopping at epoch {epoch}')
                  break


          # Store the best train and val accuracies for this fold
          best_train_accuracies.append(best_train_acc)
          best_val_accuracies.append(best_val_acc)
          best_models.append(model)

          # Append metrics for this fold
          all_accuracies.append(fold_accuracies)
          all_precisions.append(fold_precisions)
          all_recalls.append(fold_recalls)
          all_f1_scores.append(fold_f1_scores)

          # Final evaluation on the test set
          test_acc, all_test_predictions, all_test_labels = validate(model, val_loader)

          print(f'Final Test Accuracy: {test_acc:.4f}')

          # Convert predictions and labels to numpy arrays
          all_test_predictions = np.array(all_test_predictions)
          all_test_labels = np.array(all_test_labels)

          # Calculate True Positives (TP), False Positives (FP),
          # True Negatives (TN), False Negatives (FN)
          TP = np.sum((all_test_predictions == 1) & (all_test_labels == 1))
          FP = np.sum((all_test_predictions == 1) & (all_test_labels == 0))
          TN = np.sum((all_test_predictions == 0) & (all_test_labels == 0))
          FN = np.sum((all_test_predictions == 0) & (all_test_labels == 1))

          print("True Positives (TP):", TP)
          print("False Positives (FP):", FP)
          print("True Negatives (TN):", TN)
          print("False Negatives (FN):", FN)

          # Calculate accuracy, precision, recall, and F1-score
          accuracy = (TP + TN) / (TP + FP + TN + FN)
          precision = TP / (TP + FP)
          recall = TP / (TP + FN)
          f1_score = 2 * (precision * recall) / (precision + recall)

          print("Accuracy:", accuracy)
          print("Precision:", precision)
          print("Recall:", recall)
          print("F1-Score:", f1_score)

          # Append metrics for this epoch
          fold_accuracies.append(val_acc)
          fold_precisions.append(precision)
          fold_recalls.append(recall)
          fold_f1_scores.append(f1_score)

          all_best_models['model'].append(model)
          all_best_models['val_ACC'].append(max(best_val_accuracies))
          all_best_models['AUC'].append(evaluate_roc_auc(model, val_loader))

      # Calculate median of best train and val accuracies
      median_best_train_acc = np.median(best_train_accuracies)
      median_best_val_acc = np.median(best_val_accuracies)
      all_best_train_acc.append(median_best_train_acc)
      all_best_val_acc.append(median_best_val_acc)

      print("\nMedian of Best Train Accuracies:", median_best_train_acc)
      print("Median of Best Validation Accuracies:", median_best_val_acc)

      # Compare median validation accuracies to choose the better model
      if median_best_val_acc == max(median_best_train_acc, median_best_val_acc):
          best_model_idx = best_val_accuracies.index(max(best_val_accuracies))
      else:
          best_model_idx = best_train_accuracies.index(max(best_train_accuracies))

      best_model = best_models[best_model_idx]

      # Calculate median of all metrics across all folds
      median_accuracy = np.median(np.concatenate(all_accuracies))
      median_precision = np.median(np.concatenate(all_precisions))
      median_recall = np.median(np.concatenate(all_recalls))
      median_f1_score = np.median(np.concatenate(all_f1_scores))

      print("\nMedian of All Accuracies:", median_accuracy)
      print("Median of All Precisions:", median_precision)
      print("Median of All Recalls:", median_recall)
      print("Median of All F1-Scores:", median_f1_score)
      print('\n')

      with open('/content/drive/MyDrive/log_file', 'a') as file:
        file.write("\nBlock:" + str(iter + 1) + '/' + str(5) + '\n')
        file.write("Median of All Accuracies:" + str(median_accuracy) +'\n')
        file.write("Median of All Precisions:" + str(median_precision) + '\n')
        file.write("Median of All Recalls:" + str(median_recall) + '\n')
        file.write("Median of All F1-Scores:" + str(median_f1_score) + '\n')

    return all_best_models

def k_fold_no_balanced(data, num_classes, hidden_channels, iteration, k_folds, model_choosed):
    all_best_train_acc = []
    all_best_val_acc = []
    all_auc = []
    label_0_indices = []
    label_1_indices = []
    all_best_models = {'model':[], 'val_ACC': [], 'AUC':[]}

    for i in range(len(data)):
      if data[i].y == 0:
        label_0_indices.append(i)
      elif data[i].y == 1:
        label_1_indices.append(i)
    print(len(label_0_indices))
    print(len(label_1_indices))

    for iter in range(iteration):
      all_data = copy.deepcopy(data)
      print('iter:', iter)
      # Define cross-validation settings
      kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

      best_train_accuracies = []
      best_val_accuracies = []
      best_models = []

      # Collect evaluation metrics for all folds
      all_accuracies = []
      all_precisions = []
      all_recalls = []
      all_f1_scores = []

      min_size_dataset = min(len(label_0_indices), len(label_1_indices))

      random.shuffle(label_0_indices)
      random.shuffle(label_1_indices)
      label_0_indices = label_0_indices[:]
      label_1_indices = label_1_indices[:]
      print('size 0 data:' , len(label_0_indices))
      print('size 1 data:', len(label_1_indices))

      no_balanced_indices = label_0_indices + label_1_indices
      print('size total dataset:' , len(no_balanced_indices))

      no_balanced_data = [all_data[i] for i in no_balanced_indices]
      print(len(no_balanced_data))

      for btd in no_balanced_data:
       # Apply one-hot encoding for each column
        one_hot_encoded = [F.one_hot(btd.x[:, i].long(), num_classes[i]) for i in range(btd.x.shape[1])]
        one_hot_encoded_tensor = torch.cat(one_hot_encoded, dim=-1)
        btd.x = one_hot_encoded_tensor

      # Training loop with cross-validation
      for fold, (train_idx, val_idx) in enumerate(kf.split(no_balanced_indices)):
          print(f"Fold [{fold+1}/{k_folds}]")
          # Subset the data for this fold
          train_data = [no_balanced_data[i] for i in train_idx]
          val_data = [no_balanced_data[i] for i in val_idx]

          train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
          val_loader = DataLoader(val_data, batch_size=32, shuffle=False)

          total_features = sum(num_classes)
          # Define the model with the correct input dimension
          if model_choosed == 1:
            model = GCN(total_features, hidden_channels=hidden_channels)
          elif model_choosed == 2:
            model = GIN(total_features, hidden_channels=hidden_channels)
          elif model_choosed == 3:
            model = GAT(total_features, hidden_channels=hidden_channels)

          optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

          # Early stopping variables
          best_val_acc = 0.0
          best_train_acc = 0.0
          patience = 25  # Stop after no improvement for 25 epochs
          counter = 0

          # Collect evaluation metrics for this fold
          fold_accuracies = []
          fold_precisions = []
          fold_recalls = []
          fold_f1_scores = []

          # Training loop with validation and early stopping
          for epoch in range(1, 200):  # Train for maximum 100 epochs
              # Train the model and get train accuracy
              train_acc = train(model, optimizer, train_loader)

              # Validate the model
              val_acc, _, _ = validate(model, val_loader)

              print(f'Fold [{fold+1}/{k_folds}] Epoch: {epoch}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

              # Check for improvement in validation accuracy
              if val_acc > best_val_acc:
                  best_val_acc = val_acc
                  best_train_acc = train_acc
                  counter = 0  # Reset counter
              else:
                  counter += 1  # No improvement, increase counter

              # If no improvement for "patience" number of epochs, stop training
              if counter >= patience:
                  print(f'Fold [{fold+1}/{k_folds}] Early stopping at epoch {epoch}')
                  break


          # Store the best train and val accuracies for this fold
          best_train_accuracies.append(best_train_acc)
          best_val_accuracies.append(best_val_acc)
          best_models.append(model)

          # Append metrics for this fold
          all_accuracies.append(fold_accuracies)
          all_precisions.append(fold_precisions)
          all_recalls.append(fold_recalls)
          all_f1_scores.append(fold_f1_scores)

          # Final evaluation on the test set
          test_acc, all_test_predictions, all_test_labels = validate(model, val_loader)

          print(f'Final Test Accuracy: {test_acc:.4f}')

          # Convert predictions and labels to numpy arrays
          all_test_predictions = np.array(all_test_predictions)
          all_test_labels = np.array(all_test_labels)

          # Calculate True Positives (TP), False Positives (FP),
          # True Negatives (TN), False Negatives (FN)
          TP = np.sum((all_test_predictions == 1) & (all_test_labels == 1))
          FP = np.sum((all_test_predictions == 1) & (all_test_labels == 0))
          TN = np.sum((all_test_predictions == 0) & (all_test_labels == 0))
          FN = np.sum((all_test_predictions == 0) & (all_test_labels == 1))

          print("True Positives (TP):", TP)
          print("False Positives (FP):", FP)
          print("True Negatives (TN):", TN)
          print("False Negatives (FN):", FN)

          # Calculate accuracy, precision, recall, and F1-score
          accuracy = (TP + TN) / (TP + FP + TN + FN)
          precision = TP / (TP + FP)
          recall = TP / (TP + FN)
          f1_score = 2 * (precision * recall) / (precision + recall)

          print("Accuracy:", accuracy)
          print("Precision:", precision)
          print("Recall:", recall)
          print("F1-Score:", f1_score)

          # Append metrics for this epoch
          fold_accuracies.append(val_acc)
          fold_precisions.append(precision)
          fold_recalls.append(recall)
          fold_f1_scores.append(f1_score)

          all_best_models['model'].append(model)
          all_best_models['val_ACC'].append(max(best_val_accuracies))
          all_best_models['AUC'].append(evaluate_roc_auc(model, val_loader))

      # Calculate median of best train and val accuracies
      median_best_train_acc = np.median(best_train_accuracies)
      median_best_val_acc = np.median(best_val_accuracies)
      all_best_train_acc.append(median_best_train_acc)
      all_best_val_acc.append(median_best_val_acc)

      print("\nMedian of Best Train Accuracies:", median_best_train_acc)
      print("Median of Best Validation Accuracies:", median_best_val_acc)

      # Compare median validation accuracies to choose the better model
      if median_best_val_acc == max(median_best_train_acc, median_best_val_acc):
          best_model_idx = best_val_accuracies.index(max(best_val_accuracies))
      else:
          best_model_idx = best_train_accuracies.index(max(best_train_accuracies))

      best_model = best_models[best_model_idx]

      # Calculate median of all metrics across all folds
      median_accuracy = np.median(np.concatenate(all_accuracies))
      median_precision = np.median(np.concatenate(all_precisions))
      median_recall = np.median(np.concatenate(all_recalls))
      median_f1_score = np.median(np.concatenate(all_f1_scores))

      print("\nMedian of All Accuracies:", median_accuracy)
      print("Median of All Precisions:", median_precision)
      print("Median of All Recalls:", median_recall)
      print("Median of All F1-Scores:", median_f1_score)
      print('\n')

      with open('/content/drive/MyDrive/log_file', 'a') as file:
        file.write("\nBlock:" + str(iter + 1) + '/' + str(iteration) + '\n')
        file.write("Median of All Accuracies:" + str(median_accuracy) +'\n')
        file.write("Median of All Precisions:" + str(median_precision) + '\n')
        file.write("Median of All Recalls:" + str(median_recall) + '\n')
        file.write("Median of All F1-Scores:" + str(median_f1_score) + '\n')

    return all_best_models

def no_balanced_scatterfold(data, num_classes, hidden_channels,model_choosed):
    all_best_train_acc = []
    all_best_val_acc = []
    all_auc = []
    all_best_models = {'model':[], 'val_ACC': [], 'AUC':[]}

    for iter in range(1):
        all_data = copy.deepcopy(data)
        print('iter:', iter)
        print(all_data[1].x.shape[1])
        balanced_data = [all_data[i] for i in range(len(all_data))]
        # One-hot encode the features
        for btd in balanced_data:
            one_hot_encoded = [F.one_hot(btd.x[:, i].long(), num_classes[i]) for i in range(btd.x.shape[1])]
            one_hot_encoded_tensor = torch.cat(one_hot_encoded, dim=-1)
            btd.x = one_hot_encoded_tensor

        # Split the data into training (80%), validation (20%), and test (20%) sets
        train_val_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42)
        train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2

        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

        total_features = sum(num_classes)
        if model_choosed == 1:
            model = GCN(total_features, hidden_channels=hidden_channels)
        elif model_choosed == 2:
            model = GIN(total_features, hidden_channels=hidden_channels)
        elif model_choosed == 3:
            model = GAT(total_features, hidden_channels=hidden_channels)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        best_val_acc = 0.0
        best_train_acc = 0.0
        patience = 25
        counter = 0

        for epoch in range(1, 200):
            train_acc = train(model, optimizer, train_loader)
            val_acc, _, _ = validate(model, val_loader)

            print(f'Epoch: {epoch}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_train_acc = train_acc
                counter = 0
            else:
                counter += 1

            if counter >= patience:
                print(f'Early stopping at epoch {epoch}')
                break

        test_acc, all_test_predictions, all_test_labels = validate(model, test_loader)

        print(f'Final Test Accuracy: {test_acc:.4f}')

        all_test_predictions = np.array(all_test_predictions)
        all_test_labels = np.array(all_test_labels)

        TP = np.sum((all_test_predictions == 1) & (all_test_labels == 1))
        FP = np.sum((all_test_predictions == 1) & (all_test_labels == 0))
        TN = np.sum((all_test_predictions == 0) & (all_test_labels == 0))
        FN = np.sum((all_test_predictions == 0) & (all_test_labels == 1))

        accuracy = (TP + TN) / (TP + FP + TN + FN)
        precision = TP / (TP + FP)
        recall = TP / (TP + FN)
        f1_score = 2 * (precision * recall) / (precision + recall)

        print("Accuracy:", accuracy)
        print("Precision:", precision)
        print("Recall:", recall)
        print("F1-Score:", f1_score)

        all_best_models['model'].append(model)
        all_best_models['val_ACC'].append(best_val_acc)
        all_best_models['AUC'].append(evaluate_roc_auc(model, val_loader))

        all_best_train_acc.append(best_train_acc)
        all_best_val_acc.append(best_val_acc)

        with open('/content/drive/MyDrive/log_file.txt', 'a') as file:
            file.write("\nBlock:" + str(iter + 1) + '/' + str(1) + '\n')
            file.write("Test Accuracy:" + str(test_acc) + '\n')
            file.write("Accuracy:" + str(accuracy) + '\n')
            file.write("Precision:" + str(precision) + '\n')
            file.write("Recall:" + str(recall) + '\n')
            file.write("F1-Score:" + str(f1_score) + '\n')

    return all_best_models

def balanced_scatterfold(data, num_classes, hidden_channels,model_choosed):
    all_best_train_acc = []
    all_best_val_acc = []
    all_auc = []
    all_best_models = {'model':[], 'val_ACC': [], 'AUC':[]}
    label_0_indices = []
    label_1_indices = []
    for i in range(len(data)):
      if data[i].y == 0:
        label_0_indices.append(i)
      elif data[i].y == 1:
        label_1_indices.append(i)

    for iter in range(1):
      all_data = copy.deepcopy(data)

      print('iter:', iter)
      print(all_data[1].x.shape[1])
      min_size_dataset = min(len(label_0_indices), len(label_1_indices))

      random.shuffle(label_0_indices)
      random.shuffle(label_1_indices)
      label_0_indices = label_0_indices[:min_size_dataset]
      label_1_indices = label_1_indices[:min_size_dataset]
      print('size 0 data:' , len(label_0_indices))
      print('size 1 data:', len(label_1_indices))

      balanced_indices = label_0_indices + label_1_indices
      print('size total dataset:' , len(balanced_indices))

      balanced_data = [all_data[i] for i in balanced_indices]
      print(len(balanced_data))
        #balanced_data = [all_data[i] for i in range(len(all_data))]
        # One-hot encode the features
      for btd in balanced_data:
            one_hot_encoded = [F.one_hot(btd.x[:, i].long(), num_classes[i]) for i in range(btd.x.shape[1])]
            one_hot_encoded_tensor = torch.cat(one_hot_encoded, dim=-1)
            btd.x = one_hot_encoded_tensor

      # Split the data into training (80%), validation (20%), and test (20%) sets
      train_val_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42)
      train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2

      train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
      val_loader = DataLoader(val_data, batch_size=32, shuffle=False)
      test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

      total_features = sum(num_classes)
      if model_choosed == 1:
        model = GCN(total_features, hidden_channels=hidden_channels)
      elif model_choosed == 2:
        model = GIN(total_features, hidden_channels=hidden_channels)
      elif model_choosed == 3:
            model = GAT(total_features, hidden_channels=hidden_channels)
      optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

      best_val_acc = 0.0
      best_train_acc = 0.0
      patience = 25
      counter = 0

      for epoch in range(1, 200):
        train_acc = train(model, optimizer, train_loader)
        val_acc, _, _ = validate(model, val_loader)

        print(f'Epoch: {epoch}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')

        if val_acc > best_val_acc:
          best_val_acc = val_acc
          best_train_acc = train_acc
          counter = 0
        else:
          counter += 1

        if counter >= patience:
          print(f'Early stopping at epoch {epoch}')
          break

      test_acc, all_test_predictions, all_test_labels = validate(model, test_loader)

      print(f'Final Test Accuracy: {test_acc:.4f}')

      all_test_predictions = np.array(all_test_predictions)
      all_test_labels = np.array(all_test_labels)

      TP = np.sum((all_test_predictions == 1) & (all_test_labels == 1))
      FP = np.sum((all_test_predictions == 1) & (all_test_labels == 0))
      TN = np.sum((all_test_predictions == 0) & (all_test_labels == 0))
      FN = np.sum((all_test_predictions == 0) & (all_test_labels == 1))

      accuracy = (TP + TN) / (TP + FP + TN + FN)
      precision = TP / (TP + FP)
      recall = TP / (TP + FN)
      f1_score = 2 * (precision * recall) / (precision + recall)

      print("Accuracy:", accuracy)
      print("Precision:", precision)
      print("Recall:", recall)
      print("F1-Score:", f1_score)

      all_best_models['model'].append(model)
      all_best_models['val_ACC'].append(best_val_acc)
      all_best_models['AUC'].append(evaluate_roc_auc(model, val_loader))

      all_best_train_acc.append(best_train_acc)
      all_best_val_acc.append(best_val_acc)

      with open('/content/drive/MyDrive/log_file.txt', 'a') as file:
          file.write("\nBlock:" + str(iter + 1) + '/' + str(1) + '\n')
          file.write("Test Accuracy:" + str(test_acc) + '\n')
          file.write("Accuracy:" + str(accuracy) + '\n')
          file.write("Precision:" + str(precision) + '\n')
          file.write("Recall:" + str(recall) + '\n')
          file.write("F1-Score:" + str(f1_score) + '\n')

    return all_best_models

"""Evaluation of the models with extraction of the best model"""


def extract_best_model(all_best_models):
  max_auc_index = all_best_models['AUC'].index(max(all_best_models['AUC']))
  best_model = all_best_models['model'][max_auc_index]
  return best_model


def extract_metrics(all_best_models):
  max_auc = max(all_best_models['AUC'])
  mean_auc = np.mean(all_best_models['AUC'])
  std_auc = np.std(all_best_models['AUC'])

  with open('/content/drive/MyDrive/log_file.txt', 'a') as file:
    file.write("\nMax AUC Value:" + str(max_auc) + '\n')
    file.write("Mean AUC Value:" + str(mean_auc) + '\n')
    file.write("Standard Deviation AUC:" + str(std_auc) + '\n')

  return max_auc, mean_auc, std_auc


"""graph plot with node importance"""

def plot_graph_feature_importance(data,num_classes,best_model):
  tp = Chem.GetPeriodicTable()
  cp_data = copy.deepcopy(data)
  best_model.eval()
  elements = {}
  for i in range(data.num_nodes):
    first_element_int = int(data.x[i][0].item())
    elements[i] = tp.GetElementSymbol(first_element_int)

  one_hot_encoded = [F.one_hot(cp_data.x[:, j].long(), num_classes[j]) for j in range(cp_data.x.shape[1])]
  one_hot_encoded_tensor = torch.cat(one_hot_encoded, dim=-1)
  cp_data.x = one_hot_encoded_tensor

  # Forward pass to get the model output
  output = best_model(cp_data.x.float(), cp_data.edge_index, cp_data.batch)

  # Compute node importance scores
  node_importance = best_model.gradients.norm(dim=1)

  # Convert PyTorch tensor to numpy array for visualization
  node_importance_np = node_importance.detach().numpy()

  # Create a NetworkX graph from edge_index
  G = nx.Graph()
  G.add_edges_from(cp_data.edge_index.T.numpy())

  # Create a mapping from node indices to their importance scores
  node_importance_dict = {i: importance for i, importance in enumerate(node_importance_np)}

  # Define colors based on node importance (red scale)
  node_colors = [node_importance_dict[i] for i in range(len(G.nodes))]
  cmap = matplotlib.colormaps['Reds']

  # Determine node size based on importance (larger for higher importance)
  max_importance = max(node_importance_np)
  node_sizes = [2000 * (importance / max_importance) for importance in node_importance_np]

  # Draw the graph with node labels and adjusted node sizes
  plt.figure(figsize=(10, 7))
  pos = nx.spring_layout(G, seed=42)  # Positions for all nodes
  nodes = nx.draw_networkx_nodes(G, pos, node_color=node_colors, cmap=cmap, node_size=node_sizes)
  edges = nx.draw_networkx_edges(G, pos, edgelist=G.edges, alpha=0.5)
  node_labels = nx.draw_networkx_labels(G, pos, labels=elements, font_size=10, font_color='white')
  for _, text in node_labels.items():
      text.set_bbox(dict(facecolor='none', edgecolor='none'))  # No box around labels
  plt.colorbar(nodes, label='Node Importance', orientation='vertical')
  plt.title('Graph with Node Importance')
  plt.axis('off')
  plt.show()

def load_pickle_file(pt_file_path):
  with open(pt_file_path, 'rb') as f:
      data = pickle.load(f)
      
