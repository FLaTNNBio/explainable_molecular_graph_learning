{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtVle8RHNqON"
      },
      "source": [
        "# Node representation learning with Node2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tLTzxU_DNzLi",
        "outputId": "a53bc58e-f753-460c-dee1-9ebcef63406b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pysmiles\n",
            "  Downloading pysmiles-1.1.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pbr (from pysmiles)\n",
            "  Downloading pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pysmiles) (3.3)\n",
            "Downloading pysmiles-1.1.2-py2.py3-none-any.whl (22 kB)\n",
            "Downloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pbr, pysmiles\n",
            "Successfully installed pbr-6.0.0 pysmiles-1.1.2\n",
            "Collecting git+https://github.com/VenkateshwaranB/stellargraph.git\n",
            "  Cloning https://github.com/VenkateshwaranB/stellargraph.git to /tmp/pip-req-build-staxuko7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/VenkateshwaranB/stellargraph.git /tmp/pip-req-build-staxuko7\n",
            "  Resolved https://github.com/VenkateshwaranB/stellargraph.git to commit efa1f847109a4ba490e7a5105646a20ee09a3243\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stellargraph==1.3.0b0) (2.17.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from stellargraph==1.3.0b0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from stellargraph==1.3.0b0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from stellargraph==1.3.0b0) (3.3)\n",
            "Requirement already satisfied: scikit_learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from stellargraph==1.3.0b0) (1.3.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.10/dist-packages (from stellargraph==1.3.0b0) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from stellargraph==1.3.0b0) (2.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2->stellargraph==1.3.0b0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->stellargraph==1.3.0b0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->stellargraph==1.3.0b0) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=0.20->stellargraph==1.3.0b0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>=0.20->stellargraph==1.3.0b0) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.1.0->stellargraph==1.3.0b0) (0.1.2)\n",
            "Building wheels for collected packages: stellargraph\n",
            "  Building wheel for stellargraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stellargraph: filename=stellargraph-1.3.0b0-py3-none-any.whl size=431846 sha256=aade724bbd041efcf251389a6742806f1d39d1cb1c746b8ef5e322ace3abb42d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nyudk4uh/wheels/f3/06/0f/089f69af27d308a1830638f855b6c5755311d8ffc451de9980\n",
            "Successfully built stellargraph\n",
            "Installing collected packages: stellargraph\n",
            "Successfully installed stellargraph-1.3.0b0\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Downloading rdkit-2024.3.3-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.3\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              },
              "id": "16fc3d3beda745b8a2e2b9eb596813ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet-mkl==1.6.0\n",
            "  Downloading mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting numpy==1.23.1\n",
            "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (2.32.3)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet-mkl==1.6.0)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.7.4)\n",
            "Downloading mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl (76.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: numpy, graphviz, mxnet-mkl\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.3\n",
            "    Uninstalling graphviz-0.20.3:\n",
            "      Successfully uninstalled graphviz-0.20.3\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install StellarGraph if running on Google Colab\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  !pip install pysmiles\n",
        "  !pip install git+https://github.com/VenkateshwaranB/stellargraph.git\n",
        "  !pip install rdkit\n",
        "  !pip install torch_geometric\n",
        "  !pip install datasets\n",
        "  !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wcuv1AHaNqOR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from stellargraph import datasets\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fv8jU7ilW-6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b319f752-7aa9-4638-eb3e-30214c6e7e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.1)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.3.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (71.0.4)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.5.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, littleutils, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ogb\n",
            "Successfully installed littleutils-0.2.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a2IcVbx3jwCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23044aa0-6632-40b4-cab6-2bb4036c1393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C1O3XD3xibZ"
      },
      "source": [
        "Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JjfW3Bz-qAr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ae83f6-f5f6-4d16-bad6-d104edbced2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/hiv.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41127/41127 [00:00<00:00, 75252.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41127/41127 [00:01<00:00, 22079.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "\n",
        "#qua sta anche clintox e altri vedere bene documntazione però il codice funziona anche con uun proprio dataset, però il clintox ha la y diversa dalla nostra quindi in molti casi conviene avere a che fare con db propri, dove settiamo noi la y per bene e usiamo lo script SMILEs-MOL-OGB\n",
        "all_data = dataset[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4sCu2-XWLin",
        "outputId": "a557eabe-8176-4f93-c901-844f0e6af54b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Data(x=[23, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=23), Data(x=[33, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=33), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[27, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=27), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[8, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=8), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[36, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=36), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[31, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=31), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[25, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=25), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[39, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=39), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[23, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=23), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[25, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=25), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[34, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=34), Data(x=[32, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=32), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[24, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=24), Data(x=[23, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=23), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[36, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=36), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[24, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=24), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[26, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=26), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[43, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=43), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[42, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=42), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[26, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=26), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[37, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=37), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[32, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=32), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[14, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=14), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[2, 9], edge_index=[2, 2], edge_attr=[2, 3], y=[1, 1], num_nodes=2), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[1, 9], edge_index=[2, 0], edge_attr=[0, 3], y=[1, 1], num_nodes=1), Data(x=[2, 9], edge_index=[2, 2], edge_attr=[2, 3], y=[1, 1], num_nodes=2), Data(x=[13, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=13), Data(x=[7, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=7), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[12, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=12), Data(x=[13, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=13), Data(x=[12, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=12), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[26, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=26), Data(x=[20, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=20), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[8, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=8), Data(x=[6, 9], edge_index=[2, 10], edge_attr=[10, 3], y=[1, 1], num_nodes=6), Data(x=[15, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=15), Data(x=[6, 9], edge_index=[2, 10], edge_attr=[10, 3], y=[1, 1], num_nodes=6), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[7, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=7), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[6, 9], edge_index=[2, 10], edge_attr=[10, 3], y=[1, 1], num_nodes=6), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[13, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=13), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[8, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=8), Data(x=[12, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=12), Data(x=[13, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=13), Data(x=[13, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=13), Data(x=[12, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=12), Data(x=[12, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=12), Data(x=[14, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=14), Data(x=[8, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=8), Data(x=[9, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=9), Data(x=[7, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=7), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[40, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=40), Data(x=[9, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=9), Data(x=[13, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=13), Data(x=[20, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=20), Data(x=[27, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=27), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[7, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=7), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[7, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=7), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[12, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=12), Data(x=[19, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=19), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[8, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=8), Data(x=[14, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=14), Data(x=[18, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=18), Data(x=[20, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=20), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[8, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=8), Data(x=[13, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=13), Data(x=[6, 9], edge_index=[2, 10], edge_attr=[10, 3], y=[1, 1], num_nodes=6), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[9, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=9), Data(x=[15, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=15), Data(x=[14, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=14), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[39, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=39), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[14, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=14), Data(x=[9, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=9), Data(x=[16, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=16), Data(x=[9, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=9), Data(x=[15, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=15), Data(x=[18, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=18), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[19, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=19), Data(x=[15, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=15), Data(x=[20, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=20), Data(x=[46, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=46), Data(x=[50, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=50), Data(x=[23, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=23), Data(x=[17, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=17), Data(x=[19, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=19), Data(x=[40, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=40), Data(x=[15, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=15), Data(x=[16, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=16), Data(x=[3, 9], edge_index=[2, 4], edge_attr=[4, 3], y=[1, 1], num_nodes=3), Data(x=[19, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=19), Data(x=[24, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=24), Data(x=[15, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=15), Data(x=[2, 9], edge_index=[2, 2], edge_attr=[2, 3], y=[1, 1], num_nodes=2), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[3, 9], edge_index=[2, 4], edge_attr=[4, 3], y=[1, 1], num_nodes=3), Data(x=[3, 9], edge_index=[2, 4], edge_attr=[4, 3], y=[1, 1], num_nodes=3), Data(x=[3, 9], edge_index=[2, 4], edge_attr=[4, 3], y=[1, 1], num_nodes=3), Data(x=[9, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=9), Data(x=[29, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=29), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[8, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=8), Data(x=[11, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=11), Data(x=[10, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=10), Data(x=[35, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=35), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[2, 9], edge_index=[2, 2], edge_attr=[2, 3], y=[1, 1], num_nodes=2), Data(x=[3, 9], edge_index=[2, 4], edge_attr=[4, 3], y=[1, 1], num_nodes=3), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[4, 9], edge_index=[2, 6], edge_attr=[6, 3], y=[1, 1], num_nodes=4), Data(x=[3, 9], edge_index=[2, 4], edge_attr=[4, 3], y=[1, 1], num_nodes=3), Data(x=[2, 9], edge_index=[2, 2], edge_attr=[2, 3], y=[1, 1], num_nodes=2), Data(x=[5, 9], edge_index=[2, 8], edge_attr=[8, 3], y=[1, 1], num_nodes=5), Data(x=[3, 9], edge_index=[2, 4], edge_attr=[4, 3], y=[1, 1], num_nodes=3), Data(x=[35, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=35), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[35, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=35), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[36, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=36), Data(x=[36, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=36), Data(x=[35, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=35), Data(x=[36, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=36), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[35, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=35), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[26, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=26), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[24, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=24), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[26, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=26), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[8, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=8), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[23, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=23), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[39, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=39), Data(x=[39, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=39), Data(x=[40, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=40), Data(x=[38, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=38), Data(x=[39, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=39), Data(x=[36, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=36), Data(x=[51, 9], edge_index=[2, 110], edge_attr=[110, 3], y=[1, 1], num_nodes=51), Data(x=[32, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=32), Data(x=[31, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=31), Data(x=[38, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=38), Data(x=[31, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=31), Data(x=[30, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=30), Data(x=[35, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=35), Data(x=[31, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=31), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=22), Data(x=[38, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=38), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[26, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=26), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[41, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=41), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[7, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=7), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[26, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=26), Data(x=[24, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=24), Data(x=[24, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=24), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[8, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=8), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[37, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=37), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[22, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=22), Data(x=[23, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=23), Data(x=[21, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=22), Data(x=[24, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=24), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[42, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=42), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[11, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=11), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[35, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=35), Data(x=[34, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=34), Data(x=[34, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=34), Data(x=[34, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=34), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[18, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=18), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[38, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=38), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[35, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=35), Data(x=[34, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=34), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[65, 9], edge_index=[2, 136], edge_attr=[136, 3], y=[1, 1], num_nodes=65), Data(x=[68, 9], edge_index=[2, 142], edge_attr=[142, 3], y=[1, 1], num_nodes=68), Data(x=[68, 9], edge_index=[2, 142], edge_attr=[142, 3], y=[1, 1], num_nodes=68), Data(x=[73, 9], edge_index=[2, 152], edge_attr=[152, 3], y=[1, 1], num_nodes=73), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[11, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=11), Data(x=[10, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=10), Data(x=[11, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=11), Data(x=[10, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=10), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[40, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=40), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[24, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=24), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[38, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=38), Data(x=[41, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=41), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[27, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=27), Data(x=[25, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=25), Data(x=[25, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=25), Data(x=[27, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=27), Data(x=[27, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=27), Data(x=[44, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=44), Data(x=[51, 9], edge_index=[2, 106], edge_attr=[106, 3], y=[1, 1], num_nodes=51), Data(x=[52, 9], edge_index=[2, 108], edge_attr=[108, 3], y=[1, 1], num_nodes=52), Data(x=[55, 9], edge_index=[2, 114], edge_attr=[114, 3], y=[1, 1], num_nodes=55), Data(x=[31, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=31), Data(x=[30, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=30), Data(x=[48, 9], edge_index=[2, 104], edge_attr=[104, 3], y=[1, 1], num_nodes=48), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[34, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=34), Data(x=[34, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=34), Data(x=[31, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=31), Data(x=[41, 9], edge_index=[2, 92], edge_attr=[92, 3], y=[1, 1], num_nodes=41), Data(x=[40, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=40), Data(x=[43, 9], edge_index=[2, 96], edge_attr=[96, 3], y=[1, 1], num_nodes=43), Data(x=[91, 9], edge_index=[2, 192], edge_attr=[192, 3], y=[1, 1], num_nodes=91), Data(x=[91, 9], edge_index=[2, 192], edge_attr=[192, 3], y=[1, 1], num_nodes=91), Data(x=[87, 9], edge_index=[2, 184], edge_attr=[184, 3], y=[1, 1], num_nodes=87), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[19, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=19), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[101, 9], edge_index=[2, 212], edge_attr=[212, 3], y=[1, 1], num_nodes=101), Data(x=[102, 9], edge_index=[2, 214], edge_attr=[214, 3], y=[1, 1], num_nodes=102), Data(x=[112, 9], edge_index=[2, 234], edge_attr=[234, 3], y=[1, 1], num_nodes=112), Data(x=[55, 9], edge_index=[2, 124], edge_attr=[124, 3], y=[1, 1], num_nodes=55), Data(x=[54, 9], edge_index=[2, 122], edge_attr=[122, 3], y=[1, 1], num_nodes=54), Data(x=[57, 9], edge_index=[2, 128], edge_attr=[128, 3], y=[1, 1], num_nodes=57), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[29, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=29), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[27, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=27), Data(x=[74, 9], edge_index=[2, 154], edge_attr=[154, 3], y=[1, 1], num_nodes=74), Data(x=[75, 9], edge_index=[2, 156], edge_attr=[156, 3], y=[1, 1], num_nodes=75), Data(x=[73, 9], edge_index=[2, 152], edge_attr=[152, 3], y=[1, 1], num_nodes=73), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[35, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=35), Data(x=[35, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=35), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[11, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=11), Data(x=[13, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=13), Data(x=[13, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=13), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[10, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=10), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[32, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=32), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[40, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=40), Data(x=[40, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=40), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[42, 9], edge_index=[2, 94], edge_attr=[94, 3], y=[1, 1], num_nodes=42), Data(x=[44, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=44), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[33, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=33), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[36, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=36), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[37, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=37), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[25, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=25), Data(x=[30, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=30), Data(x=[39, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=39), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[69, 9], edge_index=[2, 142], edge_attr=[142, 3], y=[1, 1], num_nodes=69), Data(x=[67, 9], edge_index=[2, 138], edge_attr=[138, 3], y=[1, 1], num_nodes=67), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[8, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=8), Data(x=[60, 9], edge_index=[2, 136], edge_attr=[136, 3], y=[1, 1], num_nodes=60), Data(x=[65, 9], edge_index=[2, 144], edge_attr=[144, 3], y=[1, 1], num_nodes=65), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[17, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=16), Data(x=[58, 9], edge_index=[2, 126], edge_attr=[126, 3], y=[1, 1], num_nodes=58), Data(x=[60, 9], edge_index=[2, 130], edge_attr=[130, 3], y=[1, 1], num_nodes=60), Data(x=[37, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=37), Data(x=[42, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=42), Data(x=[39, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=39), Data(x=[39, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=39), Data(x=[33, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=33), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[35, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=35), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[29, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=29), Data(x=[29, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=29), Data(x=[36, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=36), Data(x=[36, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=36), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[24, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=24), Data(x=[35, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=35), Data(x=[39, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=39), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[36, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=36), Data(x=[29, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=29), Data(x=[32, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=32), Data(x=[35, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=35), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[29, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=29), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[48, 9], edge_index=[2, 108], edge_attr=[108, 3], y=[1, 1], num_nodes=48), Data(x=[45, 9], edge_index=[2, 102], edge_attr=[102, 3], y=[1, 1], num_nodes=45), Data(x=[67, 9], edge_index=[2, 144], edge_attr=[144, 3], y=[1, 1], num_nodes=67), Data(x=[67, 9], edge_index=[2, 144], edge_attr=[144, 3], y=[1, 1], num_nodes=67), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[101, 9], edge_index=[2, 220], edge_attr=[220, 3], y=[1, 1], num_nodes=101), Data(x=[121, 9], edge_index=[2, 260], edge_attr=[260, 3], y=[1, 1], num_nodes=121), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[31, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=31), Data(x=[31, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=31), Data(x=[37, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=37), Data(x=[37, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=37), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[42, 9], edge_index=[2, 96], edge_attr=[96, 3], y=[1, 1], num_nodes=42), Data(x=[42, 9], edge_index=[2, 96], edge_attr=[96, 3], y=[1, 1], num_nodes=42), Data(x=[37, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=37), Data(x=[35, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=35), Data(x=[57, 9], edge_index=[2, 120], edge_attr=[120, 3], y=[1, 1], num_nodes=57), Data(x=[56, 9], edge_index=[2, 118], edge_attr=[118, 3], y=[1, 1], num_nodes=56), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[5, 9], edge_index=[2, 10], edge_attr=[10, 3], y=[1, 1], num_nodes=5), Data(x=[35, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=35), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[6, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=6), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[31, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=31), Data(x=[28, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=28), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[12, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=12), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[10, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=10), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[17, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=17), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[17, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=17), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[33, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=33), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[25, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=25), Data(x=[26, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=26), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[30, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=30), Data(x=[40, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=40), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[8, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=8), Data(x=[6, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=6), Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=33), Data(x=[25, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=25), Data(x=[5, 9], edge_index=[2, 10], edge_attr=[10, 3], y=[1, 1], num_nodes=5), Data(x=[8, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=8), Data(x=[20, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=21), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[18, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=18), Data(x=[42, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=42), Data(x=[42, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=42), Data(x=[48, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=48), Data(x=[47, 9], edge_index=[2, 96], edge_attr=[96, 3], y=[1, 1], num_nodes=47), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[9, 9], edge_index=[2, 18], edge_attr=[18, 3], y=[1, 1], num_nodes=9), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[78, 9], edge_index=[2, 158], edge_attr=[158, 3], y=[1, 1], num_nodes=78), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[5, 9], edge_index=[2, 12], edge_attr=[12, 3], y=[1, 1], num_nodes=5), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[46, 9], edge_index=[2, 102], edge_attr=[102, 3], y=[1, 1], num_nodes=46), Data(x=[31, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=31), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=33), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[30, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=30), Data(x=[46, 9], edge_index=[2, 106], edge_attr=[106, 3], y=[1, 1], num_nodes=46), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[29, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=29), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[91, 9], edge_index=[2, 190], edge_attr=[190, 3], y=[1, 1], num_nodes=91), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[34, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=34), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[38, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=38), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[7, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=7), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[29, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=29), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[36, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=36), Data(x=[36, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=36), Data(x=[36, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=36), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[40, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=40), Data(x=[67, 9], edge_index=[2, 136], edge_attr=[136, 3], y=[1, 1], num_nodes=67), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[15, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=15), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[42, 9], edge_index=[2, 94], edge_attr=[94, 3], y=[1, 1], num_nodes=42), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[43, 9], edge_index=[2, 94], edge_attr=[94, 3], y=[1, 1], num_nodes=43), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[96, 9], edge_index=[2, 206], edge_attr=[206, 3], y=[1, 1], num_nodes=96), Data(x=[36, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=36), Data(x=[44, 9], edge_index=[2, 96], edge_attr=[96, 3], y=[1, 1], num_nodes=44), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[48, 9], edge_index=[2, 100], edge_attr=[100, 3], y=[1, 1], num_nodes=48), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[36, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=36), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[24, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=24), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[42, 9], edge_index=[2, 92], edge_attr=[92, 3], y=[1, 1], num_nodes=42), Data(x=[82, 9], edge_index=[2, 176], edge_attr=[176, 3], y=[1, 1], num_nodes=82), Data(x=[89, 9], edge_index=[2, 190], edge_attr=[190, 3], y=[1, 1], num_nodes=89), Data(x=[46, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=46), Data(x=[37, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=37), Data(x=[115, 9], edge_index=[2, 236], edge_attr=[236, 3], y=[1, 1], num_nodes=115), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[27, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=27), Data(x=[27, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=27), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[35, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=35), Data(x=[39, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=39), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[41, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=41), Data(x=[11, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=11), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=17), Data(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=25), Data(x=[19, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=19), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[42, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=42), Data(x=[26, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=26), Data(x=[49, 9], edge_index=[2, 106], edge_attr=[106, 3], y=[1, 1], num_nodes=49), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[57, 9], edge_index=[2, 130], edge_attr=[130, 3], y=[1, 1], num_nodes=57), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[57, 9], edge_index=[2, 130], edge_attr=[130, 3], y=[1, 1], num_nodes=57), Data(x=[43, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=43), Data(x=[43, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=43), Data(x=[34, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=34), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[72, 9], edge_index=[2, 150], edge_attr=[150, 3], y=[1, 1], num_nodes=72), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[10, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=10), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[39, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=39), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[27, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=27), Data(x=[49, 9], edge_index=[2, 110], edge_attr=[110, 3], y=[1, 1], num_nodes=49), Data(x=[85, 9], edge_index=[2, 172], edge_attr=[172, 3], y=[1, 1], num_nodes=85), Data(x=[81, 9], edge_index=[2, 162], edge_attr=[162, 3], y=[1, 1], num_nodes=81), Data(x=[100, 9], edge_index=[2, 206], edge_attr=[206, 3], y=[1, 1], num_nodes=100), Data(x=[43, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=43), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[33, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=33), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[36, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=36), Data(x=[53, 9], edge_index=[2, 116], edge_attr=[116, 3], y=[1, 1], num_nodes=53), Data(x=[85, 9], edge_index=[2, 170], edge_attr=[170, 3], y=[1, 1], num_nodes=85), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[62, 9], edge_index=[2, 136], edge_attr=[136, 3], y=[1, 1], num_nodes=62), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[73, 9], edge_index=[2, 160], edge_attr=[160, 3], y=[1, 1], num_nodes=73), Data(x=[52, 9], edge_index=[2, 108], edge_attr=[108, 3], y=[1, 1], num_nodes=52), Data(x=[56, 9], edge_index=[2, 124], edge_attr=[124, 3], y=[1, 1], num_nodes=56), Data(x=[51, 9], edge_index=[2, 114], edge_attr=[114, 3], y=[1, 1], num_nodes=51), Data(x=[31, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=31), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[35, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=35), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[57, 9], edge_index=[2, 124], edge_attr=[124, 3], y=[1, 1], num_nodes=57), Data(x=[17, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=17), Data(x=[41, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=41), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[46, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=46), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[24, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=24), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[37, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=37), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[43, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=43), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[90, 9], edge_index=[2, 192], edge_attr=[192, 3], y=[1, 1], num_nodes=90), Data(x=[25, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=25), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[90, 9], edge_index=[2, 194], edge_attr=[194, 3], y=[1, 1], num_nodes=90), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[42, 9], edge_index=[2, 92], edge_attr=[92, 3], y=[1, 1], num_nodes=42), Data(x=[53, 9], edge_index=[2, 116], edge_attr=[116, 3], y=[1, 1], num_nodes=53), Data(x=[87, 9], edge_index=[2, 192], edge_attr=[192, 3], y=[1, 1], num_nodes=87), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[62, 9], edge_index=[2, 136], edge_attr=[136, 3], y=[1, 1], num_nodes=62), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[96, 9], edge_index=[2, 202], edge_attr=[202, 3], y=[1, 1], num_nodes=96), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[40, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=40), Data(x=[44, 9], edge_index=[2, 94], edge_attr=[94, 3], y=[1, 1], num_nodes=44), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[58, 9], edge_index=[2, 126], edge_attr=[126, 3], y=[1, 1], num_nodes=58), Data(x=[76, 9], edge_index=[2, 166], edge_attr=[166, 3], y=[1, 1], num_nodes=76), Data(x=[39, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=39), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[61, 9], edge_index=[2, 132], edge_attr=[132, 3], y=[1, 1], num_nodes=61), Data(x=[63, 9], edge_index=[2, 136], edge_attr=[136, 3], y=[1, 1], num_nodes=63), Data(x=[59, 9], edge_index=[2, 126], edge_attr=[126, 3], y=[1, 1], num_nodes=59), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[30, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=30), Data(x=[24, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=24), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[40, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=40), Data(x=[42, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=42), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[35, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=35), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[38, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=38), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[32, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=32), Data(x=[24, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=24), Data(x=[12, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=12), Data(x=[38, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=38), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[32, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=32), Data(x=[37, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=37), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[29, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=29), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[36, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=36), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[43, 9], edge_index=[2, 96], edge_attr=[96, 3], y=[1, 1], num_nodes=43), Data(x=[38, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=38), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[13, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=13), Data(x=[32, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=32), Data(x=[29, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=29), Data(x=[23, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=23), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[62, 9], edge_index=[2, 126], edge_attr=[126, 3], y=[1, 1], num_nodes=62), Data(x=[43, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=43), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[29, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=29), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[71, 9], edge_index=[2, 150], edge_attr=[150, 3], y=[1, 1], num_nodes=71), Data(x=[31, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=31), Data(x=[35, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=35), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[54, 9], edge_index=[2, 112], edge_attr=[112, 3], y=[1, 1], num_nodes=54), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[18, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=18), Data(x=[38, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=38), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[96, 9], edge_index=[2, 206], edge_attr=[206, 3], y=[1, 1], num_nodes=96), Data(x=[43, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=43), Data(x=[85, 9], edge_index=[2, 180], edge_attr=[180, 3], y=[1, 1], num_nodes=85), Data(x=[95, 9], edge_index=[2, 204], edge_attr=[204, 3], y=[1, 1], num_nodes=95), Data(x=[52, 9], edge_index=[2, 110], edge_attr=[110, 3], y=[1, 1], num_nodes=52), Data(x=[50, 9], edge_index=[2, 106], edge_attr=[106, 3], y=[1, 1], num_nodes=50), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[33, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=33), Data(x=[41, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=41), Data(x=[33, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=33), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[36, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=36), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=33), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[41, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=41), Data(x=[37, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=37), Data(x=[45, 9], edge_index=[2, 98], edge_attr=[98, 3], y=[1, 1], num_nodes=45), Data(x=[49, 9], edge_index=[2, 106], edge_attr=[106, 3], y=[1, 1], num_nodes=49), Data(x=[39, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=39), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[35, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=35), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[51, 9], edge_index=[2, 106], edge_attr=[106, 3], y=[1, 1], num_nodes=51), Data(x=[37, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=37), Data(x=[36, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=36), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[35, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=35), Data(x=[26, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=26), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[18, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=18), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[12, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=12), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=22), Data(x=[21, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[32, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=32), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[74, 9], edge_index=[2, 158], edge_attr=[158, 3], y=[1, 1], num_nodes=74), Data(x=[74, 9], edge_index=[2, 158], edge_attr=[158, 3], y=[1, 1], num_nodes=74), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[26, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=26), Data(x=[38, 9], edge_index=[2, 86], edge_attr=[86, 3], y=[1, 1], num_nodes=38), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[31, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=31), Data(x=[36, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=36), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[40, 9], edge_index=[2, 84], edge_attr=[84, 3], y=[1, 1], num_nodes=40), Data(x=[65, 9], edge_index=[2, 134], edge_attr=[134, 3], y=[1, 1], num_nodes=65), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[35, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=35), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[37, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=37), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[117, 9], edge_index=[2, 248], edge_attr=[248, 3], y=[1, 1], num_nodes=117), Data(x=[35, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=35), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[94, 9], edge_index=[2, 196], edge_attr=[196, 3], y=[1, 1], num_nodes=94), Data(x=[77, 9], edge_index=[2, 164], edge_attr=[164, 3], y=[1, 1], num_nodes=77), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[18, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=18), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[35, 9], edge_index=[2, 78], edge_attr=[78, 3], y=[1, 1], num_nodes=35), Data(x=[30, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=30), Data(x=[27, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=27), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[24, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=24), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[25, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=25), Data(x=[22, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=22), Data(x=[30, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=30), Data(x=[24, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=24), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[27, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=27), Data(x=[34, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=34), Data(x=[13, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=13), Data(x=[35, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=35), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[36, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], num_nodes=36), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[52, 9], edge_index=[2, 120], edge_attr=[120, 3], y=[1, 1], num_nodes=52), Data(x=[57, 9], edge_index=[2, 120], edge_attr=[120, 3], y=[1, 1], num_nodes=57), Data(x=[47, 9], edge_index=[2, 100], edge_attr=[100, 3], y=[1, 1], num_nodes=47), Data(x=[66, 9], edge_index=[2, 148], edge_attr=[148, 3], y=[1, 1], num_nodes=66), Data(x=[44, 9], edge_index=[2, 94], edge_attr=[94, 3], y=[1, 1], num_nodes=44), Data(x=[65, 9], edge_index=[2, 132], edge_attr=[132, 3], y=[1, 1], num_nodes=65), Data(x=[79, 9], edge_index=[2, 164], edge_attr=[164, 3], y=[1, 1], num_nodes=79), Data(x=[33, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=33), Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=19), Data(x=[38, 9], edge_index=[2, 88], edge_attr=[88, 3], y=[1, 1], num_nodes=38), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[136, 9], edge_index=[2, 286], edge_attr=[286, 3], y=[1, 1], num_nodes=136), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[43, 9], edge_index=[2, 100], edge_attr=[100, 3], y=[1, 1], num_nodes=43), Data(x=[43, 9], edge_index=[2, 100], edge_attr=[100, 3], y=[1, 1], num_nodes=43), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[21, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=21), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=16), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[11, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=11), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[16, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[17, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=17), Data(x=[8, 9], edge_index=[2, 16], edge_attr=[16, 3], y=[1, 1], num_nodes=8), Data(x=[14, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=14), Data(x=[12, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=12), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[16, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=16), Data(x=[15, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=15), Data(x=[18, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=18), Data(x=[19, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=19), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[92, 9], edge_index=[2, 196], edge_attr=[196, 3], y=[1, 1], num_nodes=92), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[18, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], num_nodes=18), Data(x=[20, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=20), Data(x=[12, 9], edge_index=[2, 26], edge_attr=[26, 3], y=[1, 1], num_nodes=12), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[28, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=28), Data(x=[35, 9], edge_index=[2, 82], edge_attr=[82, 3], y=[1, 1], num_nodes=35), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[24, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=24), Data(x=[15, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=15), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[31, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=31), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[34, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=34), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[26, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=26), Data(x=[20, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=20), Data(x=[30, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=30), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[18, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=18), Data(x=[30, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=30), Data(x=[19, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=19), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[23, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=23), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[25, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=25), Data(x=[29, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=29), Data(x=[32, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=32), Data(x=[14, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=14), Data(x=[27, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=27), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[28, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=28), Data(x=[42, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=42), Data(x=[22, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=22), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[11, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=11), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[33, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=33), Data(x=[26, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=26), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[34, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=34), Data(x=[11, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=11), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[34, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=34), Data(x=[29, 9], edge_index=[2, 64], edge_attr=[64, 3], y=[1, 1], num_nodes=29), Data(x=[24, 9], edge_index=[2, 52], edge_attr=[52, 3], y=[1, 1], num_nodes=24), Data(x=[36, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=36), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[34, 9], edge_index=[2, 76], edge_attr=[76, 3], y=[1, 1], num_nodes=34), Data(x=[26, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=26), Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], y=[1, 1], num_nodes=32), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[26, 9], edge_index=[2, 62], edge_attr=[62, 3], y=[1, 1], num_nodes=26), Data(x=[13, 9], edge_index=[2, 20], edge_attr=[20, 3], y=[1, 1], num_nodes=13), Data(x=[28, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=28), Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=22), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[23, 9], edge_index=[2, 50], edge_attr=[50, 3], y=[1, 1], num_nodes=23), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[21, 9], edge_index=[2, 44], edge_attr=[44, 3], y=[1, 1], num_nodes=21), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22), Data(x=[16, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=16), Data(x=[17, 9], edge_index=[2, 36], edge_attr=[36, 3], y=[1, 1], num_nodes=17), Data(x=[25, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=25), Data(x=[30, 9], edge_index=[2, 66], edge_attr=[66, 3], y=[1, 1], num_nodes=30), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[18, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=18), Data(x=[24, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=24), Data(x=[27, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=27), Data(x=[10, 9], edge_index=[2, 22], edge_attr=[22, 3], y=[1, 1], num_nodes=10), Data(x=[27, 9], edge_index=[2, 58], edge_attr=[58, 3], y=[1, 1], num_nodes=27), Data(x=[122, 9], edge_index=[2, 264], edge_attr=[264, 3], y=[1, 1], num_nodes=122), Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], y=[1, 1], num_nodes=28), Data(x=[44, 9], edge_index=[2, 90], edge_attr=[90, 3], y=[1, 1], num_nodes=44), Data(x=[34, 9], edge_index=[2, 70], edge_attr=[70, 3], y=[1, 1], num_nodes=34), Data(x=[26, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=26), Data(x=[15, 9], edge_index=[2, 32], edge_attr=[32, 3], y=[1, 1], num_nodes=15), Data(x=[17, 9], edge_index=[2, 38], edge_attr=[38, 3], y=[1, 1], num_nodes=17), Data(x=[16, 9], edge_index=[2, 34], edge_attr=[34, 3], y=[1, 1], num_nodes=16), Data(x=[7, 9], edge_index=[2, 14], edge_attr=[14, 3], y=[1, 1], num_nodes=7), Data(x=[35, 9], edge_index=[2, 72], edge_attr=[72, 3], y=[1, 1], num_nodes=35), Data(x=[14, 9], edge_index=[2, 30], edge_attr=[30, 3], y=[1, 1], num_nodes=14), Data(x=[23, 9], edge_index=[2, 54], edge_attr=[54, 3], y=[1, 1], num_nodes=23), Data(x=[36, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=36), Data(x=[20, 9], edge_index=[2, 42], edge_attr=[42, 3], y=[1, 1], num_nodes=20), Data(x=[21, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=21), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[23, 9], edge_index=[2, 48], edge_attr=[48, 3], y=[1, 1], num_nodes=23), Data(x=[12, 9], edge_index=[2, 24], edge_attr=[24, 3], y=[1, 1], num_nodes=12), Data(x=[35, 9], edge_index=[2, 74], edge_attr=[74, 3], y=[1, 1], num_nodes=35), Data(x=[13, 9], edge_index=[2, 28], edge_attr=[28, 3], y=[1, 1], num_nodes=13), Data(x=[27, 9], edge_index=[2, 56], edge_attr=[56, 3], y=[1, 1], num_nodes=27), Data(x=[22, 9], edge_index=[2, 46], edge_attr=[46, 3], y=[1, 1], num_nodes=22)]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch_geometric\n",
        "# Load Data object from file\n",
        "with open('/content/drive/MyDrive/clintox.pt', 'rb') as f:\n",
        "    all_data = pickle.load(f)\n",
        "\n",
        "# Now loaded_data contains the Data object\n",
        "print(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nzQS4IfKzcC9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import stellargraph as sg\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from stellargraph.data import BiasedRandomWalk\n",
        "\n",
        "def create_networkx_graph(edge_index, edge_attr, x, y, num_nodes):\n",
        "    # Create an empty NetworkX graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with features\n",
        "    for i in range(num_nodes):\n",
        "        node_features = {f\"feat_{j}\": x[i, j].item() for j in range(x.shape[1])}\n",
        "        G.add_node(i, features=node_features)\n",
        "\n",
        "    # Add edges with attributes\n",
        "    for j in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[:, j].tolist()\n",
        "        edge_attributes = {f\"attr_{k}\": edge_attr[j, k].item() for k in range(edge_attr.shape[1])}\n",
        "        G.add_edge(src, dst, attributes=edge_attributes)\n",
        "\n",
        "    return G, y.item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names=None):\n",
        "    \"\"\"\n",
        "    Plots the confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (list or array): True labels.\n",
        "    y_pred (list or array): Predicted labels.\n",
        "    class_names (list): List of class names. If None, integer labels are used.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# y_true = [0, 1, 1, 0, 1, 0]\n",
        "# y_pred = [0, 1, 0, 0, 1, 1]\n",
        "# class_names = ['Class 0', 'Class 1']\n",
        "# plot_confusion_matrix(y_true, y_pred, class_names)\n"
      ],
      "metadata": {
        "id": "e_Ix2PETlKT7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZBWOStOXUU8"
      },
      "source": [
        "#Node2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxkjURhnXcIv"
      },
      "source": [
        "Construction of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "7I_OdnXOCqE8",
        "outputId": "f8513bc6-2316-437a-cf29-0181237b21b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0\n",
            "size 0 data: 1443\n",
            "size 1 data: 1443\n",
            "size total dataset: 2886\n",
            "2886\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ad4d6fb33a39>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mrw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiasedRandomWalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     walks = rw.run(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# root nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# maximum length of a random walk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stellargraph/data/explorer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, nodes, n, length, p, q, seed, weighted)\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0;31m# appropriate transition probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                         neighbours, weights = self.graph.neighbor_arrays(\n\u001b[0m\u001b[1;32m    488\u001b[0m                             \u001b[0mcurrent_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_edge_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_ilocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stellargraph/core/graph.py\u001b[0m in \u001b[0;36mneighbor_arrays\u001b[0;34m(self, node, include_edge_weight, edge_types, use_ilocs)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_iloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0medge_ilocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_ilocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_ilocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_ilocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stellargraph/core/element_data.py\u001b[0m in \u001b[0;36medge_ilocs\u001b[0;34m(self, node_id, ins, outs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stellargraph/core/element_data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"node ilocs must be non-negative.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import copy\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "label_0_indices = []\n",
        "label_1_indices = []\n",
        "dfs = []\n",
        "\n",
        "for i in range(len(all_data)):\n",
        "  if all_data[i].y == 0:\n",
        "    label_0_indices.append(i)\n",
        "  elif all_data[i].y == 1:\n",
        "    label_1_indices.append(i)\n",
        "\n",
        "for iter in range(1):\n",
        "  print('iter:', iter)\n",
        "\n",
        "  min_size_dataset = min(len(label_0_indices), len(label_1_indices))\n",
        "\n",
        "  random.shuffle(label_0_indices)\n",
        "  random.shuffle(label_1_indices)\n",
        "  label_0_indices = label_0_indices[:min_size_dataset]\n",
        "  label_1_indices = label_1_indices[:min_size_dataset]\n",
        "  print('size 0 data:' , len(label_0_indices))\n",
        "  print('size 1 data:', len(label_1_indices))\n",
        "\n",
        "  balanced_indices = label_0_indices + label_1_indices\n",
        "  print('size total dataset:' , len(balanced_indices))\n",
        "\n",
        "  balanced_data = [all_data[i] for i in balanced_indices]\n",
        "  print(len(balanced_data))\n",
        "\n",
        "  df = pd.DataFrame(columns = ['emb','label'])\n",
        "\n",
        "  for dtb in balanced_data:\n",
        "    graph,y = create_networkx_graph(dtb.edge_index, dtb.edge_attr, dtb.x, dtb.y, dtb.num_nodes)\n",
        "        # Convert node features to DataFrame\n",
        "    node_features_dict = {node: graph.nodes[node]['features'] for node in graph.nodes}\n",
        "    node_df = pd.DataFrame.from_dict(node_features_dict, orient='index')\n",
        "\n",
        "        # Convert edge attributes to DataFrame\n",
        "    edge_features_dict = {(src, dst): data['attributes'] for src, dst, data in graph.edges(data=True)}\n",
        "    edge_df = pd.DataFrame.from_dict(edge_features_dict, orient='index')\n",
        "\n",
        "        # Create 'source', 'target' columns in the edge DataFrame\n",
        "    edge_df['source'] = [edge[0] for edge in edge_df.index]\n",
        "    edge_df['target'] = [edge[1] for edge in edge_df.index]\n",
        "\n",
        "        # Create a StellarGraph from the NetworkX graph and DataFrames\n",
        "    Gs = sg.StellarGraph(nodes=node_df, edges=edge_df)\n",
        "\n",
        "    rw = BiasedRandomWalk(Gs)\n",
        "\n",
        "    walks = rw.run(\n",
        "        nodes=list(Gs.nodes()),  # root nodes\n",
        "        length=100,  # maximum length of a random walk\n",
        "        n=10,  # number of random walks per root node\n",
        "        p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
        "        q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
        "        weighted=True,\n",
        "        seed = 42\n",
        "    )\n",
        "\n",
        "    str_walks = [[str(n) for n in walk] for walk in walks]\n",
        "    model = Word2Vec(str_walks, window=5, min_count=0, sg=1, workers=2)\n",
        "\n",
        "        # Retrieve node embeddings and corresponding subjects\n",
        "    node_ids = model.wv.index_to_key  # list of node IDs\n",
        "    node_embeddings = (\n",
        "        model.wv.vectors\n",
        "    )  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
        "\n",
        "    df.loc[len(df)] = [node_embeddings, y]\n",
        "  #df.to_pickle('/content/drive/MyDrive/emb_bbbp_node2vec_data_' + str(iter + 1) + '.pkl')\n",
        "  #print(df)\n",
        "  dfs.append(df)\n",
        "  #se va tutto bene poi questo break si toglie e si fa per tuti i kfold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Udl-HSNsLnu"
      },
      "source": [
        "Creation of the Model for classification on the Embedding data given from node2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMg83xsGuhn7"
      },
      "outputs": [],
      "source": [
        "# Define a function to create and compile your model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def create_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(222, 100)),\n",
        "    layers.Dropout(0.3),  # Adding dropout for regularization\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),  # Adding dropout for regularization\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.3),  # Adding dropout for regularization\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVeC_w4UsjDq"
      },
      "source": [
        "5 k-fold on 5 datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvjMtzYbuJxl",
        "outputId": "d608da4b-9b3b-47a4-db8b-69aa2436fda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6723 - accuracy: 0.5319 - val_loss: 0.6956 - val_accuracy: 0.5440\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6715 - accuracy: 0.5324 - val_loss: 0.6985 - val_accuracy: 0.5444\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6711 - accuracy: 0.5329 - val_loss: 0.6993 - val_accuracy: 0.5438\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6707 - accuracy: 0.5331 - val_loss: 0.6974 - val_accuracy: 0.5437\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6702 - accuracy: 0.5333 - val_loss: 0.6968 - val_accuracy: 0.5435\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6699 - accuracy: 0.5334 - val_loss: 0.6967 - val_accuracy: 0.5442\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6698 - accuracy: 0.5337 - val_loss: 0.6988 - val_accuracy: 0.5440\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6694 - accuracy: 0.5340 - val_loss: 0.6997 - val_accuracy: 0.5430\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6692 - accuracy: 0.5341 - val_loss: 0.6985 - val_accuracy: 0.5436\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6687 - accuracy: 0.5345 - val_loss: 0.7002 - val_accuracy: 0.5433\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6686 - accuracy: 0.5346 - val_loss: 0.6997 - val_accuracy: 0.5432\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6683 - accuracy: 0.5350 - val_loss: 0.7013 - val_accuracy: 0.5439\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6684 - accuracy: 0.5346 - val_loss: 0.7037 - val_accuracy: 0.5433\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6682 - accuracy: 0.5350 - val_loss: 0.6996 - val_accuracy: 0.5429\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6674 - accuracy: 0.5357 - val_loss: 0.7038 - val_accuracy: 0.5438\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6672 - accuracy: 0.5354 - val_loss: 0.7017 - val_accuracy: 0.5429\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 3s 44ms/step - loss: 0.6670 - accuracy: 0.5357 - val_loss: 0.7079 - val_accuracy: 0.5433\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6670 - accuracy: 0.5357 - val_loss: 0.7037 - val_accuracy: 0.5426\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6666 - accuracy: 0.5372 - val_loss: 0.7105 - val_accuracy: 0.4859\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6684 - accuracy: 0.5193 - val_loss: 0.7016 - val_accuracy: 0.5437\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6666 - accuracy: 0.5361 - val_loss: 0.7085 - val_accuracy: 0.5436\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6671 - accuracy: 0.5299 - val_loss: 0.7062 - val_accuracy: 0.4871\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6662 - accuracy: 0.5324 - val_loss: 0.7066 - val_accuracy: 0.5435\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6659 - accuracy: 0.5365 - val_loss: 0.7081 - val_accuracy: 0.5438\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6656 - accuracy: 0.5368 - val_loss: 0.7038 - val_accuracy: 0.5438\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6656 - accuracy: 0.5366 - val_loss: 0.7054 - val_accuracy: 0.5428\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6653 - accuracy: 0.5367 - val_loss: 0.7076 - val_accuracy: 0.5434\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6656 - accuracy: 0.5368 - val_loss: 0.7088 - val_accuracy: 0.5432\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 3s 44ms/step - loss: 0.6652 - accuracy: 0.5370 - val_loss: 0.7110 - val_accuracy: 0.5435\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6647 - accuracy: 0.5374 - val_loss: 0.7076 - val_accuracy: 0.5429\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6647 - accuracy: 0.5375 - val_loss: 0.7095 - val_accuracy: 0.5434\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6647 - accuracy: 0.5373 - val_loss: 0.7102 - val_accuracy: 0.5439\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 3s 44ms/step - loss: 0.6646 - accuracy: 0.5376 - val_loss: 0.7071 - val_accuracy: 0.5432\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6651 - accuracy: 0.5373 - val_loss: 0.7090 - val_accuracy: 0.5428\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6642 - accuracy: 0.5379 - val_loss: 0.7102 - val_accuracy: 0.5440\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6641 - accuracy: 0.5379 - val_loss: 0.7100 - val_accuracy: 0.5435\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6638 - accuracy: 0.5382 - val_loss: 0.7115 - val_accuracy: 0.5442\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6636 - accuracy: 0.5384 - val_loss: 0.7138 - val_accuracy: 0.5436\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6633 - accuracy: 0.5383 - val_loss: 0.7108 - val_accuracy: 0.5431\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6634 - accuracy: 0.5381 - val_loss: 0.7142 - val_accuracy: 0.5435\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6632 - accuracy: 0.5384 - val_loss: 0.7138 - val_accuracy: 0.5435\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6628 - accuracy: 0.5384 - val_loss: 0.7158 - val_accuracy: 0.5432\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6628 - accuracy: 0.5383 - val_loss: 0.7163 - val_accuracy: 0.5435\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6629 - accuracy: 0.5387 - val_loss: 0.7151 - val_accuracy: 0.5431\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6628 - accuracy: 0.5388 - val_loss: 0.7128 - val_accuracy: 0.5433\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6627 - accuracy: 0.5385 - val_loss: 0.7147 - val_accuracy: 0.5425\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6627 - accuracy: 0.5388 - val_loss: 0.7134 - val_accuracy: 0.5431\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6623 - accuracy: 0.5390 - val_loss: 0.7148 - val_accuracy: 0.5427\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 76ms/step - loss: 0.6626 - accuracy: 0.5391 - val_loss: 0.7150 - val_accuracy: 0.5427\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6620 - accuracy: 0.5393 - val_loss: 0.7189 - val_accuracy: 0.5431\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6619 - accuracy: 0.5391 - val_loss: 0.7156 - val_accuracy: 0.5430\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6621 - accuracy: 0.5393 - val_loss: 0.7124 - val_accuracy: 0.5424\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6622 - accuracy: 0.5391 - val_loss: 0.7143 - val_accuracy: 0.5425\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6616 - accuracy: 0.5396 - val_loss: 0.7140 - val_accuracy: 0.5425\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 3s 44ms/step - loss: 0.6619 - accuracy: 0.5394 - val_loss: 0.7164 - val_accuracy: 0.5427\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6614 - accuracy: 0.5398 - val_loss: 0.7153 - val_accuracy: 0.5430\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6612 - accuracy: 0.5399 - val_loss: 0.7156 - val_accuracy: 0.5427\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6616 - accuracy: 0.5397 - val_loss: 0.7156 - val_accuracy: 0.5430\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 6s 89ms/step - loss: 0.6614 - accuracy: 0.5397 - val_loss: 0.7175 - val_accuracy: 0.5427\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6613 - accuracy: 0.5399 - val_loss: 0.7154 - val_accuracy: 0.5428\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6617 - accuracy: 0.5407 - val_loss: 0.7164 - val_accuracy: 0.5438\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6608 - accuracy: 0.5400 - val_loss: 0.7175 - val_accuracy: 0.5431\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6614 - accuracy: 0.5340 - val_loss: 0.7157 - val_accuracy: 0.5429\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6608 - accuracy: 0.5398 - val_loss: 0.7178 - val_accuracy: 0.5429\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6608 - accuracy: 0.5399 - val_loss: 0.7194 - val_accuracy: 0.5438\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6605 - accuracy: 0.5403 - val_loss: 0.7185 - val_accuracy: 0.5432\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6602 - accuracy: 0.5405 - val_loss: 0.7164 - val_accuracy: 0.5428\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6608 - accuracy: 0.5401 - val_loss: 0.7176 - val_accuracy: 0.5437\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6601 - accuracy: 0.5404 - val_loss: 0.7206 - val_accuracy: 0.5427\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6600 - accuracy: 0.5404 - val_loss: 0.7200 - val_accuracy: 0.5433\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6603 - accuracy: 0.5403 - val_loss: 0.7185 - val_accuracy: 0.5431\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6600 - accuracy: 0.5405 - val_loss: 0.7169 - val_accuracy: 0.5430\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6602 - accuracy: 0.5404 - val_loss: 0.7193 - val_accuracy: 0.5432\n",
            "19/19 [==============================] - 1s 13ms/step - loss: 0.7199 - accuracy: 0.5429\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 5s 55ms/step - loss: 0.6907 - accuracy: 0.4969 - val_loss: 0.6910 - val_accuracy: 0.5287\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6871 - accuracy: 0.5204 - val_loss: 0.6900 - val_accuracy: 0.5326\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6856 - accuracy: 0.5229 - val_loss: 0.6922 - val_accuracy: 0.5326\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6846 - accuracy: 0.5237 - val_loss: 0.6922 - val_accuracy: 0.5330\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 7s 98ms/step - loss: 0.6833 - accuracy: 0.5246 - val_loss: 0.6929 - val_accuracy: 0.5334\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6825 - accuracy: 0.5255 - val_loss: 0.6908 - val_accuracy: 0.5347\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6816 - accuracy: 0.5264 - val_loss: 0.6920 - val_accuracy: 0.5343\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6809 - accuracy: 0.5266 - val_loss: 0.6904 - val_accuracy: 0.5348\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6801 - accuracy: 0.5274 - val_loss: 0.6906 - val_accuracy: 0.5335\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6797 - accuracy: 0.5280 - val_loss: 0.6918 - val_accuracy: 0.5351\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6789 - accuracy: 0.5285 - val_loss: 0.6924 - val_accuracy: 0.5351\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6785 - accuracy: 0.5286 - val_loss: 0.6943 - val_accuracy: 0.5348\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6778 - accuracy: 0.5292 - val_loss: 0.6934 - val_accuracy: 0.5350\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6772 - accuracy: 0.5298 - val_loss: 0.6935 - val_accuracy: 0.5351\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6769 - accuracy: 0.5299 - val_loss: 0.6935 - val_accuracy: 0.5338\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6767 - accuracy: 0.5303 - val_loss: 0.6971 - val_accuracy: 0.5344\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6758 - accuracy: 0.5306 - val_loss: 0.6954 - val_accuracy: 0.5347\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6754 - accuracy: 0.5311 - val_loss: 0.6974 - val_accuracy: 0.5345\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6752 - accuracy: 0.5316 - val_loss: 0.6975 - val_accuracy: 0.5339\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6747 - accuracy: 0.5315 - val_loss: 0.6972 - val_accuracy: 0.5341\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6743 - accuracy: 0.5318 - val_loss: 0.6969 - val_accuracy: 0.5328\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6738 - accuracy: 0.5324 - val_loss: 0.6989 - val_accuracy: 0.5340\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6732 - accuracy: 0.5327 - val_loss: 0.7000 - val_accuracy: 0.5340\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6728 - accuracy: 0.5331 - val_loss: 0.7055 - val_accuracy: 0.5350\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6727 - accuracy: 0.5332 - val_loss: 0.7006 - val_accuracy: 0.5346\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6723 - accuracy: 0.5333 - val_loss: 0.7030 - val_accuracy: 0.5344\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6721 - accuracy: 0.5336 - val_loss: 0.7004 - val_accuracy: 0.5338\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6714 - accuracy: 0.5339 - val_loss: 0.7038 - val_accuracy: 0.5349\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6712 - accuracy: 0.5345 - val_loss: 0.7040 - val_accuracy: 0.5343\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6710 - accuracy: 0.5343 - val_loss: 0.7107 - val_accuracy: 0.5345\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6707 - accuracy: 0.5348 - val_loss: 0.7064 - val_accuracy: 0.5347\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6706 - accuracy: 0.5344 - val_loss: 0.7056 - val_accuracy: 0.5345\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6700 - accuracy: 0.5355 - val_loss: 0.7054 - val_accuracy: 0.5339\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6701 - accuracy: 0.5352 - val_loss: 0.7085 - val_accuracy: 0.5340\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6693 - accuracy: 0.5357 - val_loss: 0.7112 - val_accuracy: 0.5341\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6695 - accuracy: 0.5356 - val_loss: 0.7056 - val_accuracy: 0.5337\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6691 - accuracy: 0.5359 - val_loss: 0.7123 - val_accuracy: 0.5341\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6686 - accuracy: 0.5363 - val_loss: 0.7098 - val_accuracy: 0.5335\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6687 - accuracy: 0.5360 - val_loss: 0.7091 - val_accuracy: 0.5334\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6686 - accuracy: 0.5362 - val_loss: 0.7074 - val_accuracy: 0.5331\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6680 - accuracy: 0.5366 - val_loss: 0.7133 - val_accuracy: 0.5339\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6675 - accuracy: 0.5369 - val_loss: 0.7195 - val_accuracy: 0.5342\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6679 - accuracy: 0.5366 - val_loss: 0.7119 - val_accuracy: 0.5335\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6676 - accuracy: 0.5373 - val_loss: 0.7116 - val_accuracy: 0.5338\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6673 - accuracy: 0.5308 - val_loss: 0.7123 - val_accuracy: 0.5333\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6672 - accuracy: 0.5373 - val_loss: 0.7146 - val_accuracy: 0.5333\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6665 - accuracy: 0.5379 - val_loss: 0.7162 - val_accuracy: 0.5333\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6676 - accuracy: 0.5373 - val_loss: 0.7169 - val_accuracy: 0.5341\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6670 - accuracy: 0.5374 - val_loss: 0.7151 - val_accuracy: 0.5339\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6659 - accuracy: 0.5380 - val_loss: 0.7176 - val_accuracy: 0.5339\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6663 - accuracy: 0.5377 - val_loss: 0.7137 - val_accuracy: 0.5339\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6659 - accuracy: 0.5383 - val_loss: 0.7235 - val_accuracy: 0.5339\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6654 - accuracy: 0.5383 - val_loss: 0.7157 - val_accuracy: 0.5334\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6650 - accuracy: 0.5390 - val_loss: 0.7227 - val_accuracy: 0.5339\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6657 - accuracy: 0.5386 - val_loss: 0.7181 - val_accuracy: 0.5337\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6653 - accuracy: 0.5387 - val_loss: 0.7148 - val_accuracy: 0.5336\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6647 - accuracy: 0.5391 - val_loss: 0.7190 - val_accuracy: 0.5339\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6648 - accuracy: 0.5389 - val_loss: 0.7160 - val_accuracy: 0.5331\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6642 - accuracy: 0.5395 - val_loss: 0.7173 - val_accuracy: 0.5332\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6645 - accuracy: 0.5386 - val_loss: 0.7212 - val_accuracy: 0.5338\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6644 - accuracy: 0.5394 - val_loss: 0.7243 - val_accuracy: 0.5337\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6641 - accuracy: 0.5393 - val_loss: 0.7193 - val_accuracy: 0.5331\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6646 - accuracy: 0.5395 - val_loss: 0.7235 - val_accuracy: 0.5331\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6637 - accuracy: 0.5398 - val_loss: 0.7237 - val_accuracy: 0.5333\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6639 - accuracy: 0.5396 - val_loss: 0.7202 - val_accuracy: 0.5332\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6639 - accuracy: 0.5397 - val_loss: 0.7189 - val_accuracy: 0.5336\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6635 - accuracy: 0.5400 - val_loss: 0.7251 - val_accuracy: 0.5339\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6636 - accuracy: 0.5398 - val_loss: 0.7261 - val_accuracy: 0.5341\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6635 - accuracy: 0.5398 - val_loss: 0.7218 - val_accuracy: 0.5328\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6631 - accuracy: 0.5401 - val_loss: 0.7235 - val_accuracy: 0.5331\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6630 - accuracy: 0.5402 - val_loss: 0.7239 - val_accuracy: 0.5338\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6631 - accuracy: 0.5402 - val_loss: 0.7243 - val_accuracy: 0.5332\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6626 - accuracy: 0.5404 - val_loss: 0.7275 - val_accuracy: 0.5339\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6624 - accuracy: 0.5406 - val_loss: 0.7276 - val_accuracy: 0.5335\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6630 - accuracy: 0.5412 - val_loss: 0.7232 - val_accuracy: 0.5329\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6630 - accuracy: 0.5384 - val_loss: 0.7276 - val_accuracy: 0.5339\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6625 - accuracy: 0.5405 - val_loss: 0.7294 - val_accuracy: 0.5337\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6627 - accuracy: 0.5406 - val_loss: 0.7287 - val_accuracy: 0.5337\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6625 - accuracy: 0.5375 - val_loss: 0.7289 - val_accuracy: 0.5340\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6622 - accuracy: 0.5410 - val_loss: 0.7274 - val_accuracy: 0.5334\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6617 - accuracy: 0.5412 - val_loss: 0.7264 - val_accuracy: 0.5337\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6617 - accuracy: 0.5412 - val_loss: 0.7292 - val_accuracy: 0.5329\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6616 - accuracy: 0.5413 - val_loss: 0.7283 - val_accuracy: 0.5329\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6616 - accuracy: 0.5412 - val_loss: 0.7323 - val_accuracy: 0.5338\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6614 - accuracy: 0.5415 - val_loss: 0.7312 - val_accuracy: 0.5334\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6616 - accuracy: 0.5408 - val_loss: 0.7272 - val_accuracy: 0.5327\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6617 - accuracy: 0.5412 - val_loss: 0.7341 - val_accuracy: 0.5340\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6613 - accuracy: 0.5419 - val_loss: 0.7304 - val_accuracy: 0.5328\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6616 - accuracy: 0.5392 - val_loss: 0.7289 - val_accuracy: 0.5333\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6608 - accuracy: 0.5418 - val_loss: 0.7257 - val_accuracy: 0.5327\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6613 - accuracy: 0.5416 - val_loss: 0.7292 - val_accuracy: 0.5330\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6609 - accuracy: 0.5418 - val_loss: 0.7306 - val_accuracy: 0.5334\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6608 - accuracy: 0.5418 - val_loss: 0.7279 - val_accuracy: 0.5326\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6610 - accuracy: 0.5417 - val_loss: 0.7280 - val_accuracy: 0.5328\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6607 - accuracy: 0.5419 - val_loss: 0.7270 - val_accuracy: 0.5331\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6607 - accuracy: 0.5417 - val_loss: 0.7339 - val_accuracy: 0.5337\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6606 - accuracy: 0.5421 - val_loss: 0.7318 - val_accuracy: 0.5334\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6609 - accuracy: 0.5419 - val_loss: 0.7334 - val_accuracy: 0.5335\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6604 - accuracy: 0.5421 - val_loss: 0.7300 - val_accuracy: 0.5322\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6601 - accuracy: 0.5425 - val_loss: 0.7314 - val_accuracy: 0.5328\n",
            "19/19 [==============================] - 1s 13ms/step - loss: 0.7323 - accuracy: 0.5327\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 5s 50ms/step - loss: 0.6906 - accuracy: 0.5214 - val_loss: 0.6892 - val_accuracy: 0.5009\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6874 - accuracy: 0.5292 - val_loss: 0.6878 - val_accuracy: 0.5006\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6865 - accuracy: 0.5310 - val_loss: 0.6859 - val_accuracy: 0.5024\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6851 - accuracy: 0.5321 - val_loss: 0.6854 - val_accuracy: 0.5024\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6843 - accuracy: 0.5328 - val_loss: 0.6853 - val_accuracy: 0.5024\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6830 - accuracy: 0.5340 - val_loss: 0.6868 - val_accuracy: 0.5007\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6822 - accuracy: 0.5352 - val_loss: 0.6856 - val_accuracy: 0.5027\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6815 - accuracy: 0.5355 - val_loss: 0.6853 - val_accuracy: 0.5028\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6808 - accuracy: 0.5357 - val_loss: 0.6865 - val_accuracy: 0.5000\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6802 - accuracy: 0.5367 - val_loss: 0.6862 - val_accuracy: 0.5010\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6791 - accuracy: 0.5374 - val_loss: 0.6874 - val_accuracy: 0.4985\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6788 - accuracy: 0.5379 - val_loss: 0.6873 - val_accuracy: 0.4994\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6780 - accuracy: 0.5385 - val_loss: 0.6870 - val_accuracy: 0.5017\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6773 - accuracy: 0.5390 - val_loss: 0.6884 - val_accuracy: 0.4984\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6766 - accuracy: 0.5394 - val_loss: 0.6885 - val_accuracy: 0.4987\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6764 - accuracy: 0.5397 - val_loss: 0.6896 - val_accuracy: 0.4990\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6756 - accuracy: 0.5401 - val_loss: 0.6893 - val_accuracy: 0.4981\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6751 - accuracy: 0.5403 - val_loss: 0.6908 - val_accuracy: 0.4965\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6748 - accuracy: 0.5409 - val_loss: 0.6909 - val_accuracy: 0.4989\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6742 - accuracy: 0.5409 - val_loss: 0.6908 - val_accuracy: 0.4996\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6739 - accuracy: 0.5416 - val_loss: 0.6913 - val_accuracy: 0.4974\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6734 - accuracy: 0.5418 - val_loss: 0.6913 - val_accuracy: 0.4984\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6729 - accuracy: 0.5423 - val_loss: 0.6954 - val_accuracy: 0.4997\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6733 - accuracy: 0.5419 - val_loss: 0.6916 - val_accuracy: 0.4989\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6722 - accuracy: 0.5424 - val_loss: 0.6936 - val_accuracy: 0.4986\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6718 - accuracy: 0.5429 - val_loss: 0.6940 - val_accuracy: 0.4958\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6716 - accuracy: 0.5430 - val_loss: 0.6922 - val_accuracy: 0.4979\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6714 - accuracy: 0.5431 - val_loss: 0.6941 - val_accuracy: 0.4971\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6709 - accuracy: 0.5435 - val_loss: 0.6948 - val_accuracy: 0.4985\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6709 - accuracy: 0.5434 - val_loss: 0.6947 - val_accuracy: 0.4974\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6702 - accuracy: 0.5440 - val_loss: 0.6942 - val_accuracy: 0.4981\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6697 - accuracy: 0.5443 - val_loss: 0.6939 - val_accuracy: 0.4979\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6699 - accuracy: 0.5442 - val_loss: 0.6945 - val_accuracy: 0.4968\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6698 - accuracy: 0.5441 - val_loss: 0.6981 - val_accuracy: 0.4954\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6696 - accuracy: 0.5445 - val_loss: 0.6958 - val_accuracy: 0.4992\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6689 - accuracy: 0.5448 - val_loss: 0.6951 - val_accuracy: 0.4983\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6688 - accuracy: 0.5451 - val_loss: 0.6962 - val_accuracy: 0.4989\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6688 - accuracy: 0.5450 - val_loss: 0.6961 - val_accuracy: 0.4983\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6682 - accuracy: 0.5452 - val_loss: 0.6966 - val_accuracy: 0.4989\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6678 - accuracy: 0.5458 - val_loss: 0.6977 - val_accuracy: 0.4969\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6675 - accuracy: 0.5457 - val_loss: 0.6983 - val_accuracy: 0.4976\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6674 - accuracy: 0.5459 - val_loss: 0.6986 - val_accuracy: 0.4975\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6674 - accuracy: 0.5461 - val_loss: 0.6975 - val_accuracy: 0.4969\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6673 - accuracy: 0.5428 - val_loss: 0.7006 - val_accuracy: 0.4975\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6670 - accuracy: 0.5464 - val_loss: 0.6994 - val_accuracy: 0.4979\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6672 - accuracy: 0.5466 - val_loss: 0.6989 - val_accuracy: 0.4967\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6669 - accuracy: 0.5464 - val_loss: 0.6995 - val_accuracy: 0.4973\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 9s 123ms/step - loss: 0.6662 - accuracy: 0.5469 - val_loss: 0.6989 - val_accuracy: 0.4974\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6660 - accuracy: 0.5469 - val_loss: 0.7010 - val_accuracy: 0.4988\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6656 - accuracy: 0.5473 - val_loss: 0.7001 - val_accuracy: 0.4976\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6655 - accuracy: 0.5474 - val_loss: 0.6998 - val_accuracy: 0.4986\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6655 - accuracy: 0.5474 - val_loss: 0.6998 - val_accuracy: 0.4978\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6651 - accuracy: 0.5475 - val_loss: 0.7012 - val_accuracy: 0.4988\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6650 - accuracy: 0.5474 - val_loss: 0.7012 - val_accuracy: 0.4968\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6650 - accuracy: 0.5477 - val_loss: 0.7019 - val_accuracy: 0.4985\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6647 - accuracy: 0.5478 - val_loss: 0.7025 - val_accuracy: 0.4984\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 3s 45ms/step - loss: 0.6645 - accuracy: 0.5480 - val_loss: 0.7029 - val_accuracy: 0.4973\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6643 - accuracy: 0.5480 - val_loss: 0.7035 - val_accuracy: 0.4991\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6642 - accuracy: 0.5481 - val_loss: 0.7020 - val_accuracy: 0.4985\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6644 - accuracy: 0.5480 - val_loss: 0.7013 - val_accuracy: 0.4980\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6637 - accuracy: 0.5483 - val_loss: 0.7015 - val_accuracy: 0.4977\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6640 - accuracy: 0.5485 - val_loss: 0.7022 - val_accuracy: 0.4980\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6633 - accuracy: 0.5489 - val_loss: 0.7039 - val_accuracy: 0.4974\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6634 - accuracy: 0.5485 - val_loss: 0.7030 - val_accuracy: 0.4971\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6632 - accuracy: 0.5489 - val_loss: 0.7051 - val_accuracy: 0.4988\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6633 - accuracy: 0.5487 - val_loss: 0.7045 - val_accuracy: 0.4978\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6628 - accuracy: 0.5489 - val_loss: 0.7054 - val_accuracy: 0.4976\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6628 - accuracy: 0.5492 - val_loss: 0.7049 - val_accuracy: 0.4989\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6627 - accuracy: 0.5491 - val_loss: 0.7058 - val_accuracy: 0.4988\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6626 - accuracy: 0.5490 - val_loss: 0.7050 - val_accuracy: 0.4976\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6624 - accuracy: 0.5493 - val_loss: 0.7073 - val_accuracy: 0.4973\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6621 - accuracy: 0.5498 - val_loss: 0.7066 - val_accuracy: 0.4973\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6622 - accuracy: 0.5495 - val_loss: 0.7073 - val_accuracy: 0.4994\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6625 - accuracy: 0.5493 - val_loss: 0.7050 - val_accuracy: 0.4965\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6623 - accuracy: 0.5496 - val_loss: 0.7046 - val_accuracy: 0.4982\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6617 - accuracy: 0.5495 - val_loss: 0.7077 - val_accuracy: 0.4990\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6618 - accuracy: 0.5498 - val_loss: 0.7062 - val_accuracy: 0.4979\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6617 - accuracy: 0.5501 - val_loss: 0.7058 - val_accuracy: 0.4982\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6614 - accuracy: 0.5498 - val_loss: 0.7079 - val_accuracy: 0.4975\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6611 - accuracy: 0.5501 - val_loss: 0.7069 - val_accuracy: 0.4973\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6615 - accuracy: 0.5498 - val_loss: 0.7061 - val_accuracy: 0.4972\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6614 - accuracy: 0.5502 - val_loss: 0.7068 - val_accuracy: 0.4978\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6609 - accuracy: 0.5504 - val_loss: 0.7067 - val_accuracy: 0.4987\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6613 - accuracy: 0.5505 - val_loss: 0.7063 - val_accuracy: 0.4993\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6614 - accuracy: 0.5505 - val_loss: 0.7056 - val_accuracy: 0.4960\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6611 - accuracy: 0.5503 - val_loss: 0.7061 - val_accuracy: 0.4978\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6606 - accuracy: 0.5507 - val_loss: 0.7093 - val_accuracy: 0.4990\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6606 - accuracy: 0.5504 - val_loss: 0.7089 - val_accuracy: 0.4983\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6609 - accuracy: 0.5505 - val_loss: 0.7080 - val_accuracy: 0.4990\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6608 - accuracy: 0.5504 - val_loss: 0.7079 - val_accuracy: 0.4983\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6605 - accuracy: 0.5508 - val_loss: 0.7086 - val_accuracy: 0.4973\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6605 - accuracy: 0.5507 - val_loss: 0.7099 - val_accuracy: 0.4965\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6604 - accuracy: 0.5507 - val_loss: 0.7091 - val_accuracy: 0.4982\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6599 - accuracy: 0.5512 - val_loss: 0.7085 - val_accuracy: 0.4976\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6602 - accuracy: 0.5507 - val_loss: 0.7084 - val_accuracy: 0.4985\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6601 - accuracy: 0.5509 - val_loss: 0.7101 - val_accuracy: 0.4987\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6603 - accuracy: 0.5506 - val_loss: 0.7089 - val_accuracy: 0.4976\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6599 - accuracy: 0.5509 - val_loss: 0.7086 - val_accuracy: 0.4978\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6602 - accuracy: 0.5508 - val_loss: 0.7094 - val_accuracy: 0.4978\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6598 - accuracy: 0.5509 - val_loss: 0.7099 - val_accuracy: 0.4983\n",
            "19/19 [==============================] - 1s 14ms/step - loss: 0.7142 - accuracy: 0.4957\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 81ms/step - loss: 0.6909 - accuracy: 0.5043 - val_loss: 0.6885 - val_accuracy: 0.5304\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 3s 46ms/step - loss: 0.6881 - accuracy: 0.5214 - val_loss: 0.6865 - val_accuracy: 0.5305\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6869 - accuracy: 0.5227 - val_loss: 0.6856 - val_accuracy: 0.5320\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6856 - accuracy: 0.5238 - val_loss: 0.6846 - val_accuracy: 0.5335\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6847 - accuracy: 0.5246 - val_loss: 0.6847 - val_accuracy: 0.5345\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6842 - accuracy: 0.5250 - val_loss: 0.6845 - val_accuracy: 0.5341\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6829 - accuracy: 0.5265 - val_loss: 0.6839 - val_accuracy: 0.5337\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6819 - accuracy: 0.5268 - val_loss: 0.6841 - val_accuracy: 0.5341\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6812 - accuracy: 0.5276 - val_loss: 0.6845 - val_accuracy: 0.5340\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6804 - accuracy: 0.5284 - val_loss: 0.6845 - val_accuracy: 0.5336\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6797 - accuracy: 0.5289 - val_loss: 0.6845 - val_accuracy: 0.5342\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6792 - accuracy: 0.5293 - val_loss: 0.6846 - val_accuracy: 0.5345\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6785 - accuracy: 0.5299 - val_loss: 0.6850 - val_accuracy: 0.5343\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6780 - accuracy: 0.5300 - val_loss: 0.6853 - val_accuracy: 0.5340\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6776 - accuracy: 0.5303 - val_loss: 0.6848 - val_accuracy: 0.5345\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6768 - accuracy: 0.5310 - val_loss: 0.6858 - val_accuracy: 0.5334\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6765 - accuracy: 0.5313 - val_loss: 0.6860 - val_accuracy: 0.5339\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6765 - accuracy: 0.5319 - val_loss: 0.6861 - val_accuracy: 0.5332\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6758 - accuracy: 0.5321 - val_loss: 0.6868 - val_accuracy: 0.5341\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6753 - accuracy: 0.5321 - val_loss: 0.6859 - val_accuracy: 0.5337\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6748 - accuracy: 0.5326 - val_loss: 0.6878 - val_accuracy: 0.5334\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6743 - accuracy: 0.5329 - val_loss: 0.6863 - val_accuracy: 0.5341\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6743 - accuracy: 0.5330 - val_loss: 0.6861 - val_accuracy: 0.5342\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6737 - accuracy: 0.5335 - val_loss: 0.6879 - val_accuracy: 0.5334\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6730 - accuracy: 0.5342 - val_loss: 0.6872 - val_accuracy: 0.5331\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6732 - accuracy: 0.5340 - val_loss: 0.6877 - val_accuracy: 0.5334\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6722 - accuracy: 0.5345 - val_loss: 0.6891 - val_accuracy: 0.5328\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6732 - accuracy: 0.5344 - val_loss: 0.6890 - val_accuracy: 0.5333\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6721 - accuracy: 0.5346 - val_loss: 0.6878 - val_accuracy: 0.5335\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6714 - accuracy: 0.5352 - val_loss: 0.6891 - val_accuracy: 0.5335\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6710 - accuracy: 0.5356 - val_loss: 0.6882 - val_accuracy: 0.5325\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6705 - accuracy: 0.5356 - val_loss: 0.6896 - val_accuracy: 0.5335\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6705 - accuracy: 0.5358 - val_loss: 0.6888 - val_accuracy: 0.5324\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6702 - accuracy: 0.5363 - val_loss: 0.6898 - val_accuracy: 0.5327\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6700 - accuracy: 0.5364 - val_loss: 0.6881 - val_accuracy: 0.5331\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6695 - accuracy: 0.5365 - val_loss: 0.6898 - val_accuracy: 0.5333\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6695 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5333\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6692 - accuracy: 0.5369 - val_loss: 0.6894 - val_accuracy: 0.5338\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6685 - accuracy: 0.5374 - val_loss: 0.6917 - val_accuracy: 0.5334\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6684 - accuracy: 0.5375 - val_loss: 0.6898 - val_accuracy: 0.5334\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6680 - accuracy: 0.5377 - val_loss: 0.6920 - val_accuracy: 0.5331\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6685 - accuracy: 0.5376 - val_loss: 0.6916 - val_accuracy: 0.5330\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6677 - accuracy: 0.5378 - val_loss: 0.6926 - val_accuracy: 0.5321\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6672 - accuracy: 0.5382 - val_loss: 0.6923 - val_accuracy: 0.5322\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6674 - accuracy: 0.5381 - val_loss: 0.6932 - val_accuracy: 0.5335\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6671 - accuracy: 0.5384 - val_loss: 0.6913 - val_accuracy: 0.5330\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6670 - accuracy: 0.5388 - val_loss: 0.6940 - val_accuracy: 0.4949\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6667 - accuracy: 0.5335 - val_loss: 0.6923 - val_accuracy: 0.5338\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6667 - accuracy: 0.5383 - val_loss: 0.6922 - val_accuracy: 0.5336\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6664 - accuracy: 0.5386 - val_loss: 0.6932 - val_accuracy: 0.5326\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6660 - accuracy: 0.5389 - val_loss: 0.6931 - val_accuracy: 0.5329\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6656 - accuracy: 0.5394 - val_loss: 0.6935 - val_accuracy: 0.5330\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6657 - accuracy: 0.5391 - val_loss: 0.6932 - val_accuracy: 0.5331\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6657 - accuracy: 0.5394 - val_loss: 0.6948 - val_accuracy: 0.5329\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6653 - accuracy: 0.5396 - val_loss: 0.6933 - val_accuracy: 0.5334\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6650 - accuracy: 0.5396 - val_loss: 0.6966 - val_accuracy: 0.5332\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6649 - accuracy: 0.5397 - val_loss: 0.6955 - val_accuracy: 0.5326\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6645 - accuracy: 0.5399 - val_loss: 0.6942 - val_accuracy: 0.5330\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6646 - accuracy: 0.5401 - val_loss: 0.6950 - val_accuracy: 0.5332\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6644 - accuracy: 0.5400 - val_loss: 0.6948 - val_accuracy: 0.5325\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6642 - accuracy: 0.5402 - val_loss: 0.6971 - val_accuracy: 0.5324\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6647 - accuracy: 0.5401 - val_loss: 0.6963 - val_accuracy: 0.5331\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6647 - accuracy: 0.5404 - val_loss: 0.6961 - val_accuracy: 0.5334\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6639 - accuracy: 0.5405 - val_loss: 0.6949 - val_accuracy: 0.5330\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6637 - accuracy: 0.5405 - val_loss: 0.6972 - val_accuracy: 0.5332\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6640 - accuracy: 0.5404 - val_loss: 0.6962 - val_accuracy: 0.5328\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6637 - accuracy: 0.5410 - val_loss: 0.6973 - val_accuracy: 0.5323\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6634 - accuracy: 0.5407 - val_loss: 0.6983 - val_accuracy: 0.5321\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6636 - accuracy: 0.5336 - val_loss: 0.6966 - val_accuracy: 0.5330\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6631 - accuracy: 0.5411 - val_loss: 0.6959 - val_accuracy: 0.5329\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6628 - accuracy: 0.5411 - val_loss: 0.6981 - val_accuracy: 0.5333\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6632 - accuracy: 0.5409 - val_loss: 0.6974 - val_accuracy: 0.5338\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6629 - accuracy: 0.5410 - val_loss: 0.6970 - val_accuracy: 0.5331\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6627 - accuracy: 0.5414 - val_loss: 0.6959 - val_accuracy: 0.5326\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 61ms/step - loss: 0.6628 - accuracy: 0.5414 - val_loss: 0.6984 - val_accuracy: 0.5328\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6624 - accuracy: 0.5399 - val_loss: 0.6989 - val_accuracy: 0.5337\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6622 - accuracy: 0.5416 - val_loss: 0.6963 - val_accuracy: 0.5334\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6621 - accuracy: 0.5418 - val_loss: 0.6985 - val_accuracy: 0.5330\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6623 - accuracy: 0.5419 - val_loss: 0.6988 - val_accuracy: 0.5325\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6620 - accuracy: 0.5417 - val_loss: 0.6987 - val_accuracy: 0.5332\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6621 - accuracy: 0.5417 - val_loss: 0.6999 - val_accuracy: 0.5330\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6618 - accuracy: 0.5421 - val_loss: 0.7021 - val_accuracy: 0.5326\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6614 - accuracy: 0.5423 - val_loss: 0.7008 - val_accuracy: 0.5335\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6625 - accuracy: 0.5418 - val_loss: 0.6988 - val_accuracy: 0.5330\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6614 - accuracy: 0.5421 - val_loss: 0.6983 - val_accuracy: 0.5333\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 8s 107ms/step - loss: 0.6613 - accuracy: 0.5421 - val_loss: 0.7000 - val_accuracy: 0.5329\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6614 - accuracy: 0.5421 - val_loss: 0.7004 - val_accuracy: 0.5331\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6613 - accuracy: 0.5422 - val_loss: 0.6977 - val_accuracy: 0.5334\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6610 - accuracy: 0.5425 - val_loss: 0.6999 - val_accuracy: 0.5329\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6607 - accuracy: 0.5425 - val_loss: 0.7009 - val_accuracy: 0.5326\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6609 - accuracy: 0.5427 - val_loss: 0.6998 - val_accuracy: 0.5327\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6609 - accuracy: 0.5393 - val_loss: 0.6996 - val_accuracy: 0.5330\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6611 - accuracy: 0.5424 - val_loss: 0.6997 - val_accuracy: 0.5329\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6609 - accuracy: 0.5427 - val_loss: 0.6997 - val_accuracy: 0.5329\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6603 - accuracy: 0.5430 - val_loss: 0.7008 - val_accuracy: 0.5326\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6609 - accuracy: 0.5424 - val_loss: 0.6991 - val_accuracy: 0.5332\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6608 - accuracy: 0.5426 - val_loss: 0.7000 - val_accuracy: 0.5329\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6607 - accuracy: 0.5427 - val_loss: 0.7011 - val_accuracy: 0.5332\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6601 - accuracy: 0.5432 - val_loss: 0.7022 - val_accuracy: 0.5321\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6600 - accuracy: 0.5432 - val_loss: 0.7012 - val_accuracy: 0.5331\n",
            "19/19 [==============================] - 1s 14ms/step - loss: 0.7037 - accuracy: 0.5321\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 5s 53ms/step - loss: 0.6903 - accuracy: 0.5228 - val_loss: 0.6875 - val_accuracy: 0.5099\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6878 - accuracy: 0.5259 - val_loss: 0.6862 - val_accuracy: 0.5117\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6864 - accuracy: 0.5273 - val_loss: 0.6858 - val_accuracy: 0.5130\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6852 - accuracy: 0.5292 - val_loss: 0.6856 - val_accuracy: 0.5125\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6841 - accuracy: 0.5299 - val_loss: 0.6854 - val_accuracy: 0.5138\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6831 - accuracy: 0.5306 - val_loss: 0.6866 - val_accuracy: 0.5144\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6825 - accuracy: 0.5321 - val_loss: 0.6857 - val_accuracy: 0.5142\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6815 - accuracy: 0.5327 - val_loss: 0.6858 - val_accuracy: 0.5141\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6808 - accuracy: 0.5334 - val_loss: 0.6855 - val_accuracy: 0.5147\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6805 - accuracy: 0.5334 - val_loss: 0.6863 - val_accuracy: 0.5127\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6793 - accuracy: 0.5348 - val_loss: 0.6864 - val_accuracy: 0.5128\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6788 - accuracy: 0.5350 - val_loss: 0.6858 - val_accuracy: 0.5130\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6781 - accuracy: 0.5356 - val_loss: 0.6869 - val_accuracy: 0.5116\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6782 - accuracy: 0.5356 - val_loss: 0.6862 - val_accuracy: 0.5129\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6774 - accuracy: 0.5362 - val_loss: 0.6875 - val_accuracy: 0.5111\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6770 - accuracy: 0.5365 - val_loss: 0.6871 - val_accuracy: 0.5114\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6763 - accuracy: 0.5367 - val_loss: 0.6867 - val_accuracy: 0.5124\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6756 - accuracy: 0.5374 - val_loss: 0.6880 - val_accuracy: 0.5120\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 3s 48ms/step - loss: 0.6755 - accuracy: 0.5374 - val_loss: 0.6875 - val_accuracy: 0.5121\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6751 - accuracy: 0.5376 - val_loss: 0.6878 - val_accuracy: 0.5126\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6748 - accuracy: 0.5378 - val_loss: 0.6878 - val_accuracy: 0.5125\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6739 - accuracy: 0.5385 - val_loss: 0.6876 - val_accuracy: 0.5124\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6742 - accuracy: 0.5383 - val_loss: 0.6872 - val_accuracy: 0.5116\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6735 - accuracy: 0.5390 - val_loss: 0.6884 - val_accuracy: 0.5110\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6729 - accuracy: 0.5395 - val_loss: 0.6889 - val_accuracy: 0.5121\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6732 - accuracy: 0.5387 - val_loss: 0.6892 - val_accuracy: 0.5098\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 3s 47ms/step - loss: 0.6724 - accuracy: 0.5397 - val_loss: 0.6899 - val_accuracy: 0.5127\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6724 - accuracy: 0.5398 - val_loss: 0.6926 - val_accuracy: 0.5136\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6718 - accuracy: 0.5396 - val_loss: 0.6880 - val_accuracy: 0.5115\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6714 - accuracy: 0.5403 - val_loss: 0.6906 - val_accuracy: 0.5131\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6715 - accuracy: 0.5402 - val_loss: 0.6893 - val_accuracy: 0.5126\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6706 - accuracy: 0.5407 - val_loss: 0.6915 - val_accuracy: 0.5108\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6706 - accuracy: 0.5410 - val_loss: 0.6900 - val_accuracy: 0.5116\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6702 - accuracy: 0.5413 - val_loss: 0.6914 - val_accuracy: 0.5129\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6700 - accuracy: 0.5412 - val_loss: 0.6908 - val_accuracy: 0.5119\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6694 - accuracy: 0.5417 - val_loss: 0.6911 - val_accuracy: 0.5118\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6693 - accuracy: 0.5421 - val_loss: 0.6917 - val_accuracy: 0.5127\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6691 - accuracy: 0.5420 - val_loss: 0.6932 - val_accuracy: 0.5110\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6693 - accuracy: 0.5417 - val_loss: 0.6911 - val_accuracy: 0.5116\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6688 - accuracy: 0.5421 - val_loss: 0.6905 - val_accuracy: 0.5127\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6683 - accuracy: 0.5425 - val_loss: 0.6916 - val_accuracy: 0.5123\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6680 - accuracy: 0.5429 - val_loss: 0.6926 - val_accuracy: 0.5123\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6677 - accuracy: 0.5431 - val_loss: 0.6919 - val_accuracy: 0.5114\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6677 - accuracy: 0.5429 - val_loss: 0.6936 - val_accuracy: 0.5127\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6670 - accuracy: 0.5436 - val_loss: 0.6932 - val_accuracy: 0.5118\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6671 - accuracy: 0.5435 - val_loss: 0.6934 - val_accuracy: 0.5117\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6671 - accuracy: 0.5434 - val_loss: 0.6936 - val_accuracy: 0.5120\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6665 - accuracy: 0.5438 - val_loss: 0.6938 - val_accuracy: 0.5121\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6665 - accuracy: 0.5441 - val_loss: 0.6960 - val_accuracy: 0.5125\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6669 - accuracy: 0.5439 - val_loss: 0.6941 - val_accuracy: 0.5128\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6667 - accuracy: 0.5438 - val_loss: 0.6934 - val_accuracy: 0.5116\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6657 - accuracy: 0.5443 - val_loss: 0.6944 - val_accuracy: 0.5122\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6654 - accuracy: 0.5446 - val_loss: 0.6948 - val_accuracy: 0.5122\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6652 - accuracy: 0.5448 - val_loss: 0.6946 - val_accuracy: 0.5124\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6654 - accuracy: 0.5447 - val_loss: 0.6944 - val_accuracy: 0.5124\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6650 - accuracy: 0.5450 - val_loss: 0.6961 - val_accuracy: 0.5112\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6647 - accuracy: 0.5451 - val_loss: 0.6962 - val_accuracy: 0.5127\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6649 - accuracy: 0.5453 - val_loss: 0.6975 - val_accuracy: 0.5128\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6650 - accuracy: 0.5449 - val_loss: 0.6944 - val_accuracy: 0.5126\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6645 - accuracy: 0.5453 - val_loss: 0.6956 - val_accuracy: 0.5125\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6644 - accuracy: 0.5453 - val_loss: 0.6959 - val_accuracy: 0.5120\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6638 - accuracy: 0.5456 - val_loss: 0.6975 - val_accuracy: 0.5117\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6644 - accuracy: 0.5453 - val_loss: 0.6961 - val_accuracy: 0.5131\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6642 - accuracy: 0.5456 - val_loss: 0.6968 - val_accuracy: 0.5126\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6634 - accuracy: 0.5461 - val_loss: 0.6970 - val_accuracy: 0.5119\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6637 - accuracy: 0.5459 - val_loss: 0.6976 - val_accuracy: 0.5114\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6634 - accuracy: 0.5462 - val_loss: 0.6981 - val_accuracy: 0.5125\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6637 - accuracy: 0.5462 - val_loss: 0.6966 - val_accuracy: 0.5117\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6631 - accuracy: 0.5463 - val_loss: 0.6987 - val_accuracy: 0.5124\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6631 - accuracy: 0.5462 - val_loss: 0.6976 - val_accuracy: 0.5119\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6628 - accuracy: 0.5466 - val_loss: 0.6971 - val_accuracy: 0.5122\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6628 - accuracy: 0.5464 - val_loss: 0.6985 - val_accuracy: 0.5119\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6625 - accuracy: 0.5465 - val_loss: 0.6986 - val_accuracy: 0.5119\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6625 - accuracy: 0.5467 - val_loss: 0.6994 - val_accuracy: 0.5126\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6621 - accuracy: 0.5469 - val_loss: 0.6985 - val_accuracy: 0.5116\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6619 - accuracy: 0.5470 - val_loss: 0.6985 - val_accuracy: 0.5119\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6623 - accuracy: 0.5434 - val_loss: 0.7003 - val_accuracy: 0.5115\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6620 - accuracy: 0.5471 - val_loss: 0.6992 - val_accuracy: 0.5115\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6618 - accuracy: 0.5470 - val_loss: 0.6991 - val_accuracy: 0.5118\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6617 - accuracy: 0.5473 - val_loss: 0.6991 - val_accuracy: 0.5112\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6618 - accuracy: 0.5470 - val_loss: 0.6994 - val_accuracy: 0.5120\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6617 - accuracy: 0.5472 - val_loss: 0.6987 - val_accuracy: 0.5115\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6616 - accuracy: 0.5473 - val_loss: 0.7007 - val_accuracy: 0.5121\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6619 - accuracy: 0.5471 - val_loss: 0.6967 - val_accuracy: 0.5117\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6613 - accuracy: 0.5469 - val_loss: 0.6996 - val_accuracy: 0.5121\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6612 - accuracy: 0.5477 - val_loss: 0.7006 - val_accuracy: 0.5118\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6613 - accuracy: 0.5472 - val_loss: 0.7011 - val_accuracy: 0.5123\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6608 - accuracy: 0.5477 - val_loss: 0.7010 - val_accuracy: 0.5117\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6609 - accuracy: 0.5476 - val_loss: 0.7002 - val_accuracy: 0.5124\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6610 - accuracy: 0.5476 - val_loss: 0.7012 - val_accuracy: 0.5118\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6609 - accuracy: 0.5479 - val_loss: 0.7002 - val_accuracy: 0.5118\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 4s 48ms/step - loss: 0.6609 - accuracy: 0.5479 - val_loss: 0.7010 - val_accuracy: 0.5118\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6607 - accuracy: 0.5481 - val_loss: 0.7013 - val_accuracy: 0.5123\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6605 - accuracy: 0.5481 - val_loss: 0.7019 - val_accuracy: 0.5122\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6606 - accuracy: 0.5480 - val_loss: 0.7024 - val_accuracy: 0.5120\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6604 - accuracy: 0.5479 - val_loss: 0.7015 - val_accuracy: 0.5119\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6601 - accuracy: 0.5484 - val_loss: 0.7032 - val_accuracy: 0.5123\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6603 - accuracy: 0.5479 - val_loss: 0.7026 - val_accuracy: 0.5114\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6602 - accuracy: 0.5483 - val_loss: 0.7021 - val_accuracy: 0.5120\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6604 - accuracy: 0.5481 - val_loss: 0.7021 - val_accuracy: 0.5121\n",
            "19/19 [==============================] - 1s 15ms/step - loss: 0.7021 - accuracy: 0.5120\n",
            "Best accuracy for dataset 0: 0.5428648591041565\n",
            "2308\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 5s 53ms/step - loss: 0.6903 - accuracy: 0.5022 - val_loss: 0.6885 - val_accuracy: 0.5385\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6878 - accuracy: 0.5182 - val_loss: 0.6870 - val_accuracy: 0.5424\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6866 - accuracy: 0.5197 - val_loss: 0.6865 - val_accuracy: 0.5428\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6851 - accuracy: 0.5211 - val_loss: 0.6858 - val_accuracy: 0.5435\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6841 - accuracy: 0.5227 - val_loss: 0.6854 - val_accuracy: 0.5438\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6834 - accuracy: 0.5229 - val_loss: 0.6851 - val_accuracy: 0.5445\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6826 - accuracy: 0.5239 - val_loss: 0.6849 - val_accuracy: 0.5447\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6812 - accuracy: 0.5251 - val_loss: 0.6845 - val_accuracy: 0.5446\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6807 - accuracy: 0.5256 - val_loss: 0.6849 - val_accuracy: 0.5443\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6802 - accuracy: 0.5262 - val_loss: 0.6848 - val_accuracy: 0.5445\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6795 - accuracy: 0.5267 - val_loss: 0.6844 - val_accuracy: 0.5447\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6791 - accuracy: 0.5267 - val_loss: 0.6847 - val_accuracy: 0.5444\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6784 - accuracy: 0.5270 - val_loss: 0.6849 - val_accuracy: 0.5452\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6779 - accuracy: 0.5278 - val_loss: 0.6850 - val_accuracy: 0.5440\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6774 - accuracy: 0.5280 - val_loss: 0.6855 - val_accuracy: 0.5448\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6771 - accuracy: 0.5286 - val_loss: 0.6856 - val_accuracy: 0.5446\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6762 - accuracy: 0.5290 - val_loss: 0.6863 - val_accuracy: 0.5447\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6756 - accuracy: 0.5297 - val_loss: 0.6865 - val_accuracy: 0.5437\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6756 - accuracy: 0.5294 - val_loss: 0.6857 - val_accuracy: 0.5438\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6746 - accuracy: 0.5306 - val_loss: 0.6862 - val_accuracy: 0.5445\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6746 - accuracy: 0.5302 - val_loss: 0.6865 - val_accuracy: 0.5433\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6744 - accuracy: 0.5307 - val_loss: 0.6870 - val_accuracy: 0.5433\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6734 - accuracy: 0.5310 - val_loss: 0.6886 - val_accuracy: 0.5436\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6736 - accuracy: 0.5311 - val_loss: 0.6884 - val_accuracy: 0.5422\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6729 - accuracy: 0.5315 - val_loss: 0.6881 - val_accuracy: 0.5438\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6726 - accuracy: 0.5317 - val_loss: 0.6874 - val_accuracy: 0.5421\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6723 - accuracy: 0.5319 - val_loss: 0.6885 - val_accuracy: 0.5422\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6716 - accuracy: 0.5327 - val_loss: 0.6895 - val_accuracy: 0.5431\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6714 - accuracy: 0.5326 - val_loss: 0.6888 - val_accuracy: 0.5431\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6709 - accuracy: 0.5329 - val_loss: 0.6885 - val_accuracy: 0.5421\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6706 - accuracy: 0.5332 - val_loss: 0.6893 - val_accuracy: 0.5424\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 5s 76ms/step - loss: 0.6697 - accuracy: 0.5339 - val_loss: 0.6907 - val_accuracy: 0.5430\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6698 - accuracy: 0.5339 - val_loss: 0.6899 - val_accuracy: 0.5423\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6697 - accuracy: 0.5338 - val_loss: 0.6898 - val_accuracy: 0.5427\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6690 - accuracy: 0.5344 - val_loss: 0.6909 - val_accuracy: 0.5424\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6691 - accuracy: 0.5346 - val_loss: 0.6920 - val_accuracy: 0.5430\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6691 - accuracy: 0.5346 - val_loss: 0.6912 - val_accuracy: 0.5425\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6681 - accuracy: 0.5353 - val_loss: 0.6945 - val_accuracy: 0.5431\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6685 - accuracy: 0.5347 - val_loss: 0.6929 - val_accuracy: 0.5436\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6684 - accuracy: 0.5352 - val_loss: 0.6918 - val_accuracy: 0.5427\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6673 - accuracy: 0.5356 - val_loss: 0.6927 - val_accuracy: 0.5419\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6673 - accuracy: 0.5356 - val_loss: 0.6924 - val_accuracy: 0.5427\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6671 - accuracy: 0.5359 - val_loss: 0.6926 - val_accuracy: 0.5415\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6668 - accuracy: 0.5361 - val_loss: 0.6941 - val_accuracy: 0.5427\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6669 - accuracy: 0.5357 - val_loss: 0.6929 - val_accuracy: 0.5426\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6672 - accuracy: 0.5359 - val_loss: 0.6932 - val_accuracy: 0.5420\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6664 - accuracy: 0.5365 - val_loss: 0.6942 - val_accuracy: 0.5418\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6658 - accuracy: 0.5368 - val_loss: 0.6947 - val_accuracy: 0.5422\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6659 - accuracy: 0.5366 - val_loss: 0.6952 - val_accuracy: 0.5423\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6658 - accuracy: 0.5368 - val_loss: 0.6943 - val_accuracy: 0.5431\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6652 - accuracy: 0.5373 - val_loss: 0.6964 - val_accuracy: 0.5421\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6656 - accuracy: 0.5369 - val_loss: 0.6963 - val_accuracy: 0.5417\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6652 - accuracy: 0.5375 - val_loss: 0.6974 - val_accuracy: 0.5424\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6650 - accuracy: 0.5374 - val_loss: 0.6944 - val_accuracy: 0.5414\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6649 - accuracy: 0.5375 - val_loss: 0.6960 - val_accuracy: 0.5417\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6652 - accuracy: 0.5375 - val_loss: 0.6945 - val_accuracy: 0.5414\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6648 - accuracy: 0.5367 - val_loss: 0.6947 - val_accuracy: 0.5417\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6645 - accuracy: 0.5380 - val_loss: 0.6969 - val_accuracy: 0.5428\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6644 - accuracy: 0.5379 - val_loss: 0.6954 - val_accuracy: 0.5421\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6642 - accuracy: 0.5381 - val_loss: 0.6965 - val_accuracy: 0.5420\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6640 - accuracy: 0.5379 - val_loss: 0.6964 - val_accuracy: 0.5425\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6645 - accuracy: 0.5377 - val_loss: 0.6977 - val_accuracy: 0.5423\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6636 - accuracy: 0.5384 - val_loss: 0.6974 - val_accuracy: 0.5419\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6631 - accuracy: 0.5387 - val_loss: 0.6966 - val_accuracy: 0.5418\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6636 - accuracy: 0.5388 - val_loss: 0.6972 - val_accuracy: 0.5421\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6631 - accuracy: 0.5387 - val_loss: 0.6973 - val_accuracy: 0.5414\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6634 - accuracy: 0.5387 - val_loss: 0.7001 - val_accuracy: 0.5419\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6634 - accuracy: 0.5386 - val_loss: 0.7002 - val_accuracy: 0.5417\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6631 - accuracy: 0.5387 - val_loss: 0.6986 - val_accuracy: 0.5416\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6627 - accuracy: 0.5391 - val_loss: 0.6981 - val_accuracy: 0.5417\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6625 - accuracy: 0.5394 - val_loss: 0.6992 - val_accuracy: 0.5419\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6623 - accuracy: 0.5393 - val_loss: 0.6999 - val_accuracy: 0.5416\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6625 - accuracy: 0.5394 - val_loss: 0.7006 - val_accuracy: 0.5419\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6625 - accuracy: 0.5393 - val_loss: 0.7008 - val_accuracy: 0.5419\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6616 - accuracy: 0.5396 - val_loss: 0.7000 - val_accuracy: 0.5418\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6633 - accuracy: 0.5392 - val_loss: 0.6962 - val_accuracy: 0.5415\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6620 - accuracy: 0.5399 - val_loss: 0.7003 - val_accuracy: 0.5416\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6617 - accuracy: 0.5395 - val_loss: 0.7007 - val_accuracy: 0.5415\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6618 - accuracy: 0.5396 - val_loss: 0.7018 - val_accuracy: 0.5418\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6619 - accuracy: 0.5397 - val_loss: 0.7035 - val_accuracy: 0.5425\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6620 - accuracy: 0.5395 - val_loss: 0.7017 - val_accuracy: 0.5420\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6618 - accuracy: 0.5399 - val_loss: 0.7001 - val_accuracy: 0.5415\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6613 - accuracy: 0.5400 - val_loss: 0.7048 - val_accuracy: 0.5424\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6613 - accuracy: 0.5401 - val_loss: 0.7022 - val_accuracy: 0.5414\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6608 - accuracy: 0.5403 - val_loss: 0.7010 - val_accuracy: 0.5410\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6608 - accuracy: 0.5404 - val_loss: 0.7006 - val_accuracy: 0.5419\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6609 - accuracy: 0.5404 - val_loss: 0.7024 - val_accuracy: 0.5420\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6606 - accuracy: 0.5404 - val_loss: 0.7038 - val_accuracy: 0.5422\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6606 - accuracy: 0.5405 - val_loss: 0.7016 - val_accuracy: 0.5412\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6607 - accuracy: 0.5404 - val_loss: 0.7009 - val_accuracy: 0.5416\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6607 - accuracy: 0.5404 - val_loss: 0.7038 - val_accuracy: 0.5422\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6606 - accuracy: 0.5404 - val_loss: 0.7020 - val_accuracy: 0.5420\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6608 - accuracy: 0.5405 - val_loss: 0.7023 - val_accuracy: 0.5423\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6599 - accuracy: 0.5396 - val_loss: 0.7033 - val_accuracy: 0.5417\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6611 - accuracy: 0.5347 - val_loss: 0.7019 - val_accuracy: 0.5420\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6602 - accuracy: 0.5407 - val_loss: 0.7019 - val_accuracy: 0.5417\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6601 - accuracy: 0.5411 - val_loss: 0.7023 - val_accuracy: 0.5417\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6603 - accuracy: 0.5409 - val_loss: 0.7072 - val_accuracy: 0.5417\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6599 - accuracy: 0.5381 - val_loss: 0.7039 - val_accuracy: 0.5421\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6598 - accuracy: 0.5413 - val_loss: 0.7026 - val_accuracy: 0.5416\n",
            "19/19 [==============================] - 1s 14ms/step - loss: 0.7063 - accuracy: 0.5401\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 8s 57ms/step - loss: 0.6908 - accuracy: 0.5153 - val_loss: 0.6877 - val_accuracy: 0.5343\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 8s 108ms/step - loss: 0.6882 - accuracy: 0.5190 - val_loss: 0.6860 - val_accuracy: 0.5363\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6867 - accuracy: 0.5210 - val_loss: 0.6861 - val_accuracy: 0.5386\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6856 - accuracy: 0.5216 - val_loss: 0.6850 - val_accuracy: 0.5389\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6847 - accuracy: 0.5230 - val_loss: 0.6851 - val_accuracy: 0.5396\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6837 - accuracy: 0.5235 - val_loss: 0.6842 - val_accuracy: 0.5395\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6828 - accuracy: 0.5249 - val_loss: 0.6845 - val_accuracy: 0.5390\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6821 - accuracy: 0.5256 - val_loss: 0.6853 - val_accuracy: 0.5393\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6820 - accuracy: 0.5258 - val_loss: 0.6851 - val_accuracy: 0.5391\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6812 - accuracy: 0.5259 - val_loss: 0.6850 - val_accuracy: 0.5394\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6802 - accuracy: 0.5269 - val_loss: 0.6852 - val_accuracy: 0.5397\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6795 - accuracy: 0.5280 - val_loss: 0.6847 - val_accuracy: 0.5384\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6792 - accuracy: 0.5281 - val_loss: 0.6847 - val_accuracy: 0.5392\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6784 - accuracy: 0.5288 - val_loss: 0.6855 - val_accuracy: 0.5394\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6780 - accuracy: 0.5292 - val_loss: 0.6859 - val_accuracy: 0.5393\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6774 - accuracy: 0.5293 - val_loss: 0.6856 - val_accuracy: 0.5392\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6772 - accuracy: 0.5294 - val_loss: 0.6860 - val_accuracy: 0.5389\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6762 - accuracy: 0.5302 - val_loss: 0.6861 - val_accuracy: 0.5395\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6762 - accuracy: 0.5304 - val_loss: 0.6869 - val_accuracy: 0.5391\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6757 - accuracy: 0.5310 - val_loss: 0.6864 - val_accuracy: 0.5385\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6749 - accuracy: 0.5316 - val_loss: 0.6878 - val_accuracy: 0.5391\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6747 - accuracy: 0.5315 - val_loss: 0.6869 - val_accuracy: 0.5384\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6742 - accuracy: 0.5318 - val_loss: 0.6870 - val_accuracy: 0.5390\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6734 - accuracy: 0.5326 - val_loss: 0.6893 - val_accuracy: 0.5389\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6735 - accuracy: 0.5322 - val_loss: 0.6896 - val_accuracy: 0.5390\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6731 - accuracy: 0.5328 - val_loss: 0.6883 - val_accuracy: 0.5372\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6725 - accuracy: 0.5335 - val_loss: 0.6879 - val_accuracy: 0.5373\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6720 - accuracy: 0.5334 - val_loss: 0.6913 - val_accuracy: 0.5386\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6721 - accuracy: 0.5336 - val_loss: 0.6897 - val_accuracy: 0.5385\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6719 - accuracy: 0.5338 - val_loss: 0.6892 - val_accuracy: 0.5381\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6711 - accuracy: 0.5344 - val_loss: 0.6903 - val_accuracy: 0.5376\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6707 - accuracy: 0.5346 - val_loss: 0.6914 - val_accuracy: 0.5393\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6708 - accuracy: 0.5343 - val_loss: 0.6903 - val_accuracy: 0.5375\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6701 - accuracy: 0.5351 - val_loss: 0.6906 - val_accuracy: 0.5383\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6703 - accuracy: 0.5348 - val_loss: 0.6904 - val_accuracy: 0.5388\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6695 - accuracy: 0.5352 - val_loss: 0.6923 - val_accuracy: 0.5386\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6694 - accuracy: 0.5356 - val_loss: 0.6913 - val_accuracy: 0.5378\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6693 - accuracy: 0.5355 - val_loss: 0.6905 - val_accuracy: 0.5387\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6691 - accuracy: 0.5358 - val_loss: 0.6912 - val_accuracy: 0.5368\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6687 - accuracy: 0.5361 - val_loss: 0.6914 - val_accuracy: 0.5384\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6680 - accuracy: 0.5364 - val_loss: 0.6914 - val_accuracy: 0.5374\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6675 - accuracy: 0.5368 - val_loss: 0.6929 - val_accuracy: 0.5377\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6681 - accuracy: 0.5367 - val_loss: 0.6924 - val_accuracy: 0.5382\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6681 - accuracy: 0.5369 - val_loss: 0.6925 - val_accuracy: 0.5385\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6673 - accuracy: 0.5373 - val_loss: 0.6929 - val_accuracy: 0.5378\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6675 - accuracy: 0.5369 - val_loss: 0.6927 - val_accuracy: 0.5382\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6675 - accuracy: 0.5371 - val_loss: 0.6938 - val_accuracy: 0.5377\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6671 - accuracy: 0.5373 - val_loss: 0.6955 - val_accuracy: 0.5386\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6669 - accuracy: 0.5373 - val_loss: 0.6931 - val_accuracy: 0.5374\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6664 - accuracy: 0.5374 - val_loss: 0.6937 - val_accuracy: 0.5376\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6661 - accuracy: 0.5378 - val_loss: 0.6950 - val_accuracy: 0.5375\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6657 - accuracy: 0.5381 - val_loss: 0.6972 - val_accuracy: 0.5385\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6662 - accuracy: 0.5379 - val_loss: 0.6961 - val_accuracy: 0.5378\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6657 - accuracy: 0.5380 - val_loss: 0.6954 - val_accuracy: 0.5366\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6652 - accuracy: 0.5384 - val_loss: 0.6966 - val_accuracy: 0.5377\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6657 - accuracy: 0.5384 - val_loss: 0.6974 - val_accuracy: 0.5379\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6655 - accuracy: 0.5383 - val_loss: 0.6964 - val_accuracy: 0.5371\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6652 - accuracy: 0.5384 - val_loss: 0.6967 - val_accuracy: 0.5378\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6646 - accuracy: 0.5391 - val_loss: 0.6969 - val_accuracy: 0.5374\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6645 - accuracy: 0.5389 - val_loss: 0.6963 - val_accuracy: 0.5377\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6641 - accuracy: 0.5394 - val_loss: 0.6965 - val_accuracy: 0.5379\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6646 - accuracy: 0.5389 - val_loss: 0.6970 - val_accuracy: 0.5379\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6641 - accuracy: 0.5395 - val_loss: 0.6974 - val_accuracy: 0.5371\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6642 - accuracy: 0.5390 - val_loss: 0.6960 - val_accuracy: 0.5377\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6638 - accuracy: 0.5396 - val_loss: 0.6980 - val_accuracy: 0.5380\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6642 - accuracy: 0.5394 - val_loss: 0.6964 - val_accuracy: 0.5375\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6634 - accuracy: 0.5397 - val_loss: 0.6978 - val_accuracy: 0.5369\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6636 - accuracy: 0.5396 - val_loss: 0.6977 - val_accuracy: 0.5373\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6636 - accuracy: 0.5397 - val_loss: 0.6984 - val_accuracy: 0.5380\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6634 - accuracy: 0.5396 - val_loss: 0.6971 - val_accuracy: 0.5374\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6634 - accuracy: 0.5396 - val_loss: 0.6975 - val_accuracy: 0.5377\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6630 - accuracy: 0.5400 - val_loss: 0.6974 - val_accuracy: 0.5368\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6627 - accuracy: 0.5404 - val_loss: 0.6999 - val_accuracy: 0.5363\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6634 - accuracy: 0.5401 - val_loss: 0.6996 - val_accuracy: 0.5375\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6632 - accuracy: 0.5301 - val_loss: 0.7001 - val_accuracy: 0.5383\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6626 - accuracy: 0.5403 - val_loss: 0.6985 - val_accuracy: 0.5375\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6626 - accuracy: 0.5404 - val_loss: 0.6985 - val_accuracy: 0.5375\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6622 - accuracy: 0.5404 - val_loss: 0.6997 - val_accuracy: 0.5378\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6620 - accuracy: 0.5407 - val_loss: 0.7008 - val_accuracy: 0.5381\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6617 - accuracy: 0.5408 - val_loss: 0.7013 - val_accuracy: 0.5375\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6620 - accuracy: 0.5410 - val_loss: 0.7013 - val_accuracy: 0.5362\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6618 - accuracy: 0.5410 - val_loss: 0.6999 - val_accuracy: 0.5382\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6617 - accuracy: 0.5410 - val_loss: 0.7020 - val_accuracy: 0.5372\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6622 - accuracy: 0.5408 - val_loss: 0.7008 - val_accuracy: 0.5371\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6615 - accuracy: 0.5411 - val_loss: 0.7015 - val_accuracy: 0.5373\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6615 - accuracy: 0.5411 - val_loss: 0.6994 - val_accuracy: 0.5368\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6616 - accuracy: 0.5413 - val_loss: 0.7010 - val_accuracy: 0.5375\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6616 - accuracy: 0.5411 - val_loss: 0.6999 - val_accuracy: 0.5375\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6613 - accuracy: 0.5411 - val_loss: 0.7004 - val_accuracy: 0.5371\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6613 - accuracy: 0.5414 - val_loss: 0.7009 - val_accuracy: 0.5378\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6612 - accuracy: 0.5412 - val_loss: 0.7004 - val_accuracy: 0.5368\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6616 - accuracy: 0.5412 - val_loss: 0.7005 - val_accuracy: 0.5380\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6612 - accuracy: 0.5414 - val_loss: 0.7018 - val_accuracy: 0.5376\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6609 - accuracy: 0.5416 - val_loss: 0.7024 - val_accuracy: 0.5373\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6610 - accuracy: 0.5414 - val_loss: 0.7013 - val_accuracy: 0.5374\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6605 - accuracy: 0.5416 - val_loss: 0.7018 - val_accuracy: 0.5370\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6611 - accuracy: 0.5414 - val_loss: 0.7014 - val_accuracy: 0.5365\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6610 - accuracy: 0.5415 - val_loss: 0.7028 - val_accuracy: 0.5373\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6606 - accuracy: 0.5418 - val_loss: 0.7014 - val_accuracy: 0.5377\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6609 - accuracy: 0.5418 - val_loss: 0.7033 - val_accuracy: 0.5377\n",
            "19/19 [==============================] - 1s 16ms/step - loss: 0.7050 - accuracy: 0.5371\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 76ms/step - loss: 0.6900 - accuracy: 0.5194 - val_loss: 0.6891 - val_accuracy: 0.4965\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6876 - accuracy: 0.5305 - val_loss: 0.6886 - val_accuracy: 0.4975\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6858 - accuracy: 0.5313 - val_loss: 0.6881 - val_accuracy: 0.4974\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6842 - accuracy: 0.5333 - val_loss: 0.6886 - val_accuracy: 0.4973\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6834 - accuracy: 0.5345 - val_loss: 0.6886 - val_accuracy: 0.4978\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6823 - accuracy: 0.5351 - val_loss: 0.6881 - val_accuracy: 0.4991\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6816 - accuracy: 0.5359 - val_loss: 0.6889 - val_accuracy: 0.4980\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6807 - accuracy: 0.5366 - val_loss: 0.6881 - val_accuracy: 0.4989\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6800 - accuracy: 0.5376 - val_loss: 0.6887 - val_accuracy: 0.5002\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6795 - accuracy: 0.5377 - val_loss: 0.6885 - val_accuracy: 0.4983\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6786 - accuracy: 0.5386 - val_loss: 0.6892 - val_accuracy: 0.4987\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6780 - accuracy: 0.5387 - val_loss: 0.6894 - val_accuracy: 0.4968\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6776 - accuracy: 0.5396 - val_loss: 0.6923 - val_accuracy: 0.4972\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6774 - accuracy: 0.5398 - val_loss: 0.6891 - val_accuracy: 0.4990\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6763 - accuracy: 0.5401 - val_loss: 0.6917 - val_accuracy: 0.4988\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6758 - accuracy: 0.5407 - val_loss: 0.6912 - val_accuracy: 0.4975\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6752 - accuracy: 0.5410 - val_loss: 0.6905 - val_accuracy: 0.4970\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6750 - accuracy: 0.5411 - val_loss: 0.6922 - val_accuracy: 0.4996\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6745 - accuracy: 0.5413 - val_loss: 0.6910 - val_accuracy: 0.4980\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6739 - accuracy: 0.5417 - val_loss: 0.6914 - val_accuracy: 0.4977\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6734 - accuracy: 0.5423 - val_loss: 0.6925 - val_accuracy: 0.4983\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6731 - accuracy: 0.5425 - val_loss: 0.6929 - val_accuracy: 0.4976\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6728 - accuracy: 0.5426 - val_loss: 0.6925 - val_accuracy: 0.4981\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 4s 49ms/step - loss: 0.6728 - accuracy: 0.5423 - val_loss: 0.6932 - val_accuracy: 0.4982\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6720 - accuracy: 0.5431 - val_loss: 0.6933 - val_accuracy: 0.4975\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6715 - accuracy: 0.5432 - val_loss: 0.6935 - val_accuracy: 0.4979\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6712 - accuracy: 0.5435 - val_loss: 0.6949 - val_accuracy: 0.4986\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 8s 105ms/step - loss: 0.6711 - accuracy: 0.5434 - val_loss: 0.6941 - val_accuracy: 0.4972\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6706 - accuracy: 0.5440 - val_loss: 0.6975 - val_accuracy: 0.4992\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6708 - accuracy: 0.5439 - val_loss: 0.6949 - val_accuracy: 0.4975\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6704 - accuracy: 0.5442 - val_loss: 0.6957 - val_accuracy: 0.4976\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 5s 76ms/step - loss: 0.6703 - accuracy: 0.5442 - val_loss: 0.6952 - val_accuracy: 0.4976\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6692 - accuracy: 0.5448 - val_loss: 0.6968 - val_accuracy: 0.4967\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6693 - accuracy: 0.5450 - val_loss: 0.6958 - val_accuracy: 0.4969\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6685 - accuracy: 0.5452 - val_loss: 0.6976 - val_accuracy: 0.4975\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6686 - accuracy: 0.5453 - val_loss: 0.6981 - val_accuracy: 0.4975\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6685 - accuracy: 0.5455 - val_loss: 0.6999 - val_accuracy: 0.4981\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6682 - accuracy: 0.5459 - val_loss: 0.6990 - val_accuracy: 0.4974\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6680 - accuracy: 0.5458 - val_loss: 0.7009 - val_accuracy: 0.4977\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6676 - accuracy: 0.5461 - val_loss: 0.6987 - val_accuracy: 0.4974\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6675 - accuracy: 0.5459 - val_loss: 0.6984 - val_accuracy: 0.4969\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6673 - accuracy: 0.5462 - val_loss: 0.6994 - val_accuracy: 0.4986\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6669 - accuracy: 0.5464 - val_loss: 0.7017 - val_accuracy: 0.4985\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6668 - accuracy: 0.5468 - val_loss: 0.7012 - val_accuracy: 0.4978\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6667 - accuracy: 0.5466 - val_loss: 0.7024 - val_accuracy: 0.4975\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6670 - accuracy: 0.5464 - val_loss: 0.7008 - val_accuracy: 0.4983\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6662 - accuracy: 0.5470 - val_loss: 0.7008 - val_accuracy: 0.4972\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6655 - accuracy: 0.5474 - val_loss: 0.7041 - val_accuracy: 0.4983\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6660 - accuracy: 0.5471 - val_loss: 0.7026 - val_accuracy: 0.4978\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6654 - accuracy: 0.5476 - val_loss: 0.7052 - val_accuracy: 0.4986\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6659 - accuracy: 0.5474 - val_loss: 0.7025 - val_accuracy: 0.4980\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6653 - accuracy: 0.5476 - val_loss: 0.7056 - val_accuracy: 0.4966\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6656 - accuracy: 0.5474 - val_loss: 0.7031 - val_accuracy: 0.4976\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6650 - accuracy: 0.5481 - val_loss: 0.7060 - val_accuracy: 0.4974\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6644 - accuracy: 0.5482 - val_loss: 0.7056 - val_accuracy: 0.4973\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6646 - accuracy: 0.5479 - val_loss: 0.7084 - val_accuracy: 0.4982\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6647 - accuracy: 0.5480 - val_loss: 0.7038 - val_accuracy: 0.4977\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6640 - accuracy: 0.5483 - val_loss: 0.7076 - val_accuracy: 0.4986\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6644 - accuracy: 0.5484 - val_loss: 0.7052 - val_accuracy: 0.4979\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6642 - accuracy: 0.5485 - val_loss: 0.7062 - val_accuracy: 0.4983\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6637 - accuracy: 0.5486 - val_loss: 0.7051 - val_accuracy: 0.4985\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6637 - accuracy: 0.5489 - val_loss: 0.7062 - val_accuracy: 0.4981\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6636 - accuracy: 0.5487 - val_loss: 0.7101 - val_accuracy: 0.4996\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6638 - accuracy: 0.5451 - val_loss: 0.7069 - val_accuracy: 0.4966\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6634 - accuracy: 0.5491 - val_loss: 0.7063 - val_accuracy: 0.4979\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6632 - accuracy: 0.5490 - val_loss: 0.7100 - val_accuracy: 0.4984\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6631 - accuracy: 0.5494 - val_loss: 0.7066 - val_accuracy: 0.4970\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6631 - accuracy: 0.5496 - val_loss: 0.7079 - val_accuracy: 0.4988\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6630 - accuracy: 0.5492 - val_loss: 0.7067 - val_accuracy: 0.4975\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6625 - accuracy: 0.5498 - val_loss: 0.7086 - val_accuracy: 0.4979\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6629 - accuracy: 0.5495 - val_loss: 0.7076 - val_accuracy: 0.4977\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6621 - accuracy: 0.5497 - val_loss: 0.7093 - val_accuracy: 0.4977\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6624 - accuracy: 0.5497 - val_loss: 0.7097 - val_accuracy: 0.4981\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6621 - accuracy: 0.5498 - val_loss: 0.7115 - val_accuracy: 0.4980\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6626 - accuracy: 0.5495 - val_loss: 0.7084 - val_accuracy: 0.4974\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6622 - accuracy: 0.5500 - val_loss: 0.7092 - val_accuracy: 0.4981\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6619 - accuracy: 0.5501 - val_loss: 0.7104 - val_accuracy: 0.4979\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6619 - accuracy: 0.5501 - val_loss: 0.7099 - val_accuracy: 0.4978\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6621 - accuracy: 0.5498 - val_loss: 0.7084 - val_accuracy: 0.4979\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6616 - accuracy: 0.5501 - val_loss: 0.7095 - val_accuracy: 0.4979\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6610 - accuracy: 0.5507 - val_loss: 0.7127 - val_accuracy: 0.4985\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 4s 50ms/step - loss: 0.6617 - accuracy: 0.5506 - val_loss: 0.7132 - val_accuracy: 0.4990\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6616 - accuracy: 0.5506 - val_loss: 0.7128 - val_accuracy: 0.4982\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6612 - accuracy: 0.5506 - val_loss: 0.7122 - val_accuracy: 0.4977\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6612 - accuracy: 0.5507 - val_loss: 0.7101 - val_accuracy: 0.4977\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6610 - accuracy: 0.5507 - val_loss: 0.7106 - val_accuracy: 0.4979\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6608 - accuracy: 0.5507 - val_loss: 0.7126 - val_accuracy: 0.4984\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6614 - accuracy: 0.5505 - val_loss: 0.7124 - val_accuracy: 0.4985\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6605 - accuracy: 0.5509 - val_loss: 0.7131 - val_accuracy: 0.4990\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6605 - accuracy: 0.5509 - val_loss: 0.7123 - val_accuracy: 0.4986\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6603 - accuracy: 0.5513 - val_loss: 0.7100 - val_accuracy: 0.4974\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6602 - accuracy: 0.5512 - val_loss: 0.7131 - val_accuracy: 0.4987\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6605 - accuracy: 0.5511 - val_loss: 0.7118 - val_accuracy: 0.4975\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6606 - accuracy: 0.5510 - val_loss: 0.7115 - val_accuracy: 0.4983\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6599 - accuracy: 0.5516 - val_loss: 0.7120 - val_accuracy: 0.4980\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6603 - accuracy: 0.5513 - val_loss: 0.7112 - val_accuracy: 0.4986\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 4s 52ms/step - loss: 0.6599 - accuracy: 0.5515 - val_loss: 0.7129 - val_accuracy: 0.4980\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6600 - accuracy: 0.5516 - val_loss: 0.7161 - val_accuracy: 0.4974\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6603 - accuracy: 0.5514 - val_loss: 0.7162 - val_accuracy: 0.4984\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 4s 53ms/step - loss: 0.6597 - accuracy: 0.5515 - val_loss: 0.7144 - val_accuracy: 0.4979\n",
            "19/19 [==============================] - 1s 15ms/step - loss: 0.7201 - accuracy: 0.4960\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 5s 57ms/step - loss: 0.6903 - accuracy: 0.5021 - val_loss: 0.6866 - val_accuracy: 0.5352\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 7s 91ms/step - loss: 0.6879 - accuracy: 0.5200 - val_loss: 0.6858 - val_accuracy: 0.5385\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 51ms/step - loss: 0.6869 - accuracy: 0.5217 - val_loss: 0.6847 - val_accuracy: 0.5387\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6860 - accuracy: 0.5215 - val_loss: 0.6844 - val_accuracy: 0.5390\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6847 - accuracy: 0.5231 - val_loss: 0.6843 - val_accuracy: 0.5394\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6839 - accuracy: 0.5242 - val_loss: 0.6843 - val_accuracy: 0.5393\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6831 - accuracy: 0.5249 - val_loss: 0.6840 - val_accuracy: 0.5387\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6824 - accuracy: 0.5256 - val_loss: 0.6834 - val_accuracy: 0.5396\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6817 - accuracy: 0.5260 - val_loss: 0.6840 - val_accuracy: 0.5390\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 4s 54ms/step - loss: 0.6808 - accuracy: 0.5266 - val_loss: 0.6857 - val_accuracy: 0.5357\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6802 - accuracy: 0.5271 - val_loss: 0.6843 - val_accuracy: 0.5382\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6795 - accuracy: 0.5278 - val_loss: 0.6853 - val_accuracy: 0.5392\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6792 - accuracy: 0.5282 - val_loss: 0.6850 - val_accuracy: 0.5379\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6783 - accuracy: 0.5285 - val_loss: 0.6851 - val_accuracy: 0.5385\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6780 - accuracy: 0.5291 - val_loss: 0.6851 - val_accuracy: 0.5378\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6777 - accuracy: 0.5292 - val_loss: 0.6868 - val_accuracy: 0.5393\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6772 - accuracy: 0.5297 - val_loss: 0.6862 - val_accuracy: 0.5394\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6764 - accuracy: 0.5305 - val_loss: 0.6855 - val_accuracy: 0.5386\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6758 - accuracy: 0.5308 - val_loss: 0.6857 - val_accuracy: 0.5389\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6754 - accuracy: 0.5309 - val_loss: 0.6861 - val_accuracy: 0.5383\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6749 - accuracy: 0.5316 - val_loss: 0.6867 - val_accuracy: 0.5376\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6745 - accuracy: 0.5321 - val_loss: 0.6879 - val_accuracy: 0.5356\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6744 - accuracy: 0.5318 - val_loss: 0.6872 - val_accuracy: 0.5388\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6738 - accuracy: 0.5325 - val_loss: 0.6874 - val_accuracy: 0.5373\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6733 - accuracy: 0.5327 - val_loss: 0.6871 - val_accuracy: 0.5385\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6735 - accuracy: 0.5324 - val_loss: 0.6868 - val_accuracy: 0.5391\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6726 - accuracy: 0.5332 - val_loss: 0.6877 - val_accuracy: 0.5382\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6721 - accuracy: 0.5336 - val_loss: 0.6885 - val_accuracy: 0.5373\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6720 - accuracy: 0.5337 - val_loss: 0.6880 - val_accuracy: 0.5377\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6715 - accuracy: 0.5343 - val_loss: 0.6901 - val_accuracy: 0.5360\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6712 - accuracy: 0.5340 - val_loss: 0.6893 - val_accuracy: 0.5390\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6710 - accuracy: 0.5347 - val_loss: 0.6886 - val_accuracy: 0.5380\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6707 - accuracy: 0.5347 - val_loss: 0.6886 - val_accuracy: 0.5381\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6702 - accuracy: 0.5351 - val_loss: 0.6895 - val_accuracy: 0.5395\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6699 - accuracy: 0.5354 - val_loss: 0.6904 - val_accuracy: 0.5386\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6698 - accuracy: 0.5352 - val_loss: 0.6896 - val_accuracy: 0.5383\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6691 - accuracy: 0.5289 - val_loss: 0.6914 - val_accuracy: 0.5369\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6692 - accuracy: 0.5284 - val_loss: 0.6908 - val_accuracy: 0.5380\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6692 - accuracy: 0.5358 - val_loss: 0.6919 - val_accuracy: 0.5388\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6687 - accuracy: 0.5360 - val_loss: 0.6904 - val_accuracy: 0.5386\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6686 - accuracy: 0.5349 - val_loss: 0.6930 - val_accuracy: 0.5381\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6685 - accuracy: 0.5332 - val_loss: 0.6912 - val_accuracy: 0.5376\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6677 - accuracy: 0.5366 - val_loss: 0.6928 - val_accuracy: 0.5381\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6678 - accuracy: 0.5367 - val_loss: 0.6943 - val_accuracy: 0.5376\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6681 - accuracy: 0.5338 - val_loss: 0.6928 - val_accuracy: 0.5376\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6672 - accuracy: 0.5349 - val_loss: 0.6937 - val_accuracy: 0.5374\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6671 - accuracy: 0.5374 - val_loss: 0.6939 - val_accuracy: 0.5377\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6668 - accuracy: 0.5375 - val_loss: 0.6938 - val_accuracy: 0.5380\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6663 - accuracy: 0.5380 - val_loss: 0.6945 - val_accuracy: 0.5378\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6663 - accuracy: 0.5379 - val_loss: 0.6943 - val_accuracy: 0.5379\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6665 - accuracy: 0.5277 - val_loss: 0.6952 - val_accuracy: 0.5370\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6666 - accuracy: 0.5378 - val_loss: 0.6960 - val_accuracy: 0.5380\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6661 - accuracy: 0.5381 - val_loss: 0.6951 - val_accuracy: 0.5369\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6662 - accuracy: 0.5382 - val_loss: 0.6935 - val_accuracy: 0.5371\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 4s 55ms/step - loss: 0.6657 - accuracy: 0.5382 - val_loss: 0.6952 - val_accuracy: 0.5370\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6652 - accuracy: 0.5388 - val_loss: 0.6951 - val_accuracy: 0.5373\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6649 - accuracy: 0.5390 - val_loss: 0.6975 - val_accuracy: 0.5372\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6653 - accuracy: 0.5385 - val_loss: 0.6959 - val_accuracy: 0.5376\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6651 - accuracy: 0.5385 - val_loss: 0.6967 - val_accuracy: 0.5384\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6648 - accuracy: 0.5389 - val_loss: 0.6956 - val_accuracy: 0.5372\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6643 - accuracy: 0.5392 - val_loss: 0.6977 - val_accuracy: 0.5375\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6648 - accuracy: 0.5386 - val_loss: 0.6972 - val_accuracy: 0.5373\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6648 - accuracy: 0.5393 - val_loss: 0.6961 - val_accuracy: 0.5380\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6643 - accuracy: 0.5392 - val_loss: 0.6984 - val_accuracy: 0.5377\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6642 - accuracy: 0.5392 - val_loss: 0.6978 - val_accuracy: 0.5376\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6640 - accuracy: 0.5394 - val_loss: 0.6998 - val_accuracy: 0.5377\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6637 - accuracy: 0.5396 - val_loss: 0.6977 - val_accuracy: 0.5375\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6639 - accuracy: 0.5395 - val_loss: 0.6984 - val_accuracy: 0.5370\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6635 - accuracy: 0.5398 - val_loss: 0.6990 - val_accuracy: 0.5371\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6637 - accuracy: 0.5398 - val_loss: 0.6976 - val_accuracy: 0.5368\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6637 - accuracy: 0.5396 - val_loss: 0.6983 - val_accuracy: 0.5376\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6630 - accuracy: 0.5403 - val_loss: 0.6983 - val_accuracy: 0.5371\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6633 - accuracy: 0.5400 - val_loss: 0.6993 - val_accuracy: 0.5370\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6628 - accuracy: 0.5403 - val_loss: 0.6984 - val_accuracy: 0.5374\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6629 - accuracy: 0.5404 - val_loss: 0.6998 - val_accuracy: 0.5362\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6627 - accuracy: 0.5404 - val_loss: 0.7006 - val_accuracy: 0.5371\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6622 - accuracy: 0.5408 - val_loss: 0.7005 - val_accuracy: 0.5365\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6627 - accuracy: 0.5405 - val_loss: 0.7010 - val_accuracy: 0.5362\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6624 - accuracy: 0.5405 - val_loss: 0.6997 - val_accuracy: 0.5363\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6626 - accuracy: 0.5405 - val_loss: 0.7015 - val_accuracy: 0.5375\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6621 - accuracy: 0.5407 - val_loss: 0.7016 - val_accuracy: 0.5366\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6619 - accuracy: 0.5409 - val_loss: 0.7016 - val_accuracy: 0.5367\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6625 - accuracy: 0.5407 - val_loss: 0.6995 - val_accuracy: 0.5364\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6622 - accuracy: 0.5407 - val_loss: 0.7011 - val_accuracy: 0.5373\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6615 - accuracy: 0.5413 - val_loss: 0.7010 - val_accuracy: 0.5369\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6616 - accuracy: 0.5410 - val_loss: 0.7015 - val_accuracy: 0.5368\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6621 - accuracy: 0.5404 - val_loss: 0.7023 - val_accuracy: 0.5366\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6619 - accuracy: 0.5410 - val_loss: 0.7034 - val_accuracy: 0.5360\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6623 - accuracy: 0.5406 - val_loss: 0.7023 - val_accuracy: 0.5368\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6613 - accuracy: 0.5410 - val_loss: 0.7038 - val_accuracy: 0.5370\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6611 - accuracy: 0.5413 - val_loss: 0.7047 - val_accuracy: 0.5373\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6616 - accuracy: 0.5413 - val_loss: 0.7026 - val_accuracy: 0.5368\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6617 - accuracy: 0.5405 - val_loss: 0.7030 - val_accuracy: 0.5368\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6612 - accuracy: 0.5412 - val_loss: 0.7036 - val_accuracy: 0.5369\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6612 - accuracy: 0.5418 - val_loss: 0.7035 - val_accuracy: 0.5371\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6612 - accuracy: 0.5415 - val_loss: 0.7033 - val_accuracy: 0.5365\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6611 - accuracy: 0.5416 - val_loss: 0.7035 - val_accuracy: 0.5369\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6610 - accuracy: 0.5415 - val_loss: 0.7042 - val_accuracy: 0.5373\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6608 - accuracy: 0.5411 - val_loss: 0.7049 - val_accuracy: 0.5362\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6611 - accuracy: 0.5331 - val_loss: 0.7044 - val_accuracy: 0.5376\n",
            "19/19 [==============================] - 1s 25ms/step - loss: 0.7067 - accuracy: 0.5369\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 6s 63ms/step - loss: 0.6905 - accuracy: 0.5185 - val_loss: 0.6894 - val_accuracy: 0.5084\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6874 - accuracy: 0.5265 - val_loss: 0.6890 - val_accuracy: 0.5090\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6856 - accuracy: 0.5291 - val_loss: 0.6885 - val_accuracy: 0.5097\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6848 - accuracy: 0.5302 - val_loss: 0.6883 - val_accuracy: 0.5091\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6840 - accuracy: 0.5305 - val_loss: 0.6882 - val_accuracy: 0.5097\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6830 - accuracy: 0.5323 - val_loss: 0.6880 - val_accuracy: 0.5100\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6816 - accuracy: 0.5334 - val_loss: 0.6895 - val_accuracy: 0.5103\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6813 - accuracy: 0.5336 - val_loss: 0.6885 - val_accuracy: 0.5092\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6801 - accuracy: 0.5345 - val_loss: 0.6896 - val_accuracy: 0.5097\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6795 - accuracy: 0.5343 - val_loss: 0.6895 - val_accuracy: 0.5100\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6788 - accuracy: 0.5355 - val_loss: 0.6909 - val_accuracy: 0.5100\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6784 - accuracy: 0.5355 - val_loss: 0.6899 - val_accuracy: 0.5097\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6782 - accuracy: 0.5357 - val_loss: 0.6917 - val_accuracy: 0.5100\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6774 - accuracy: 0.5362 - val_loss: 0.6930 - val_accuracy: 0.5098\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6767 - accuracy: 0.5367 - val_loss: 0.6933 - val_accuracy: 0.5103\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6760 - accuracy: 0.5375 - val_loss: 0.6934 - val_accuracy: 0.5099\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6757 - accuracy: 0.5375 - val_loss: 0.6950 - val_accuracy: 0.5100\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6752 - accuracy: 0.5379 - val_loss: 0.6980 - val_accuracy: 0.5093\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6752 - accuracy: 0.5382 - val_loss: 0.6935 - val_accuracy: 0.5101\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6743 - accuracy: 0.5387 - val_loss: 0.6948 - val_accuracy: 0.5098\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6739 - accuracy: 0.5389 - val_loss: 0.6970 - val_accuracy: 0.5103\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6736 - accuracy: 0.5390 - val_loss: 0.6957 - val_accuracy: 0.5100\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6731 - accuracy: 0.5395 - val_loss: 0.7005 - val_accuracy: 0.5095\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6729 - accuracy: 0.5396 - val_loss: 0.6979 - val_accuracy: 0.5091\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6724 - accuracy: 0.5399 - val_loss: 0.6997 - val_accuracy: 0.5092\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6716 - accuracy: 0.5404 - val_loss: 0.7001 - val_accuracy: 0.5093\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6717 - accuracy: 0.5404 - val_loss: 0.6996 - val_accuracy: 0.5096\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6711 - accuracy: 0.5407 - val_loss: 0.7020 - val_accuracy: 0.5096\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6711 - accuracy: 0.5407 - val_loss: 0.7030 - val_accuracy: 0.5096\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6708 - accuracy: 0.5413 - val_loss: 0.7033 - val_accuracy: 0.5090\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6706 - accuracy: 0.5412 - val_loss: 0.7027 - val_accuracy: 0.5092\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6700 - accuracy: 0.5417 - val_loss: 0.7023 - val_accuracy: 0.5094\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6695 - accuracy: 0.5417 - val_loss: 0.7077 - val_accuracy: 0.5089\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6695 - accuracy: 0.5417 - val_loss: 0.7056 - val_accuracy: 0.5089\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6696 - accuracy: 0.5420 - val_loss: 0.7037 - val_accuracy: 0.5092\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6694 - accuracy: 0.5420 - val_loss: 0.7052 - val_accuracy: 0.5095\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6689 - accuracy: 0.5424 - val_loss: 0.7046 - val_accuracy: 0.5095\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6683 - accuracy: 0.5429 - val_loss: 0.7082 - val_accuracy: 0.5093\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6685 - accuracy: 0.5427 - val_loss: 0.7076 - val_accuracy: 0.5093\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6679 - accuracy: 0.5432 - val_loss: 0.7080 - val_accuracy: 0.5090\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6677 - accuracy: 0.5431 - val_loss: 0.7091 - val_accuracy: 0.5092\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6673 - accuracy: 0.5434 - val_loss: 0.7086 - val_accuracy: 0.5090\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6673 - accuracy: 0.5432 - val_loss: 0.7096 - val_accuracy: 0.5087\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6673 - accuracy: 0.5438 - val_loss: 0.7138 - val_accuracy: 0.5093\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6670 - accuracy: 0.5439 - val_loss: 0.7104 - val_accuracy: 0.5090\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6665 - accuracy: 0.5443 - val_loss: 0.7124 - val_accuracy: 0.5094\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6664 - accuracy: 0.5439 - val_loss: 0.7133 - val_accuracy: 0.5089\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6661 - accuracy: 0.5444 - val_loss: 0.7128 - val_accuracy: 0.5088\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6663 - accuracy: 0.5444 - val_loss: 0.7111 - val_accuracy: 0.5084\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6656 - accuracy: 0.5448 - val_loss: 0.7142 - val_accuracy: 0.5090\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6658 - accuracy: 0.5447 - val_loss: 0.7134 - val_accuracy: 0.5093\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6654 - accuracy: 0.5449 - val_loss: 0.7159 - val_accuracy: 0.5089\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6648 - accuracy: 0.5454 - val_loss: 0.7158 - val_accuracy: 0.5089\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6650 - accuracy: 0.5451 - val_loss: 0.7137 - val_accuracy: 0.5088\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6647 - accuracy: 0.5452 - val_loss: 0.7147 - val_accuracy: 0.5086\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6645 - accuracy: 0.5455 - val_loss: 0.7156 - val_accuracy: 0.5085\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6643 - accuracy: 0.5456 - val_loss: 0.7176 - val_accuracy: 0.5086\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6643 - accuracy: 0.5458 - val_loss: 0.7179 - val_accuracy: 0.5083\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6640 - accuracy: 0.5459 - val_loss: 0.7174 - val_accuracy: 0.5081\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6640 - accuracy: 0.5461 - val_loss: 0.7169 - val_accuracy: 0.5083\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6634 - accuracy: 0.5462 - val_loss: 0.7204 - val_accuracy: 0.5081\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6635 - accuracy: 0.5463 - val_loss: 0.7182 - val_accuracy: 0.5085\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6633 - accuracy: 0.5465 - val_loss: 0.7205 - val_accuracy: 0.5081\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6635 - accuracy: 0.5462 - val_loss: 0.7244 - val_accuracy: 0.5084\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6634 - accuracy: 0.5464 - val_loss: 0.7218 - val_accuracy: 0.5085\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6631 - accuracy: 0.5465 - val_loss: 0.7175 - val_accuracy: 0.5081\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6628 - accuracy: 0.5468 - val_loss: 0.7232 - val_accuracy: 0.5085\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6631 - accuracy: 0.5467 - val_loss: 0.7208 - val_accuracy: 0.5083\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6638 - accuracy: 0.5463 - val_loss: 0.7193 - val_accuracy: 0.5084\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6627 - accuracy: 0.5471 - val_loss: 0.7253 - val_accuracy: 0.5086\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6630 - accuracy: 0.5384 - val_loss: 0.7202 - val_accuracy: 0.5085\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6626 - accuracy: 0.5470 - val_loss: 0.7258 - val_accuracy: 0.5088\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6625 - accuracy: 0.5470 - val_loss: 0.7196 - val_accuracy: 0.5083\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6624 - accuracy: 0.5469 - val_loss: 0.7249 - val_accuracy: 0.5082\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6617 - accuracy: 0.5473 - val_loss: 0.7204 - val_accuracy: 0.5081\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6618 - accuracy: 0.5472 - val_loss: 0.7271 - val_accuracy: 0.5079\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6621 - accuracy: 0.5472 - val_loss: 0.7209 - val_accuracy: 0.5083\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6617 - accuracy: 0.5474 - val_loss: 0.7251 - val_accuracy: 0.5087\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6611 - accuracy: 0.5476 - val_loss: 0.7261 - val_accuracy: 0.5087\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6612 - accuracy: 0.5477 - val_loss: 0.7229 - val_accuracy: 0.5084\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6608 - accuracy: 0.5479 - val_loss: 0.7265 - val_accuracy: 0.5087\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6612 - accuracy: 0.5478 - val_loss: 0.7251 - val_accuracy: 0.5082\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6609 - accuracy: 0.5480 - val_loss: 0.7212 - val_accuracy: 0.5086\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6609 - accuracy: 0.5479 - val_loss: 0.7221 - val_accuracy: 0.5084\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6609 - accuracy: 0.5479 - val_loss: 0.7251 - val_accuracy: 0.5083\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6607 - accuracy: 0.5482 - val_loss: 0.7284 - val_accuracy: 0.5084\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6607 - accuracy: 0.5481 - val_loss: 0.7242 - val_accuracy: 0.5085\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6608 - accuracy: 0.5478 - val_loss: 0.7252 - val_accuracy: 0.5085\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6604 - accuracy: 0.5483 - val_loss: 0.7307 - val_accuracy: 0.5087\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6602 - accuracy: 0.5482 - val_loss: 0.7275 - val_accuracy: 0.5085\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6600 - accuracy: 0.5484 - val_loss: 0.7264 - val_accuracy: 0.5082\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6598 - accuracy: 0.5485 - val_loss: 0.7251 - val_accuracy: 0.5087\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6598 - accuracy: 0.5487 - val_loss: 0.7286 - val_accuracy: 0.5086\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6602 - accuracy: 0.5492 - val_loss: 0.7256 - val_accuracy: 0.5084\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6604 - accuracy: 0.5420 - val_loss: 0.7304 - val_accuracy: 0.5085\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6596 - accuracy: 0.5487 - val_loss: 0.7285 - val_accuracy: 0.5083\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6596 - accuracy: 0.5488 - val_loss: 0.7276 - val_accuracy: 0.5086\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6597 - accuracy: 0.5487 - val_loss: 0.7268 - val_accuracy: 0.5088\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6596 - accuracy: 0.5487 - val_loss: 0.7298 - val_accuracy: 0.5080\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6598 - accuracy: 0.5487 - val_loss: 0.7304 - val_accuracy: 0.5092\n",
            "19/19 [==============================] - 1s 17ms/step - loss: 0.7279 - accuracy: 0.5091\n",
            "Best accuracy for dataset 1: 0.5400651693344116\n",
            "2308\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 71ms/step - loss: 0.6912 - accuracy: 0.4946 - val_loss: 0.6877 - val_accuracy: 0.5371\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6886 - accuracy: 0.5106 - val_loss: 0.6870 - val_accuracy: 0.5435\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6870 - accuracy: 0.5194 - val_loss: 0.6851 - val_accuracy: 0.5429\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6859 - accuracy: 0.5201 - val_loss: 0.6846 - val_accuracy: 0.5437\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6847 - accuracy: 0.5215 - val_loss: 0.6845 - val_accuracy: 0.5436\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6842 - accuracy: 0.5224 - val_loss: 0.6853 - val_accuracy: 0.5424\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6832 - accuracy: 0.5228 - val_loss: 0.6846 - val_accuracy: 0.5453\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6829 - accuracy: 0.5240 - val_loss: 0.6847 - val_accuracy: 0.5439\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6815 - accuracy: 0.5246 - val_loss: 0.6849 - val_accuracy: 0.5441\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6811 - accuracy: 0.5247 - val_loss: 0.6843 - val_accuracy: 0.5453\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6799 - accuracy: 0.5261 - val_loss: 0.6849 - val_accuracy: 0.5447\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6794 - accuracy: 0.5264 - val_loss: 0.6848 - val_accuracy: 0.5448\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6788 - accuracy: 0.5269 - val_loss: 0.6848 - val_accuracy: 0.5449\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6784 - accuracy: 0.5272 - val_loss: 0.6858 - val_accuracy: 0.5444\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6775 - accuracy: 0.5276 - val_loss: 0.6876 - val_accuracy: 0.5441\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6774 - accuracy: 0.5284 - val_loss: 0.6862 - val_accuracy: 0.5453\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6765 - accuracy: 0.5291 - val_loss: 0.6867 - val_accuracy: 0.5446\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6759 - accuracy: 0.5295 - val_loss: 0.6861 - val_accuracy: 0.5450\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6757 - accuracy: 0.5295 - val_loss: 0.6855 - val_accuracy: 0.5442\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6753 - accuracy: 0.5297 - val_loss: 0.6870 - val_accuracy: 0.5447\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6751 - accuracy: 0.5300 - val_loss: 0.6867 - val_accuracy: 0.5447\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6747 - accuracy: 0.5301 - val_loss: 0.6866 - val_accuracy: 0.5447\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6740 - accuracy: 0.5310 - val_loss: 0.6862 - val_accuracy: 0.5447\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6735 - accuracy: 0.5311 - val_loss: 0.6863 - val_accuracy: 0.5445\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6730 - accuracy: 0.5314 - val_loss: 0.6870 - val_accuracy: 0.5447\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6724 - accuracy: 0.5316 - val_loss: 0.6872 - val_accuracy: 0.5447\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6722 - accuracy: 0.5319 - val_loss: 0.6879 - val_accuracy: 0.5451\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6718 - accuracy: 0.5324 - val_loss: 0.6881 - val_accuracy: 0.5445\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6712 - accuracy: 0.5329 - val_loss: 0.6903 - val_accuracy: 0.5450\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6709 - accuracy: 0.5327 - val_loss: 0.6895 - val_accuracy: 0.5448\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6709 - accuracy: 0.5286 - val_loss: 0.6888 - val_accuracy: 0.5447\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6703 - accuracy: 0.5335 - val_loss: 0.6890 - val_accuracy: 0.5445\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6705 - accuracy: 0.5329 - val_loss: 0.6896 - val_accuracy: 0.5449\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6706 - accuracy: 0.5336 - val_loss: 0.6904 - val_accuracy: 0.5450\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6699 - accuracy: 0.5337 - val_loss: 0.6889 - val_accuracy: 0.5458\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6694 - accuracy: 0.5341 - val_loss: 0.6889 - val_accuracy: 0.5459\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6688 - accuracy: 0.5344 - val_loss: 0.6893 - val_accuracy: 0.5450\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6689 - accuracy: 0.5344 - val_loss: 0.6890 - val_accuracy: 0.5453\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6683 - accuracy: 0.5348 - val_loss: 0.6893 - val_accuracy: 0.5455\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6684 - accuracy: 0.5347 - val_loss: 0.6891 - val_accuracy: 0.5451\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6678 - accuracy: 0.5338 - val_loss: 0.6899 - val_accuracy: 0.5451\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6676 - accuracy: 0.5355 - val_loss: 0.6909 - val_accuracy: 0.5451\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6681 - accuracy: 0.5350 - val_loss: 0.6904 - val_accuracy: 0.5448\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6674 - accuracy: 0.5356 - val_loss: 0.6931 - val_accuracy: 0.5449\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6670 - accuracy: 0.5359 - val_loss: 0.6902 - val_accuracy: 0.5449\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6664 - accuracy: 0.5361 - val_loss: 0.6913 - val_accuracy: 0.5447\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6661 - accuracy: 0.5366 - val_loss: 0.6927 - val_accuracy: 0.5448\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6666 - accuracy: 0.5361 - val_loss: 0.6923 - val_accuracy: 0.5451\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6661 - accuracy: 0.5365 - val_loss: 0.6923 - val_accuracy: 0.5451\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6659 - accuracy: 0.5367 - val_loss: 0.6931 - val_accuracy: 0.5454\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6657 - accuracy: 0.5369 - val_loss: 0.6918 - val_accuracy: 0.5454\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6654 - accuracy: 0.5371 - val_loss: 0.6930 - val_accuracy: 0.5457\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6654 - accuracy: 0.5371 - val_loss: 0.6940 - val_accuracy: 0.5451\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6653 - accuracy: 0.5332 - val_loss: 0.6927 - val_accuracy: 0.5453\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6652 - accuracy: 0.5374 - val_loss: 0.6919 - val_accuracy: 0.5451\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6650 - accuracy: 0.5372 - val_loss: 0.6932 - val_accuracy: 0.5456\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6646 - accuracy: 0.5376 - val_loss: 0.6934 - val_accuracy: 0.5448\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6644 - accuracy: 0.5374 - val_loss: 0.6935 - val_accuracy: 0.5453\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6643 - accuracy: 0.5378 - val_loss: 0.6928 - val_accuracy: 0.5452\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6638 - accuracy: 0.5378 - val_loss: 0.6935 - val_accuracy: 0.5454\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6640 - accuracy: 0.5383 - val_loss: 0.6933 - val_accuracy: 0.5444\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6644 - accuracy: 0.5375 - val_loss: 0.6963 - val_accuracy: 0.5445\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6640 - accuracy: 0.5378 - val_loss: 0.6922 - val_accuracy: 0.5445\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6635 - accuracy: 0.5382 - val_loss: 0.6943 - val_accuracy: 0.5451\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6642 - accuracy: 0.5373 - val_loss: 0.6940 - val_accuracy: 0.5452\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6633 - accuracy: 0.5381 - val_loss: 0.6947 - val_accuracy: 0.5450\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6642 - accuracy: 0.5381 - val_loss: 0.6930 - val_accuracy: 0.5443\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6629 - accuracy: 0.5385 - val_loss: 0.6961 - val_accuracy: 0.5448\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6629 - accuracy: 0.5387 - val_loss: 0.6961 - val_accuracy: 0.5450\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6630 - accuracy: 0.5385 - val_loss: 0.6935 - val_accuracy: 0.5453\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6626 - accuracy: 0.5391 - val_loss: 0.6963 - val_accuracy: 0.5444\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6625 - accuracy: 0.5391 - val_loss: 0.6954 - val_accuracy: 0.5447\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6623 - accuracy: 0.5382 - val_loss: 0.6959 - val_accuracy: 0.5453\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6623 - accuracy: 0.5392 - val_loss: 0.6967 - val_accuracy: 0.5448\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6623 - accuracy: 0.5394 - val_loss: 0.6957 - val_accuracy: 0.5444\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6624 - accuracy: 0.5393 - val_loss: 0.6967 - val_accuracy: 0.5438\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6626 - accuracy: 0.5388 - val_loss: 0.6967 - val_accuracy: 0.5439\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6621 - accuracy: 0.5391 - val_loss: 0.6953 - val_accuracy: 0.5445\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6623 - accuracy: 0.5389 - val_loss: 0.6982 - val_accuracy: 0.5438\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6620 - accuracy: 0.5392 - val_loss: 0.6971 - val_accuracy: 0.5445\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6615 - accuracy: 0.5399 - val_loss: 0.6968 - val_accuracy: 0.5437\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6617 - accuracy: 0.5396 - val_loss: 0.6978 - val_accuracy: 0.5445\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6615 - accuracy: 0.5398 - val_loss: 0.6990 - val_accuracy: 0.5440\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 5s 61ms/step - loss: 0.6614 - accuracy: 0.5397 - val_loss: 0.6973 - val_accuracy: 0.5438\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6614 - accuracy: 0.5397 - val_loss: 0.6980 - val_accuracy: 0.5441\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6612 - accuracy: 0.5369 - val_loss: 0.6991 - val_accuracy: 0.5448\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6611 - accuracy: 0.5399 - val_loss: 0.6997 - val_accuracy: 0.5444\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6610 - accuracy: 0.5400 - val_loss: 0.7004 - val_accuracy: 0.5444\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6612 - accuracy: 0.5398 - val_loss: 0.6992 - val_accuracy: 0.5441\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6611 - accuracy: 0.5398 - val_loss: 0.6976 - val_accuracy: 0.5445\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6608 - accuracy: 0.5403 - val_loss: 0.6990 - val_accuracy: 0.5443\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6606 - accuracy: 0.5402 - val_loss: 0.6997 - val_accuracy: 0.5435\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6607 - accuracy: 0.5405 - val_loss: 0.6996 - val_accuracy: 0.5441\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6604 - accuracy: 0.5404 - val_loss: 0.7001 - val_accuracy: 0.5440\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6610 - accuracy: 0.5403 - val_loss: 0.7001 - val_accuracy: 0.5436\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6604 - accuracy: 0.5405 - val_loss: 0.6994 - val_accuracy: 0.5441\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6603 - accuracy: 0.5406 - val_loss: 0.6997 - val_accuracy: 0.5439\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6601 - accuracy: 0.5408 - val_loss: 0.7009 - val_accuracy: 0.5438\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6602 - accuracy: 0.5406 - val_loss: 0.7014 - val_accuracy: 0.5439\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6602 - accuracy: 0.5380 - val_loss: 0.6996 - val_accuracy: 0.5442\n",
            "19/19 [==============================] - 1s 16ms/step - loss: 0.6999 - accuracy: 0.5439\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 78ms/step - loss: 0.6901 - accuracy: 0.5081 - val_loss: 0.6885 - val_accuracy: 0.5335\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6878 - accuracy: 0.5208 - val_loss: 0.6875 - val_accuracy: 0.5350\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6867 - accuracy: 0.5224 - val_loss: 0.6873 - val_accuracy: 0.5370\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6858 - accuracy: 0.5233 - val_loss: 0.6863 - val_accuracy: 0.5369\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6846 - accuracy: 0.5243 - val_loss: 0.6854 - val_accuracy: 0.5374\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6843 - accuracy: 0.5248 - val_loss: 0.6852 - val_accuracy: 0.5365\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6832 - accuracy: 0.5251 - val_loss: 0.6851 - val_accuracy: 0.5375\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6828 - accuracy: 0.5256 - val_loss: 0.6852 - val_accuracy: 0.5370\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6818 - accuracy: 0.5266 - val_loss: 0.6851 - val_accuracy: 0.5381\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6812 - accuracy: 0.5269 - val_loss: 0.6854 - val_accuracy: 0.5374\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6805 - accuracy: 0.5275 - val_loss: 0.6853 - val_accuracy: 0.5386\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6797 - accuracy: 0.5284 - val_loss: 0.6856 - val_accuracy: 0.5380\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6794 - accuracy: 0.5286 - val_loss: 0.6854 - val_accuracy: 0.5389\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6789 - accuracy: 0.5286 - val_loss: 0.6859 - val_accuracy: 0.5377\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6785 - accuracy: 0.5290 - val_loss: 0.6858 - val_accuracy: 0.5393\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6784 - accuracy: 0.5295 - val_loss: 0.6871 - val_accuracy: 0.5386\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6776 - accuracy: 0.5299 - val_loss: 0.6863 - val_accuracy: 0.5380\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6767 - accuracy: 0.5307 - val_loss: 0.6873 - val_accuracy: 0.5381\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6765 - accuracy: 0.5302 - val_loss: 0.6870 - val_accuracy: 0.5383\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 4s 56ms/step - loss: 0.6759 - accuracy: 0.5307 - val_loss: 0.6879 - val_accuracy: 0.5377\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6754 - accuracy: 0.5314 - val_loss: 0.6885 - val_accuracy: 0.5382\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6753 - accuracy: 0.5315 - val_loss: 0.6884 - val_accuracy: 0.5379\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6747 - accuracy: 0.5319 - val_loss: 0.6910 - val_accuracy: 0.5375\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6740 - accuracy: 0.5326 - val_loss: 0.6880 - val_accuracy: 0.5386\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6740 - accuracy: 0.5325 - val_loss: 0.6899 - val_accuracy: 0.5382\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6734 - accuracy: 0.5326 - val_loss: 0.6887 - val_accuracy: 0.5382\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6733 - accuracy: 0.5329 - val_loss: 0.6885 - val_accuracy: 0.5379\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6729 - accuracy: 0.5332 - val_loss: 0.6891 - val_accuracy: 0.5381\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6724 - accuracy: 0.5335 - val_loss: 0.6903 - val_accuracy: 0.5379\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6722 - accuracy: 0.5337 - val_loss: 0.6911 - val_accuracy: 0.5383\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6714 - accuracy: 0.5343 - val_loss: 0.6891 - val_accuracy: 0.5377\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6717 - accuracy: 0.5342 - val_loss: 0.6902 - val_accuracy: 0.5384\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6715 - accuracy: 0.5343 - val_loss: 0.6916 - val_accuracy: 0.5382\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6710 - accuracy: 0.5345 - val_loss: 0.6932 - val_accuracy: 0.5379\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6705 - accuracy: 0.5350 - val_loss: 0.6915 - val_accuracy: 0.5383\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6703 - accuracy: 0.5352 - val_loss: 0.6933 - val_accuracy: 0.5384\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6701 - accuracy: 0.5352 - val_loss: 0.6929 - val_accuracy: 0.5379\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6695 - accuracy: 0.5357 - val_loss: 0.6931 - val_accuracy: 0.5377\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6695 - accuracy: 0.5355 - val_loss: 0.6911 - val_accuracy: 0.5376\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6695 - accuracy: 0.5270 - val_loss: 0.6962 - val_accuracy: 0.5375\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6688 - accuracy: 0.5359 - val_loss: 0.6942 - val_accuracy: 0.5385\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6688 - accuracy: 0.5361 - val_loss: 0.6929 - val_accuracy: 0.5376\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6688 - accuracy: 0.5361 - val_loss: 0.6957 - val_accuracy: 0.5381\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6682 - accuracy: 0.5367 - val_loss: 0.6944 - val_accuracy: 0.5377\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6681 - accuracy: 0.5364 - val_loss: 0.6954 - val_accuracy: 0.5379\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6678 - accuracy: 0.5368 - val_loss: 0.6944 - val_accuracy: 0.5370\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6681 - accuracy: 0.5369 - val_loss: 0.6947 - val_accuracy: 0.5376\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6671 - accuracy: 0.5373 - val_loss: 0.6948 - val_accuracy: 0.5376\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6672 - accuracy: 0.5372 - val_loss: 0.6978 - val_accuracy: 0.5376\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6668 - accuracy: 0.5378 - val_loss: 0.6988 - val_accuracy: 0.5373\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6667 - accuracy: 0.5375 - val_loss: 0.6962 - val_accuracy: 0.5382\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6671 - accuracy: 0.5352 - val_loss: 0.6931 - val_accuracy: 0.5375\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6669 - accuracy: 0.5374 - val_loss: 0.6960 - val_accuracy: 0.5371\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6661 - accuracy: 0.5375 - val_loss: 0.6974 - val_accuracy: 0.5379\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6659 - accuracy: 0.5345 - val_loss: 0.6983 - val_accuracy: 0.5367\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6657 - accuracy: 0.5384 - val_loss: 0.6994 - val_accuracy: 0.5374\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6661 - accuracy: 0.5381 - val_loss: 0.6976 - val_accuracy: 0.5372\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6654 - accuracy: 0.5385 - val_loss: 0.6992 - val_accuracy: 0.5377\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6652 - accuracy: 0.5384 - val_loss: 0.6976 - val_accuracy: 0.5374\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6652 - accuracy: 0.5386 - val_loss: 0.7005 - val_accuracy: 0.5377\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6652 - accuracy: 0.5387 - val_loss: 0.7015 - val_accuracy: 0.5379\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6649 - accuracy: 0.5389 - val_loss: 0.7001 - val_accuracy: 0.5367\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6645 - accuracy: 0.5384 - val_loss: 0.7010 - val_accuracy: 0.5371\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6647 - accuracy: 0.5388 - val_loss: 0.6979 - val_accuracy: 0.5374\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6646 - accuracy: 0.5392 - val_loss: 0.6994 - val_accuracy: 0.5374\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6644 - accuracy: 0.5394 - val_loss: 0.6980 - val_accuracy: 0.5371\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6649 - accuracy: 0.5387 - val_loss: 0.6998 - val_accuracy: 0.5376\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6640 - accuracy: 0.5396 - val_loss: 0.6998 - val_accuracy: 0.5376\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6635 - accuracy: 0.5400 - val_loss: 0.7011 - val_accuracy: 0.5371\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6635 - accuracy: 0.5399 - val_loss: 0.7036 - val_accuracy: 0.5371\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6634 - accuracy: 0.5399 - val_loss: 0.7010 - val_accuracy: 0.5381\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6636 - accuracy: 0.5392 - val_loss: 0.7005 - val_accuracy: 0.5381\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6635 - accuracy: 0.5402 - val_loss: 0.7018 - val_accuracy: 0.5368\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6635 - accuracy: 0.5397 - val_loss: 0.7047 - val_accuracy: 0.5381\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6636 - accuracy: 0.5398 - val_loss: 0.7018 - val_accuracy: 0.5375\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6627 - accuracy: 0.5403 - val_loss: 0.7050 - val_accuracy: 0.5374\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6626 - accuracy: 0.5403 - val_loss: 0.7052 - val_accuracy: 0.5380\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6632 - accuracy: 0.5402 - val_loss: 0.6991 - val_accuracy: 0.5375\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6627 - accuracy: 0.5405 - val_loss: 0.7062 - val_accuracy: 0.5379\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6631 - accuracy: 0.5403 - val_loss: 0.7055 - val_accuracy: 0.5378\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6627 - accuracy: 0.5406 - val_loss: 0.7042 - val_accuracy: 0.5375\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6626 - accuracy: 0.5343 - val_loss: 0.7050 - val_accuracy: 0.5377\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6624 - accuracy: 0.5406 - val_loss: 0.7037 - val_accuracy: 0.5375\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6621 - accuracy: 0.5408 - val_loss: 0.7059 - val_accuracy: 0.5377\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6623 - accuracy: 0.5408 - val_loss: 0.7064 - val_accuracy: 0.5372\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6619 - accuracy: 0.5408 - val_loss: 0.7068 - val_accuracy: 0.5377\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6617 - accuracy: 0.5412 - val_loss: 0.7048 - val_accuracy: 0.5381\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6622 - accuracy: 0.5408 - val_loss: 0.7061 - val_accuracy: 0.5378\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6617 - accuracy: 0.5410 - val_loss: 0.7062 - val_accuracy: 0.5369\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6615 - accuracy: 0.5413 - val_loss: 0.7111 - val_accuracy: 0.5377\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6614 - accuracy: 0.5413 - val_loss: 0.7081 - val_accuracy: 0.5380\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6615 - accuracy: 0.5411 - val_loss: 0.7070 - val_accuracy: 0.5381\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6615 - accuracy: 0.5400 - val_loss: 0.7051 - val_accuracy: 0.5377\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6615 - accuracy: 0.5411 - val_loss: 0.7107 - val_accuracy: 0.5375\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6613 - accuracy: 0.5413 - val_loss: 0.7075 - val_accuracy: 0.5377\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6611 - accuracy: 0.5415 - val_loss: 0.7053 - val_accuracy: 0.5379\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6610 - accuracy: 0.5415 - val_loss: 0.7080 - val_accuracy: 0.5373\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6612 - accuracy: 0.5414 - val_loss: 0.7073 - val_accuracy: 0.5380\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6612 - accuracy: 0.5413 - val_loss: 0.7059 - val_accuracy: 0.5374\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6608 - accuracy: 0.5414 - val_loss: 0.7082 - val_accuracy: 0.5375\n",
            "19/19 [==============================] - 1s 19ms/step - loss: 0.7112 - accuracy: 0.5358\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 6s 64ms/step - loss: 0.6901 - accuracy: 0.5234 - val_loss: 0.6867 - val_accuracy: 0.5033\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6876 - accuracy: 0.5298 - val_loss: 0.6864 - val_accuracy: 0.5032\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6864 - accuracy: 0.5304 - val_loss: 0.6870 - val_accuracy: 0.5027\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6852 - accuracy: 0.5324 - val_loss: 0.6867 - val_accuracy: 0.5030\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6839 - accuracy: 0.5327 - val_loss: 0.6877 - val_accuracy: 0.5007\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6836 - accuracy: 0.5340 - val_loss: 0.6865 - val_accuracy: 0.5035\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6828 - accuracy: 0.5343 - val_loss: 0.6876 - val_accuracy: 0.5008\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6820 - accuracy: 0.5350 - val_loss: 0.6882 - val_accuracy: 0.4999\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6814 - accuracy: 0.5357 - val_loss: 0.6867 - val_accuracy: 0.5022\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6805 - accuracy: 0.5363 - val_loss: 0.6865 - val_accuracy: 0.5027\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 4s 57ms/step - loss: 0.6798 - accuracy: 0.5364 - val_loss: 0.6872 - val_accuracy: 0.5030\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6794 - accuracy: 0.5369 - val_loss: 0.6870 - val_accuracy: 0.5021\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6784 - accuracy: 0.5379 - val_loss: 0.6873 - val_accuracy: 0.5025\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6779 - accuracy: 0.5379 - val_loss: 0.6877 - val_accuracy: 0.5022\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6775 - accuracy: 0.5384 - val_loss: 0.6873 - val_accuracy: 0.5019\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6770 - accuracy: 0.5388 - val_loss: 0.6877 - val_accuracy: 0.5030\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6765 - accuracy: 0.5390 - val_loss: 0.6877 - val_accuracy: 0.5007\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6757 - accuracy: 0.5397 - val_loss: 0.6874 - val_accuracy: 0.5025\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6757 - accuracy: 0.5397 - val_loss: 0.6876 - val_accuracy: 0.5021\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6751 - accuracy: 0.5404 - val_loss: 0.6878 - val_accuracy: 0.5022\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6746 - accuracy: 0.5409 - val_loss: 0.6887 - val_accuracy: 0.4998\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6748 - accuracy: 0.5406 - val_loss: 0.6892 - val_accuracy: 0.5031\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6740 - accuracy: 0.5412 - val_loss: 0.6884 - val_accuracy: 0.5011\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6734 - accuracy: 0.5413 - val_loss: 0.6897 - val_accuracy: 0.5001\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6738 - accuracy: 0.5417 - val_loss: 0.6894 - val_accuracy: 0.5033\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6724 - accuracy: 0.5424 - val_loss: 0.6890 - val_accuracy: 0.5024\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6721 - accuracy: 0.5424 - val_loss: 0.6906 - val_accuracy: 0.4999\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6716 - accuracy: 0.5428 - val_loss: 0.6889 - val_accuracy: 0.5014\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6719 - accuracy: 0.5425 - val_loss: 0.6906 - val_accuracy: 0.5026\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6717 - accuracy: 0.5427 - val_loss: 0.6890 - val_accuracy: 0.5028\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6710 - accuracy: 0.5434 - val_loss: 0.6902 - val_accuracy: 0.5011\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6711 - accuracy: 0.5432 - val_loss: 0.6901 - val_accuracy: 0.5024\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6705 - accuracy: 0.5438 - val_loss: 0.6892 - val_accuracy: 0.5029\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6699 - accuracy: 0.5441 - val_loss: 0.6908 - val_accuracy: 0.5017\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6704 - accuracy: 0.5441 - val_loss: 0.6896 - val_accuracy: 0.5011\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6699 - accuracy: 0.5440 - val_loss: 0.6901 - val_accuracy: 0.5009\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6694 - accuracy: 0.5444 - val_loss: 0.6913 - val_accuracy: 0.5016\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6691 - accuracy: 0.5444 - val_loss: 0.6921 - val_accuracy: 0.5008\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6690 - accuracy: 0.5448 - val_loss: 0.6926 - val_accuracy: 0.5012\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6689 - accuracy: 0.5447 - val_loss: 0.6930 - val_accuracy: 0.5023\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6683 - accuracy: 0.5453 - val_loss: 0.6934 - val_accuracy: 0.5029\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6681 - accuracy: 0.5457 - val_loss: 0.6940 - val_accuracy: 0.5026\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6676 - accuracy: 0.5459 - val_loss: 0.6939 - val_accuracy: 0.5022\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6674 - accuracy: 0.5458 - val_loss: 0.6934 - val_accuracy: 0.5032\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6671 - accuracy: 0.5462 - val_loss: 0.6931 - val_accuracy: 0.5023\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6670 - accuracy: 0.5462 - val_loss: 0.6948 - val_accuracy: 0.5033\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6668 - accuracy: 0.5464 - val_loss: 0.6950 - val_accuracy: 0.5034\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6667 - accuracy: 0.5466 - val_loss: 0.6944 - val_accuracy: 0.5016\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6663 - accuracy: 0.5467 - val_loss: 0.6946 - val_accuracy: 0.5026\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6664 - accuracy: 0.5466 - val_loss: 0.6935 - val_accuracy: 0.5012\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6657 - accuracy: 0.5472 - val_loss: 0.6951 - val_accuracy: 0.5007\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6655 - accuracy: 0.5471 - val_loss: 0.6955 - val_accuracy: 0.5029\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6656 - accuracy: 0.5472 - val_loss: 0.6961 - val_accuracy: 0.5019\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 4s 58ms/step - loss: 0.6660 - accuracy: 0.5471 - val_loss: 0.6957 - val_accuracy: 0.5031\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6660 - accuracy: 0.5469 - val_loss: 0.6955 - val_accuracy: 0.5022\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6653 - accuracy: 0.5475 - val_loss: 0.6967 - val_accuracy: 0.5028\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6649 - accuracy: 0.5477 - val_loss: 0.6969 - val_accuracy: 0.5022\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 59ms/step - loss: 0.6644 - accuracy: 0.5480 - val_loss: 0.6984 - val_accuracy: 0.5023\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6646 - accuracy: 0.5478 - val_loss: 0.6986 - val_accuracy: 0.5012\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6646 - accuracy: 0.5477 - val_loss: 0.6979 - val_accuracy: 0.5010\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6647 - accuracy: 0.5480 - val_loss: 0.6979 - val_accuracy: 0.5020\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6642 - accuracy: 0.5482 - val_loss: 0.6973 - val_accuracy: 0.5013\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6640 - accuracy: 0.5483 - val_loss: 0.6957 - val_accuracy: 0.5022\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6639 - accuracy: 0.5485 - val_loss: 0.6991 - val_accuracy: 0.5003\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6638 - accuracy: 0.5489 - val_loss: 0.7002 - val_accuracy: 0.5017\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6638 - accuracy: 0.5484 - val_loss: 0.6972 - val_accuracy: 0.5018\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6640 - accuracy: 0.5485 - val_loss: 0.6984 - val_accuracy: 0.5019\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6639 - accuracy: 0.5486 - val_loss: 0.6970 - val_accuracy: 0.4998\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6631 - accuracy: 0.5490 - val_loss: 0.6991 - val_accuracy: 0.5010\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6627 - accuracy: 0.5493 - val_loss: 0.7012 - val_accuracy: 0.5019\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6633 - accuracy: 0.5490 - val_loss: 0.6999 - val_accuracy: 0.5010\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6630 - accuracy: 0.5491 - val_loss: 0.6995 - val_accuracy: 0.5027\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6627 - accuracy: 0.5495 - val_loss: 0.6990 - val_accuracy: 0.5013\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6627 - accuracy: 0.5493 - val_loss: 0.6978 - val_accuracy: 0.5010\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6625 - accuracy: 0.5497 - val_loss: 0.6986 - val_accuracy: 0.5010\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6630 - accuracy: 0.5493 - val_loss: 0.6993 - val_accuracy: 0.5024\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6630 - accuracy: 0.5492 - val_loss: 0.6982 - val_accuracy: 0.5020\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6621 - accuracy: 0.5493 - val_loss: 0.7003 - val_accuracy: 0.5023\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6622 - accuracy: 0.5495 - val_loss: 0.6994 - val_accuracy: 0.5004\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6625 - accuracy: 0.5495 - val_loss: 0.7006 - val_accuracy: 0.5021\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6617 - accuracy: 0.5502 - val_loss: 0.7002 - val_accuracy: 0.5013\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6617 - accuracy: 0.5501 - val_loss: 0.6998 - val_accuracy: 0.5009\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6622 - accuracy: 0.5501 - val_loss: 0.6999 - val_accuracy: 0.5018\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6617 - accuracy: 0.5501 - val_loss: 0.7007 - val_accuracy: 0.5017\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6617 - accuracy: 0.5499 - val_loss: 0.7019 - val_accuracy: 0.5005\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6618 - accuracy: 0.5500 - val_loss: 0.7027 - val_accuracy: 0.5014\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6616 - accuracy: 0.5501 - val_loss: 0.7017 - val_accuracy: 0.5014\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6612 - accuracy: 0.5504 - val_loss: 0.7020 - val_accuracy: 0.5018\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6612 - accuracy: 0.5505 - val_loss: 0.7018 - val_accuracy: 0.5020\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6610 - accuracy: 0.5504 - val_loss: 0.7022 - val_accuracy: 0.5015\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6613 - accuracy: 0.5504 - val_loss: 0.7021 - val_accuracy: 0.5012\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6613 - accuracy: 0.5504 - val_loss: 0.7004 - val_accuracy: 0.5009\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6608 - accuracy: 0.5506 - val_loss: 0.7030 - val_accuracy: 0.5009\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6612 - accuracy: 0.5506 - val_loss: 0.7012 - val_accuracy: 0.5002\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6609 - accuracy: 0.5496 - val_loss: 0.7016 - val_accuracy: 0.4998\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6606 - accuracy: 0.5507 - val_loss: 0.7027 - val_accuracy: 0.5002\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6605 - accuracy: 0.5509 - val_loss: 0.7018 - val_accuracy: 0.5003\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6609 - accuracy: 0.5505 - val_loss: 0.7032 - val_accuracy: 0.5012\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6604 - accuracy: 0.5509 - val_loss: 0.7034 - val_accuracy: 0.5015\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6606 - accuracy: 0.5508 - val_loss: 0.7013 - val_accuracy: 0.5010\n",
            "19/19 [==============================] - 1s 18ms/step - loss: 0.7041 - accuracy: 0.5002\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 8s 90ms/step - loss: 0.6903 - accuracy: 0.5053 - val_loss: 0.6884 - val_accuracy: 0.4946\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6877 - accuracy: 0.5141 - val_loss: 0.6873 - val_accuracy: 0.5344\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6861 - accuracy: 0.5226 - val_loss: 0.6869 - val_accuracy: 0.5346\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6851 - accuracy: 0.5237 - val_loss: 0.6863 - val_accuracy: 0.5344\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6844 - accuracy: 0.5243 - val_loss: 0.6856 - val_accuracy: 0.5351\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6835 - accuracy: 0.5252 - val_loss: 0.6858 - val_accuracy: 0.5349\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6827 - accuracy: 0.5261 - val_loss: 0.6854 - val_accuracy: 0.5355\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6816 - accuracy: 0.5267 - val_loss: 0.6854 - val_accuracy: 0.5355\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6813 - accuracy: 0.5275 - val_loss: 0.6851 - val_accuracy: 0.5354\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6803 - accuracy: 0.5284 - val_loss: 0.6856 - val_accuracy: 0.5353\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6797 - accuracy: 0.5288 - val_loss: 0.6854 - val_accuracy: 0.5355\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6789 - accuracy: 0.5289 - val_loss: 0.6861 - val_accuracy: 0.5354\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6784 - accuracy: 0.5297 - val_loss: 0.6863 - val_accuracy: 0.5352\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6783 - accuracy: 0.5299 - val_loss: 0.6864 - val_accuracy: 0.5338\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6777 - accuracy: 0.5302 - val_loss: 0.6868 - val_accuracy: 0.5340\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6769 - accuracy: 0.5309 - val_loss: 0.6891 - val_accuracy: 0.5347\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6766 - accuracy: 0.5311 - val_loss: 0.6916 - val_accuracy: 0.5348\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6761 - accuracy: 0.5312 - val_loss: 0.6887 - val_accuracy: 0.5348\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6757 - accuracy: 0.5316 - val_loss: 0.6881 - val_accuracy: 0.5340\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6756 - accuracy: 0.5318 - val_loss: 0.6891 - val_accuracy: 0.5349\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6751 - accuracy: 0.5322 - val_loss: 0.6907 - val_accuracy: 0.5352\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6743 - accuracy: 0.5329 - val_loss: 0.6904 - val_accuracy: 0.5339\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6736 - accuracy: 0.5334 - val_loss: 0.6921 - val_accuracy: 0.5349\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6739 - accuracy: 0.5332 - val_loss: 0.6930 - val_accuracy: 0.5354\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6732 - accuracy: 0.5341 - val_loss: 0.6926 - val_accuracy: 0.5349\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6730 - accuracy: 0.5337 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6726 - accuracy: 0.5344 - val_loss: 0.6956 - val_accuracy: 0.5343\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6722 - accuracy: 0.5343 - val_loss: 0.6931 - val_accuracy: 0.5346\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6719 - accuracy: 0.5350 - val_loss: 0.6917 - val_accuracy: 0.5331\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6718 - accuracy: 0.5348 - val_loss: 0.6927 - val_accuracy: 0.5344\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6710 - accuracy: 0.5356 - val_loss: 0.6931 - val_accuracy: 0.5344\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6706 - accuracy: 0.5356 - val_loss: 0.6952 - val_accuracy: 0.5347\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6705 - accuracy: 0.5355 - val_loss: 0.6943 - val_accuracy: 0.5347\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6703 - accuracy: 0.5359 - val_loss: 0.6963 - val_accuracy: 0.5355\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6696 - accuracy: 0.5360 - val_loss: 0.6978 - val_accuracy: 0.5352\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6697 - accuracy: 0.5363 - val_loss: 0.6966 - val_accuracy: 0.5343\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6694 - accuracy: 0.5365 - val_loss: 0.6994 - val_accuracy: 0.5354\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6691 - accuracy: 0.5365 - val_loss: 0.6977 - val_accuracy: 0.5347\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6687 - accuracy: 0.5371 - val_loss: 0.6955 - val_accuracy: 0.5340\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6685 - accuracy: 0.5367 - val_loss: 0.6974 - val_accuracy: 0.5343\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6681 - accuracy: 0.5373 - val_loss: 0.6985 - val_accuracy: 0.5349\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6683 - accuracy: 0.5372 - val_loss: 0.6973 - val_accuracy: 0.5348\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6676 - accuracy: 0.5378 - val_loss: 0.6980 - val_accuracy: 0.5347\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6675 - accuracy: 0.5323 - val_loss: 0.7017 - val_accuracy: 0.5354\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6676 - accuracy: 0.5376 - val_loss: 0.6986 - val_accuracy: 0.5347\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6673 - accuracy: 0.5381 - val_loss: 0.6990 - val_accuracy: 0.5345\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6668 - accuracy: 0.5381 - val_loss: 0.6988 - val_accuracy: 0.5346\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6667 - accuracy: 0.5385 - val_loss: 0.7003 - val_accuracy: 0.5340\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6663 - accuracy: 0.5337 - val_loss: 0.7028 - val_accuracy: 0.5350\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6666 - accuracy: 0.5383 - val_loss: 0.7004 - val_accuracy: 0.5346\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6663 - accuracy: 0.5385 - val_loss: 0.7010 - val_accuracy: 0.5346\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6665 - accuracy: 0.5383 - val_loss: 0.7005 - val_accuracy: 0.5349\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6658 - accuracy: 0.5388 - val_loss: 0.6981 - val_accuracy: 0.5339\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6659 - accuracy: 0.5389 - val_loss: 0.7017 - val_accuracy: 0.5351\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6654 - accuracy: 0.5392 - val_loss: 0.7012 - val_accuracy: 0.5350\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6655 - accuracy: 0.5390 - val_loss: 0.7016 - val_accuracy: 0.5348\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6651 - accuracy: 0.5397 - val_loss: 0.7039 - val_accuracy: 0.5352\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6651 - accuracy: 0.5395 - val_loss: 0.7005 - val_accuracy: 0.5347\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6650 - accuracy: 0.5396 - val_loss: 0.7040 - val_accuracy: 0.5350\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6647 - accuracy: 0.5395 - val_loss: 0.7035 - val_accuracy: 0.5341\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6648 - accuracy: 0.5395 - val_loss: 0.7042 - val_accuracy: 0.5350\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6644 - accuracy: 0.5399 - val_loss: 0.7048 - val_accuracy: 0.5350\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6639 - accuracy: 0.5403 - val_loss: 0.7046 - val_accuracy: 0.5352\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6637 - accuracy: 0.5403 - val_loss: 0.7036 - val_accuracy: 0.5345\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6642 - accuracy: 0.5401 - val_loss: 0.7068 - val_accuracy: 0.5346\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6640 - accuracy: 0.5401 - val_loss: 0.7054 - val_accuracy: 0.5348\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6639 - accuracy: 0.5401 - val_loss: 0.7064 - val_accuracy: 0.5352\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 4s 60ms/step - loss: 0.6636 - accuracy: 0.5404 - val_loss: 0.7053 - val_accuracy: 0.5348\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6638 - accuracy: 0.5384 - val_loss: 0.7035 - val_accuracy: 0.5347\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6637 - accuracy: 0.5405 - val_loss: 0.7051 - val_accuracy: 0.5343\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6633 - accuracy: 0.5404 - val_loss: 0.7051 - val_accuracy: 0.5347\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6632 - accuracy: 0.5406 - val_loss: 0.7054 - val_accuracy: 0.5345\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6630 - accuracy: 0.5408 - val_loss: 0.7089 - val_accuracy: 0.5347\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6630 - accuracy: 0.5409 - val_loss: 0.7096 - val_accuracy: 0.5348\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6633 - accuracy: 0.5404 - val_loss: 0.7108 - val_accuracy: 0.5352\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6630 - accuracy: 0.5407 - val_loss: 0.7078 - val_accuracy: 0.5351\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6626 - accuracy: 0.5412 - val_loss: 0.7063 - val_accuracy: 0.5344\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6625 - accuracy: 0.5413 - val_loss: 0.7061 - val_accuracy: 0.5347\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6626 - accuracy: 0.5413 - val_loss: 0.7061 - val_accuracy: 0.5349\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6624 - accuracy: 0.5415 - val_loss: 0.7089 - val_accuracy: 0.5352\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6621 - accuracy: 0.5416 - val_loss: 0.7077 - val_accuracy: 0.5345\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6619 - accuracy: 0.5417 - val_loss: 0.7069 - val_accuracy: 0.5345\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6620 - accuracy: 0.5415 - val_loss: 0.7081 - val_accuracy: 0.5347\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6620 - accuracy: 0.5413 - val_loss: 0.7084 - val_accuracy: 0.5350\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6618 - accuracy: 0.5414 - val_loss: 0.7086 - val_accuracy: 0.5353\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6617 - accuracy: 0.5417 - val_loss: 0.7086 - val_accuracy: 0.5347\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6615 - accuracy: 0.5419 - val_loss: 0.7098 - val_accuracy: 0.5348\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6615 - accuracy: 0.5395 - val_loss: 0.7087 - val_accuracy: 0.5353\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6617 - accuracy: 0.5417 - val_loss: 0.7068 - val_accuracy: 0.5344\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6617 - accuracy: 0.5418 - val_loss: 0.7069 - val_accuracy: 0.5345\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6611 - accuracy: 0.5418 - val_loss: 0.7112 - val_accuracy: 0.5350\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6609 - accuracy: 0.5421 - val_loss: 0.7101 - val_accuracy: 0.5350\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6613 - accuracy: 0.5417 - val_loss: 0.7092 - val_accuracy: 0.5348\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6613 - accuracy: 0.5420 - val_loss: 0.7114 - val_accuracy: 0.5348\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6608 - accuracy: 0.5423 - val_loss: 0.7088 - val_accuracy: 0.5346\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6606 - accuracy: 0.5423 - val_loss: 0.7092 - val_accuracy: 0.5343\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6607 - accuracy: 0.5423 - val_loss: 0.7066 - val_accuracy: 0.5341\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6614 - accuracy: 0.5408 - val_loss: 0.7106 - val_accuracy: 0.5355\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6606 - accuracy: 0.5426 - val_loss: 0.7112 - val_accuracy: 0.5348\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6606 - accuracy: 0.5424 - val_loss: 0.7103 - val_accuracy: 0.5340\n",
            "19/19 [==============================] - 1s 17ms/step - loss: 0.7122 - accuracy: 0.5332\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 6s 71ms/step - loss: 0.6907 - accuracy: 0.5046 - val_loss: 0.6899 - val_accuracy: 0.5098\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6871 - accuracy: 0.5271 - val_loss: 0.6888 - val_accuracy: 0.5099\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6853 - accuracy: 0.5285 - val_loss: 0.6889 - val_accuracy: 0.5105\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6840 - accuracy: 0.5300 - val_loss: 0.6892 - val_accuracy: 0.5095\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6834 - accuracy: 0.5305 - val_loss: 0.6897 - val_accuracy: 0.5105\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 61ms/step - loss: 0.6819 - accuracy: 0.5313 - val_loss: 0.6898 - val_accuracy: 0.5096\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6811 - accuracy: 0.5326 - val_loss: 0.6912 - val_accuracy: 0.5078\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6800 - accuracy: 0.5334 - val_loss: 0.6909 - val_accuracy: 0.5098\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6797 - accuracy: 0.5341 - val_loss: 0.6918 - val_accuracy: 0.5102\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6789 - accuracy: 0.5345 - val_loss: 0.6924 - val_accuracy: 0.5101\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6783 - accuracy: 0.5352 - val_loss: 0.6935 - val_accuracy: 0.5088\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6775 - accuracy: 0.5354 - val_loss: 0.6935 - val_accuracy: 0.5088\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6770 - accuracy: 0.5359 - val_loss: 0.6954 - val_accuracy: 0.5077\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6761 - accuracy: 0.5368 - val_loss: 0.6963 - val_accuracy: 0.5080\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6757 - accuracy: 0.5371 - val_loss: 0.6969 - val_accuracy: 0.5079\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6747 - accuracy: 0.5374 - val_loss: 0.6989 - val_accuracy: 0.5081\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6753 - accuracy: 0.5372 - val_loss: 0.6976 - val_accuracy: 0.5084\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6740 - accuracy: 0.5384 - val_loss: 0.7002 - val_accuracy: 0.5085\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6740 - accuracy: 0.5383 - val_loss: 0.7001 - val_accuracy: 0.5082\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6733 - accuracy: 0.5389 - val_loss: 0.6989 - val_accuracy: 0.5078\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6730 - accuracy: 0.5391 - val_loss: 0.7021 - val_accuracy: 0.5082\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6720 - accuracy: 0.5398 - val_loss: 0.7017 - val_accuracy: 0.5073\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6717 - accuracy: 0.5400 - val_loss: 0.7017 - val_accuracy: 0.5053\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6722 - accuracy: 0.5395 - val_loss: 0.7029 - val_accuracy: 0.5052\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6713 - accuracy: 0.5402 - val_loss: 0.7035 - val_accuracy: 0.5067\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6711 - accuracy: 0.5405 - val_loss: 0.7037 - val_accuracy: 0.5065\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6708 - accuracy: 0.5410 - val_loss: 0.7037 - val_accuracy: 0.5053\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6703 - accuracy: 0.5410 - val_loss: 0.7081 - val_accuracy: 0.5081\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6697 - accuracy: 0.5412 - val_loss: 0.7095 - val_accuracy: 0.5083\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6695 - accuracy: 0.5414 - val_loss: 0.7089 - val_accuracy: 0.5073\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6692 - accuracy: 0.5419 - val_loss: 0.7079 - val_accuracy: 0.5069\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6692 - accuracy: 0.5416 - val_loss: 0.7083 - val_accuracy: 0.5065\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6688 - accuracy: 0.5421 - val_loss: 0.7099 - val_accuracy: 0.5074\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6682 - accuracy: 0.5427 - val_loss: 0.7088 - val_accuracy: 0.5061\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6680 - accuracy: 0.5424 - val_loss: 0.7100 - val_accuracy: 0.5060\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6679 - accuracy: 0.5431 - val_loss: 0.7092 - val_accuracy: 0.5052\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6674 - accuracy: 0.5433 - val_loss: 0.7098 - val_accuracy: 0.5064\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6671 - accuracy: 0.5434 - val_loss: 0.7159 - val_accuracy: 0.5060\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6676 - accuracy: 0.5432 - val_loss: 0.7123 - val_accuracy: 0.5071\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6664 - accuracy: 0.5436 - val_loss: 0.7160 - val_accuracy: 0.5077\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6665 - accuracy: 0.5439 - val_loss: 0.7120 - val_accuracy: 0.5064\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6661 - accuracy: 0.5440 - val_loss: 0.7176 - val_accuracy: 0.5083\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6663 - accuracy: 0.5439 - val_loss: 0.7177 - val_accuracy: 0.5078\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6656 - accuracy: 0.5445 - val_loss: 0.7158 - val_accuracy: 0.5071\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6653 - accuracy: 0.5446 - val_loss: 0.7159 - val_accuracy: 0.5063\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6654 - accuracy: 0.5445 - val_loss: 0.7172 - val_accuracy: 0.5067\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6649 - accuracy: 0.5448 - val_loss: 0.7180 - val_accuracy: 0.5074\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6650 - accuracy: 0.5450 - val_loss: 0.7188 - val_accuracy: 0.5068\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6650 - accuracy: 0.5355 - val_loss: 0.7185 - val_accuracy: 0.5067\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6640 - accuracy: 0.5456 - val_loss: 0.7259 - val_accuracy: 0.5081\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6645 - accuracy: 0.5449 - val_loss: 0.7181 - val_accuracy: 0.5062\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6642 - accuracy: 0.5455 - val_loss: 0.7182 - val_accuracy: 0.5062\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6640 - accuracy: 0.5455 - val_loss: 0.7181 - val_accuracy: 0.5051\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6642 - accuracy: 0.5455 - val_loss: 0.7190 - val_accuracy: 0.5067\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6635 - accuracy: 0.5459 - val_loss: 0.7184 - val_accuracy: 0.5060\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6638 - accuracy: 0.5458 - val_loss: 0.7184 - val_accuracy: 0.5062\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6628 - accuracy: 0.5464 - val_loss: 0.7217 - val_accuracy: 0.5066\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6630 - accuracy: 0.5461 - val_loss: 0.7189 - val_accuracy: 0.5065\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6628 - accuracy: 0.5466 - val_loss: 0.7257 - val_accuracy: 0.5086\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6634 - accuracy: 0.5458 - val_loss: 0.7198 - val_accuracy: 0.5048\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6626 - accuracy: 0.5463 - val_loss: 0.7218 - val_accuracy: 0.5064\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6626 - accuracy: 0.5463 - val_loss: 0.7217 - val_accuracy: 0.5069\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6622 - accuracy: 0.5469 - val_loss: 0.7235 - val_accuracy: 0.5061\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6617 - accuracy: 0.5471 - val_loss: 0.7230 - val_accuracy: 0.5067\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6620 - accuracy: 0.5468 - val_loss: 0.7185 - val_accuracy: 0.5053\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6621 - accuracy: 0.5469 - val_loss: 0.7225 - val_accuracy: 0.5059\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6614 - accuracy: 0.5471 - val_loss: 0.7229 - val_accuracy: 0.5069\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6612 - accuracy: 0.5477 - val_loss: 0.7250 - val_accuracy: 0.5062\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6619 - accuracy: 0.5473 - val_loss: 0.7231 - val_accuracy: 0.5063\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6612 - accuracy: 0.5475 - val_loss: 0.7232 - val_accuracy: 0.5068\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6614 - accuracy: 0.5472 - val_loss: 0.7223 - val_accuracy: 0.5054\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6609 - accuracy: 0.5478 - val_loss: 0.7260 - val_accuracy: 0.5065\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6608 - accuracy: 0.5476 - val_loss: 0.7245 - val_accuracy: 0.5066\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6610 - accuracy: 0.5479 - val_loss: 0.7235 - val_accuracy: 0.5054\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6607 - accuracy: 0.5478 - val_loss: 0.7234 - val_accuracy: 0.5059\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6606 - accuracy: 0.5481 - val_loss: 0.7243 - val_accuracy: 0.5053\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6603 - accuracy: 0.5481 - val_loss: 0.7257 - val_accuracy: 0.5066\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6605 - accuracy: 0.5482 - val_loss: 0.7257 - val_accuracy: 0.5061\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6602 - accuracy: 0.5480 - val_loss: 0.7242 - val_accuracy: 0.5052\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6607 - accuracy: 0.5479 - val_loss: 0.7254 - val_accuracy: 0.5067\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6602 - accuracy: 0.5483 - val_loss: 0.7261 - val_accuracy: 0.5064\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6601 - accuracy: 0.5482 - val_loss: 0.7243 - val_accuracy: 0.5063\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6595 - accuracy: 0.5485 - val_loss: 0.7246 - val_accuracy: 0.5058\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6597 - accuracy: 0.5485 - val_loss: 0.7237 - val_accuracy: 0.5060\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6598 - accuracy: 0.5485 - val_loss: 0.7258 - val_accuracy: 0.5061\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6597 - accuracy: 0.5487 - val_loss: 0.7253 - val_accuracy: 0.5058\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6596 - accuracy: 0.5489 - val_loss: 0.7230 - val_accuracy: 0.5061\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6601 - accuracy: 0.5482 - val_loss: 0.7241 - val_accuracy: 0.5061\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6596 - accuracy: 0.5486 - val_loss: 0.7250 - val_accuracy: 0.5055\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6594 - accuracy: 0.5484 - val_loss: 0.7270 - val_accuracy: 0.5063\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6594 - accuracy: 0.5487 - val_loss: 0.7268 - val_accuracy: 0.5060\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6593 - accuracy: 0.5486 - val_loss: 0.7269 - val_accuracy: 0.5070\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6591 - accuracy: 0.5490 - val_loss: 0.7269 - val_accuracy: 0.5060\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6592 - accuracy: 0.5490 - val_loss: 0.7266 - val_accuracy: 0.5059\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6589 - accuracy: 0.5491 - val_loss: 0.7268 - val_accuracy: 0.5060\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6590 - accuracy: 0.5490 - val_loss: 0.7264 - val_accuracy: 0.5063\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6594 - accuracy: 0.5490 - val_loss: 0.7257 - val_accuracy: 0.5062\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6585 - accuracy: 0.5493 - val_loss: 0.7276 - val_accuracy: 0.5065\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6587 - accuracy: 0.5492 - val_loss: 0.7281 - val_accuracy: 0.5065\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6583 - accuracy: 0.5496 - val_loss: 0.7249 - val_accuracy: 0.5059\n",
            "19/19 [==============================] - 1s 18ms/step - loss: 0.7246 - accuracy: 0.5058\n",
            "Best accuracy for dataset 2: 0.5439110994338989\n",
            "2308\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 83ms/step - loss: 0.6889 - accuracy: 0.5069 - val_loss: 0.6882 - val_accuracy: 0.5351\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6870 - accuracy: 0.5198 - val_loss: 0.6870 - val_accuracy: 0.5410\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6854 - accuracy: 0.5217 - val_loss: 0.6869 - val_accuracy: 0.5408\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6845 - accuracy: 0.5229 - val_loss: 0.6874 - val_accuracy: 0.5416\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6836 - accuracy: 0.5238 - val_loss: 0.6868 - val_accuracy: 0.5420\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6829 - accuracy: 0.5247 - val_loss: 0.6879 - val_accuracy: 0.5410\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6824 - accuracy: 0.5253 - val_loss: 0.6869 - val_accuracy: 0.5418\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6818 - accuracy: 0.5264 - val_loss: 0.6873 - val_accuracy: 0.5418\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6805 - accuracy: 0.5266 - val_loss: 0.6878 - val_accuracy: 0.5411\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 4s 61ms/step - loss: 0.6800 - accuracy: 0.5268 - val_loss: 0.6880 - val_accuracy: 0.5418\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6794 - accuracy: 0.5276 - val_loss: 0.6883 - val_accuracy: 0.5407\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6787 - accuracy: 0.5283 - val_loss: 0.6883 - val_accuracy: 0.5416\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6781 - accuracy: 0.5283 - val_loss: 0.6885 - val_accuracy: 0.5402\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6770 - accuracy: 0.5294 - val_loss: 0.6896 - val_accuracy: 0.5411\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6766 - accuracy: 0.5296 - val_loss: 0.6898 - val_accuracy: 0.5408\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6766 - accuracy: 0.5299 - val_loss: 0.6908 - val_accuracy: 0.5414\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6758 - accuracy: 0.5301 - val_loss: 0.6902 - val_accuracy: 0.5411\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6755 - accuracy: 0.5306 - val_loss: 0.6903 - val_accuracy: 0.5408\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6747 - accuracy: 0.5307 - val_loss: 0.6901 - val_accuracy: 0.5406\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6740 - accuracy: 0.5316 - val_loss: 0.6911 - val_accuracy: 0.5405\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6737 - accuracy: 0.5314 - val_loss: 0.6929 - val_accuracy: 0.5409\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6732 - accuracy: 0.5318 - val_loss: 0.6932 - val_accuracy: 0.5402\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6732 - accuracy: 0.5317 - val_loss: 0.6911 - val_accuracy: 0.5404\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6721 - accuracy: 0.5325 - val_loss: 0.6925 - val_accuracy: 0.5407\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6719 - accuracy: 0.5328 - val_loss: 0.6923 - val_accuracy: 0.5400\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6714 - accuracy: 0.5335 - val_loss: 0.6953 - val_accuracy: 0.5399\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6713 - accuracy: 0.5332 - val_loss: 0.6928 - val_accuracy: 0.5398\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6705 - accuracy: 0.5338 - val_loss: 0.6938 - val_accuracy: 0.5396\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6703 - accuracy: 0.5341 - val_loss: 0.6946 - val_accuracy: 0.5390\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6700 - accuracy: 0.5341 - val_loss: 0.6942 - val_accuracy: 0.5400\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6696 - accuracy: 0.5346 - val_loss: 0.6947 - val_accuracy: 0.5393\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6693 - accuracy: 0.5347 - val_loss: 0.6969 - val_accuracy: 0.5400\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6690 - accuracy: 0.5349 - val_loss: 0.6960 - val_accuracy: 0.5392\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6686 - accuracy: 0.5353 - val_loss: 0.6979 - val_accuracy: 0.5397\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6691 - accuracy: 0.5349 - val_loss: 0.6976 - val_accuracy: 0.5401\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6683 - accuracy: 0.5357 - val_loss: 0.6948 - val_accuracy: 0.5385\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6681 - accuracy: 0.5291 - val_loss: 0.6968 - val_accuracy: 0.5394\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6675 - accuracy: 0.5363 - val_loss: 0.6993 - val_accuracy: 0.5397\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6672 - accuracy: 0.5361 - val_loss: 0.6984 - val_accuracy: 0.5401\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6670 - accuracy: 0.5364 - val_loss: 0.6998 - val_accuracy: 0.5393\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6667 - accuracy: 0.5366 - val_loss: 0.6997 - val_accuracy: 0.5390\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6665 - accuracy: 0.5368 - val_loss: 0.7022 - val_accuracy: 0.5391\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6663 - accuracy: 0.5369 - val_loss: 0.7026 - val_accuracy: 0.5394\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6664 - accuracy: 0.5370 - val_loss: 0.7015 - val_accuracy: 0.5385\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6659 - accuracy: 0.5372 - val_loss: 0.7001 - val_accuracy: 0.5389\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 6s 89ms/step - loss: 0.6657 - accuracy: 0.5374 - val_loss: 0.7009 - val_accuracy: 0.5385\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6650 - accuracy: 0.5378 - val_loss: 0.7015 - val_accuracy: 0.5387\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6652 - accuracy: 0.5376 - val_loss: 0.7017 - val_accuracy: 0.5371\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6653 - accuracy: 0.5376 - val_loss: 0.7025 - val_accuracy: 0.5379\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6647 - accuracy: 0.5381 - val_loss: 0.7023 - val_accuracy: 0.5388\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6643 - accuracy: 0.5381 - val_loss: 0.7040 - val_accuracy: 0.5393\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6642 - accuracy: 0.5387 - val_loss: 0.7099 - val_accuracy: 0.5392\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6642 - accuracy: 0.5384 - val_loss: 0.7053 - val_accuracy: 0.5388\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6635 - accuracy: 0.5389 - val_loss: 0.7097 - val_accuracy: 0.5393\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6637 - accuracy: 0.5374 - val_loss: 0.7052 - val_accuracy: 0.5384\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6635 - accuracy: 0.5386 - val_loss: 0.7054 - val_accuracy: 0.5381\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6634 - accuracy: 0.5388 - val_loss: 0.7057 - val_accuracy: 0.5379\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6630 - accuracy: 0.5392 - val_loss: 0.7073 - val_accuracy: 0.5392\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6629 - accuracy: 0.5391 - val_loss: 0.7069 - val_accuracy: 0.5379\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6633 - accuracy: 0.5390 - val_loss: 0.7063 - val_accuracy: 0.5382\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6628 - accuracy: 0.5393 - val_loss: 0.7102 - val_accuracy: 0.5391\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6624 - accuracy: 0.5395 - val_loss: 0.7080 - val_accuracy: 0.5388\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6624 - accuracy: 0.5395 - val_loss: 0.7071 - val_accuracy: 0.5385\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6623 - accuracy: 0.5397 - val_loss: 0.7075 - val_accuracy: 0.5393\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6622 - accuracy: 0.5386 - val_loss: 0.7086 - val_accuracy: 0.5388\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6619 - accuracy: 0.5398 - val_loss: 0.7078 - val_accuracy: 0.5390\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6616 - accuracy: 0.5399 - val_loss: 0.7076 - val_accuracy: 0.5388\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6616 - accuracy: 0.5402 - val_loss: 0.7080 - val_accuracy: 0.5389\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6612 - accuracy: 0.5384 - val_loss: 0.7125 - val_accuracy: 0.5391\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6612 - accuracy: 0.5402 - val_loss: 0.7105 - val_accuracy: 0.5396\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6613 - accuracy: 0.5405 - val_loss: 0.7098 - val_accuracy: 0.5376\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6610 - accuracy: 0.5406 - val_loss: 0.7103 - val_accuracy: 0.5401\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6610 - accuracy: 0.5406 - val_loss: 0.7105 - val_accuracy: 0.5388\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6613 - accuracy: 0.5401 - val_loss: 0.7111 - val_accuracy: 0.5396\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6612 - accuracy: 0.5402 - val_loss: 0.7088 - val_accuracy: 0.5388\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6607 - accuracy: 0.5408 - val_loss: 0.7121 - val_accuracy: 0.5395\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6605 - accuracy: 0.5409 - val_loss: 0.7112 - val_accuracy: 0.5388\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6605 - accuracy: 0.5407 - val_loss: 0.7121 - val_accuracy: 0.5396\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6603 - accuracy: 0.5409 - val_loss: 0.7134 - val_accuracy: 0.5380\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6602 - accuracy: 0.5408 - val_loss: 0.7117 - val_accuracy: 0.5386\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6601 - accuracy: 0.5411 - val_loss: 0.7128 - val_accuracy: 0.5396\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6601 - accuracy: 0.5409 - val_loss: 0.7122 - val_accuracy: 0.5393\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6599 - accuracy: 0.5407 - val_loss: 0.7130 - val_accuracy: 0.5390\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 4s 62ms/step - loss: 0.6605 - accuracy: 0.5404 - val_loss: 0.7120 - val_accuracy: 0.5388\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6604 - accuracy: 0.5403 - val_loss: 0.7119 - val_accuracy: 0.5376\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6599 - accuracy: 0.5415 - val_loss: 0.7131 - val_accuracy: 0.5384\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6598 - accuracy: 0.5414 - val_loss: 0.7128 - val_accuracy: 0.5378\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6597 - accuracy: 0.5414 - val_loss: 0.7137 - val_accuracy: 0.5386\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6594 - accuracy: 0.5414 - val_loss: 0.7106 - val_accuracy: 0.5383\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6595 - accuracy: 0.5417 - val_loss: 0.7131 - val_accuracy: 0.5380\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6596 - accuracy: 0.5414 - val_loss: 0.7147 - val_accuracy: 0.5396\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6594 - accuracy: 0.5417 - val_loss: 0.7159 - val_accuracy: 0.5392\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 62ms/step - loss: 0.6587 - accuracy: 0.5423 - val_loss: 0.7147 - val_accuracy: 0.5384\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6589 - accuracy: 0.5417 - val_loss: 0.7160 - val_accuracy: 0.5393\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6596 - accuracy: 0.5372 - val_loss: 0.7124 - val_accuracy: 0.5388\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6590 - accuracy: 0.5418 - val_loss: 0.7178 - val_accuracy: 0.5395\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6590 - accuracy: 0.5418 - val_loss: 0.7167 - val_accuracy: 0.5396\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6588 - accuracy: 0.5419 - val_loss: 0.7178 - val_accuracy: 0.5396\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6584 - accuracy: 0.5420 - val_loss: 0.7173 - val_accuracy: 0.5391\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6585 - accuracy: 0.5422 - val_loss: 0.7162 - val_accuracy: 0.5395\n",
            "19/19 [==============================] - 1s 19ms/step - loss: 0.7218 - accuracy: 0.5374\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 8s 87ms/step - loss: 0.6893 - accuracy: 0.5062 - val_loss: 0.6892 - val_accuracy: 0.5303\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6864 - accuracy: 0.5224 - val_loss: 0.6888 - val_accuracy: 0.5318\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6852 - accuracy: 0.5237 - val_loss: 0.6890 - val_accuracy: 0.5324\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6841 - accuracy: 0.5250 - val_loss: 0.6886 - val_accuracy: 0.5329\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6830 - accuracy: 0.5252 - val_loss: 0.6892 - val_accuracy: 0.5331\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6825 - accuracy: 0.5263 - val_loss: 0.6894 - val_accuracy: 0.5340\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6814 - accuracy: 0.5273 - val_loss: 0.6897 - val_accuracy: 0.5344\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6807 - accuracy: 0.5283 - val_loss: 0.6904 - val_accuracy: 0.5340\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6800 - accuracy: 0.5286 - val_loss: 0.6898 - val_accuracy: 0.5342\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6792 - accuracy: 0.5290 - val_loss: 0.6906 - val_accuracy: 0.5342\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6785 - accuracy: 0.5298 - val_loss: 0.6908 - val_accuracy: 0.5344\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6778 - accuracy: 0.5301 - val_loss: 0.6910 - val_accuracy: 0.5335\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6772 - accuracy: 0.5305 - val_loss: 0.6903 - val_accuracy: 0.5333\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6769 - accuracy: 0.5309 - val_loss: 0.6925 - val_accuracy: 0.5337\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6762 - accuracy: 0.5311 - val_loss: 0.6934 - val_accuracy: 0.5345\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6753 - accuracy: 0.5320 - val_loss: 0.6923 - val_accuracy: 0.5333\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6749 - accuracy: 0.5322 - val_loss: 0.6951 - val_accuracy: 0.5340\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6748 - accuracy: 0.5324 - val_loss: 0.6943 - val_accuracy: 0.5337\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6745 - accuracy: 0.5325 - val_loss: 0.6937 - val_accuracy: 0.5336\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6733 - accuracy: 0.5332 - val_loss: 0.6957 - val_accuracy: 0.5335\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6732 - accuracy: 0.5334 - val_loss: 0.6923 - val_accuracy: 0.5324\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6726 - accuracy: 0.5339 - val_loss: 0.6970 - val_accuracy: 0.5341\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6718 - accuracy: 0.5346 - val_loss: 0.6968 - val_accuracy: 0.5334\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6720 - accuracy: 0.5344 - val_loss: 0.6987 - val_accuracy: 0.5334\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6715 - accuracy: 0.5346 - val_loss: 0.6975 - val_accuracy: 0.5336\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6710 - accuracy: 0.5351 - val_loss: 0.6989 - val_accuracy: 0.5338\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6706 - accuracy: 0.5352 - val_loss: 0.6950 - val_accuracy: 0.5331\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6707 - accuracy: 0.5353 - val_loss: 0.6990 - val_accuracy: 0.5335\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6696 - accuracy: 0.5358 - val_loss: 0.6994 - val_accuracy: 0.5334\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6694 - accuracy: 0.5361 - val_loss: 0.7037 - val_accuracy: 0.5335\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6694 - accuracy: 0.5362 - val_loss: 0.7020 - val_accuracy: 0.5334\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6689 - accuracy: 0.5362 - val_loss: 0.7008 - val_accuracy: 0.5332\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6684 - accuracy: 0.5365 - val_loss: 0.7019 - val_accuracy: 0.5333\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6680 - accuracy: 0.5369 - val_loss: 0.7011 - val_accuracy: 0.5334\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6680 - accuracy: 0.5372 - val_loss: 0.7042 - val_accuracy: 0.5330\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6680 - accuracy: 0.5371 - val_loss: 0.7004 - val_accuracy: 0.5324\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6679 - accuracy: 0.5371 - val_loss: 0.6999 - val_accuracy: 0.5324\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6674 - accuracy: 0.5375 - val_loss: 0.7005 - val_accuracy: 0.5324\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6670 - accuracy: 0.5377 - val_loss: 0.7058 - val_accuracy: 0.5335\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6667 - accuracy: 0.5379 - val_loss: 0.7040 - val_accuracy: 0.5325\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6665 - accuracy: 0.5383 - val_loss: 0.7035 - val_accuracy: 0.5324\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6663 - accuracy: 0.5383 - val_loss: 0.7032 - val_accuracy: 0.5326\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6657 - accuracy: 0.5385 - val_loss: 0.7022 - val_accuracy: 0.5326\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6657 - accuracy: 0.5387 - val_loss: 0.7052 - val_accuracy: 0.5327\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6655 - accuracy: 0.5385 - val_loss: 0.7058 - val_accuracy: 0.5331\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6653 - accuracy: 0.5389 - val_loss: 0.7041 - val_accuracy: 0.5327\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6648 - accuracy: 0.5392 - val_loss: 0.7103 - val_accuracy: 0.5334\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6647 - accuracy: 0.5393 - val_loss: 0.7084 - val_accuracy: 0.5332\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6644 - accuracy: 0.5394 - val_loss: 0.7050 - val_accuracy: 0.5330\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6643 - accuracy: 0.5395 - val_loss: 0.7113 - val_accuracy: 0.5330\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6643 - accuracy: 0.5396 - val_loss: 0.7077 - val_accuracy: 0.5325\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6645 - accuracy: 0.5394 - val_loss: 0.7084 - val_accuracy: 0.5329\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6635 - accuracy: 0.5400 - val_loss: 0.7085 - val_accuracy: 0.5326\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6644 - accuracy: 0.5398 - val_loss: 0.7113 - val_accuracy: 0.5331\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6635 - accuracy: 0.5401 - val_loss: 0.7108 - val_accuracy: 0.5330\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6640 - accuracy: 0.5400 - val_loss: 0.7128 - val_accuracy: 0.5333\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6631 - accuracy: 0.5406 - val_loss: 0.7112 - val_accuracy: 0.5330\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6632 - accuracy: 0.5402 - val_loss: 0.7109 - val_accuracy: 0.5334\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6628 - accuracy: 0.5404 - val_loss: 0.7123 - val_accuracy: 0.5329\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6625 - accuracy: 0.5407 - val_loss: 0.7144 - val_accuracy: 0.5333\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6627 - accuracy: 0.5408 - val_loss: 0.7130 - val_accuracy: 0.5327\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6624 - accuracy: 0.5409 - val_loss: 0.7133 - val_accuracy: 0.5330\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6623 - accuracy: 0.5411 - val_loss: 0.7110 - val_accuracy: 0.5325\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6620 - accuracy: 0.5410 - val_loss: 0.7116 - val_accuracy: 0.5325\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6622 - accuracy: 0.5413 - val_loss: 0.7099 - val_accuracy: 0.5326\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6619 - accuracy: 0.5413 - val_loss: 0.7129 - val_accuracy: 0.5330\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6620 - accuracy: 0.5413 - val_loss: 0.7146 - val_accuracy: 0.5331\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6618 - accuracy: 0.5413 - val_loss: 0.7148 - val_accuracy: 0.5330\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6611 - accuracy: 0.5418 - val_loss: 0.7144 - val_accuracy: 0.5327\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6617 - accuracy: 0.5415 - val_loss: 0.7157 - val_accuracy: 0.5328\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6612 - accuracy: 0.5417 - val_loss: 0.7129 - val_accuracy: 0.5330\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6614 - accuracy: 0.5418 - val_loss: 0.7123 - val_accuracy: 0.5325\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6609 - accuracy: 0.5418 - val_loss: 0.7179 - val_accuracy: 0.5330\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6612 - accuracy: 0.5417 - val_loss: 0.7151 - val_accuracy: 0.5328\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6608 - accuracy: 0.5421 - val_loss: 0.7187 - val_accuracy: 0.5331\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6606 - accuracy: 0.5420 - val_loss: 0.7175 - val_accuracy: 0.5327\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6605 - accuracy: 0.5424 - val_loss: 0.7139 - val_accuracy: 0.5322\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6606 - accuracy: 0.5423 - val_loss: 0.7146 - val_accuracy: 0.5325\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6605 - accuracy: 0.5423 - val_loss: 0.7149 - val_accuracy: 0.5322\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6604 - accuracy: 0.5422 - val_loss: 0.7168 - val_accuracy: 0.5325\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6602 - accuracy: 0.5424 - val_loss: 0.7192 - val_accuracy: 0.5326\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6599 - accuracy: 0.5426 - val_loss: 0.7179 - val_accuracy: 0.5328\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6598 - accuracy: 0.5428 - val_loss: 0.7151 - val_accuracy: 0.5319\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6600 - accuracy: 0.5424 - val_loss: 0.7191 - val_accuracy: 0.5328\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6601 - accuracy: 0.5427 - val_loss: 0.7184 - val_accuracy: 0.5329\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6600 - accuracy: 0.5427 - val_loss: 0.7202 - val_accuracy: 0.5329\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6596 - accuracy: 0.5426 - val_loss: 0.7167 - val_accuracy: 0.5326\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6596 - accuracy: 0.5431 - val_loss: 0.7179 - val_accuracy: 0.5327\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6596 - accuracy: 0.5427 - val_loss: 0.7216 - val_accuracy: 0.5331\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6592 - accuracy: 0.5431 - val_loss: 0.7185 - val_accuracy: 0.5327\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6593 - accuracy: 0.5433 - val_loss: 0.7175 - val_accuracy: 0.5327\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6592 - accuracy: 0.5433 - val_loss: 0.7198 - val_accuracy: 0.5327\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6596 - accuracy: 0.5427 - val_loss: 0.7193 - val_accuracy: 0.5323\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6593 - accuracy: 0.5377 - val_loss: 0.7167 - val_accuracy: 0.5328\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6590 - accuracy: 0.5428 - val_loss: 0.7158 - val_accuracy: 0.5324\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6590 - accuracy: 0.5434 - val_loss: 0.7179 - val_accuracy: 0.5324\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6589 - accuracy: 0.5431 - val_loss: 0.7182 - val_accuracy: 0.5322\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6591 - accuracy: 0.5431 - val_loss: 0.7203 - val_accuracy: 0.5331\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6592 - accuracy: 0.5430 - val_loss: 0.7196 - val_accuracy: 0.5326\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6589 - accuracy: 0.5433 - val_loss: 0.7204 - val_accuracy: 0.5327\n",
            "19/19 [==============================] - 1s 21ms/step - loss: 0.7261 - accuracy: 0.5311\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 8s 92ms/step - loss: 0.6907 - accuracy: 0.5175 - val_loss: 0.6896 - val_accuracy: 0.5056\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6873 - accuracy: 0.5281 - val_loss: 0.6868 - val_accuracy: 0.5068\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6857 - accuracy: 0.5303 - val_loss: 0.6878 - val_accuracy: 0.5040\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6847 - accuracy: 0.5317 - val_loss: 0.6890 - val_accuracy: 0.5023\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6839 - accuracy: 0.5324 - val_loss: 0.6870 - val_accuracy: 0.5057\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6832 - accuracy: 0.5333 - val_loss: 0.6877 - val_accuracy: 0.5044\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6826 - accuracy: 0.5336 - val_loss: 0.6870 - val_accuracy: 0.5061\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6819 - accuracy: 0.5344 - val_loss: 0.6884 - val_accuracy: 0.5041\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6814 - accuracy: 0.5351 - val_loss: 0.6880 - val_accuracy: 0.5059\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6801 - accuracy: 0.5359 - val_loss: 0.6892 - val_accuracy: 0.5063\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6800 - accuracy: 0.5359 - val_loss: 0.6893 - val_accuracy: 0.5062\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6789 - accuracy: 0.5368 - val_loss: 0.6913 - val_accuracy: 0.5036\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6790 - accuracy: 0.5369 - val_loss: 0.6913 - val_accuracy: 0.5045\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6781 - accuracy: 0.5376 - val_loss: 0.6915 - val_accuracy: 0.5041\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6771 - accuracy: 0.5383 - val_loss: 0.6920 - val_accuracy: 0.5034\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6767 - accuracy: 0.5386 - val_loss: 0.6933 - val_accuracy: 0.5055\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6769 - accuracy: 0.5384 - val_loss: 0.6943 - val_accuracy: 0.5057\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6759 - accuracy: 0.5395 - val_loss: 0.6966 - val_accuracy: 0.5018\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6759 - accuracy: 0.5391 - val_loss: 0.6940 - val_accuracy: 0.5023\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6747 - accuracy: 0.5403 - val_loss: 0.6953 - val_accuracy: 0.5015\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6746 - accuracy: 0.5401 - val_loss: 0.6958 - val_accuracy: 0.5045\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6742 - accuracy: 0.5405 - val_loss: 0.6958 - val_accuracy: 0.5043\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6733 - accuracy: 0.5412 - val_loss: 0.6983 - val_accuracy: 0.5016\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6731 - accuracy: 0.5414 - val_loss: 0.6988 - val_accuracy: 0.5028\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6727 - accuracy: 0.5410 - val_loss: 0.7001 - val_accuracy: 0.5035\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6720 - accuracy: 0.5419 - val_loss: 0.7004 - val_accuracy: 0.5051\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6720 - accuracy: 0.5420 - val_loss: 0.7000 - val_accuracy: 0.5049\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6716 - accuracy: 0.5422 - val_loss: 0.7014 - val_accuracy: 0.5039\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6712 - accuracy: 0.5424 - val_loss: 0.7028 - val_accuracy: 0.5035\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6706 - accuracy: 0.5431 - val_loss: 0.7021 - val_accuracy: 0.5026\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6705 - accuracy: 0.5431 - val_loss: 0.7026 - val_accuracy: 0.5013\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6703 - accuracy: 0.5430 - val_loss: 0.7018 - val_accuracy: 0.5024\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6695 - accuracy: 0.5436 - val_loss: 0.7041 - val_accuracy: 0.5044\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6698 - accuracy: 0.5435 - val_loss: 0.7047 - val_accuracy: 0.5045\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6690 - accuracy: 0.5441 - val_loss: 0.7047 - val_accuracy: 0.5023\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6687 - accuracy: 0.5444 - val_loss: 0.7066 - val_accuracy: 0.5045\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6686 - accuracy: 0.5446 - val_loss: 0.7066 - val_accuracy: 0.5029\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 63ms/step - loss: 0.6683 - accuracy: 0.5446 - val_loss: 0.7078 - val_accuracy: 0.5049\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6680 - accuracy: 0.5448 - val_loss: 0.7086 - val_accuracy: 0.5051\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6682 - accuracy: 0.5411 - val_loss: 0.7101 - val_accuracy: 0.5044\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6681 - accuracy: 0.5448 - val_loss: 0.7064 - val_accuracy: 0.5032\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6673 - accuracy: 0.5453 - val_loss: 0.7104 - val_accuracy: 0.5052\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6672 - accuracy: 0.5452 - val_loss: 0.7091 - val_accuracy: 0.5021\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6666 - accuracy: 0.5456 - val_loss: 0.7142 - val_accuracy: 0.5050\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6663 - accuracy: 0.5457 - val_loss: 0.7132 - val_accuracy: 0.5047\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6661 - accuracy: 0.5464 - val_loss: 0.7160 - val_accuracy: 0.5037\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6663 - accuracy: 0.5461 - val_loss: 0.7141 - val_accuracy: 0.5031\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6656 - accuracy: 0.5464 - val_loss: 0.7149 - val_accuracy: 0.5030\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6654 - accuracy: 0.5466 - val_loss: 0.7150 - val_accuracy: 0.5019\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6654 - accuracy: 0.5466 - val_loss: 0.7147 - val_accuracy: 0.5035\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6652 - accuracy: 0.5468 - val_loss: 0.7159 - val_accuracy: 0.5020\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6649 - accuracy: 0.5470 - val_loss: 0.7157 - val_accuracy: 0.5008\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6648 - accuracy: 0.5467 - val_loss: 0.7168 - val_accuracy: 0.5048\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6650 - accuracy: 0.5471 - val_loss: 0.7143 - val_accuracy: 0.5018\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6646 - accuracy: 0.5472 - val_loss: 0.7174 - val_accuracy: 0.5046\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6646 - accuracy: 0.5472 - val_loss: 0.7194 - val_accuracy: 0.5038\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6643 - accuracy: 0.5474 - val_loss: 0.7189 - val_accuracy: 0.5043\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6640 - accuracy: 0.5477 - val_loss: 0.7176 - val_accuracy: 0.5036\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6640 - accuracy: 0.5474 - val_loss: 0.7159 - val_accuracy: 0.5029\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6641 - accuracy: 0.5474 - val_loss: 0.7183 - val_accuracy: 0.5029\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6634 - accuracy: 0.5477 - val_loss: 0.7164 - val_accuracy: 0.5033\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6633 - accuracy: 0.5481 - val_loss: 0.7208 - val_accuracy: 0.5034\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6638 - accuracy: 0.5478 - val_loss: 0.7195 - val_accuracy: 0.5032\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6630 - accuracy: 0.5483 - val_loss: 0.7193 - val_accuracy: 0.5025\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6630 - accuracy: 0.5482 - val_loss: 0.7200 - val_accuracy: 0.5033\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6626 - accuracy: 0.5484 - val_loss: 0.7213 - val_accuracy: 0.5032\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6630 - accuracy: 0.5483 - val_loss: 0.7222 - val_accuracy: 0.5042\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6626 - accuracy: 0.5485 - val_loss: 0.7211 - val_accuracy: 0.5031\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6626 - accuracy: 0.5487 - val_loss: 0.7214 - val_accuracy: 0.5039\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6622 - accuracy: 0.5487 - val_loss: 0.7234 - val_accuracy: 0.5035\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6623 - accuracy: 0.5487 - val_loss: 0.7211 - val_accuracy: 0.5009\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6621 - accuracy: 0.5492 - val_loss: 0.7217 - val_accuracy: 0.5027\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6617 - accuracy: 0.5492 - val_loss: 0.7252 - val_accuracy: 0.5040\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6619 - accuracy: 0.5492 - val_loss: 0.7257 - val_accuracy: 0.5026\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6619 - accuracy: 0.5491 - val_loss: 0.7263 - val_accuracy: 0.5023\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6619 - accuracy: 0.5490 - val_loss: 0.7214 - val_accuracy: 0.5019\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6619 - accuracy: 0.5493 - val_loss: 0.7245 - val_accuracy: 0.5040\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6614 - accuracy: 0.5492 - val_loss: 0.7267 - val_accuracy: 0.5018\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6613 - accuracy: 0.5495 - val_loss: 0.7269 - val_accuracy: 0.5037\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6615 - accuracy: 0.5493 - val_loss: 0.7258 - val_accuracy: 0.5009\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6614 - accuracy: 0.5494 - val_loss: 0.7258 - val_accuracy: 0.5037\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6615 - accuracy: 0.5493 - val_loss: 0.7249 - val_accuracy: 0.5027\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6607 - accuracy: 0.5496 - val_loss: 0.7263 - val_accuracy: 0.5031\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6608 - accuracy: 0.5497 - val_loss: 0.7263 - val_accuracy: 0.5014\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6605 - accuracy: 0.5499 - val_loss: 0.7278 - val_accuracy: 0.5033\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6608 - accuracy: 0.5497 - val_loss: 0.7266 - val_accuracy: 0.5033\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6603 - accuracy: 0.5502 - val_loss: 0.7292 - val_accuracy: 0.5043\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6609 - accuracy: 0.5498 - val_loss: 0.7275 - val_accuracy: 0.5040\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6605 - accuracy: 0.5500 - val_loss: 0.7269 - val_accuracy: 0.5032\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6603 - accuracy: 0.5498 - val_loss: 0.7259 - val_accuracy: 0.5026\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6603 - accuracy: 0.5500 - val_loss: 0.7275 - val_accuracy: 0.5022\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6607 - accuracy: 0.5499 - val_loss: 0.7268 - val_accuracy: 0.5030\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6607 - accuracy: 0.5498 - val_loss: 0.7272 - val_accuracy: 0.5027\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6602 - accuracy: 0.5501 - val_loss: 0.7261 - val_accuracy: 0.5033\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6603 - accuracy: 0.5499 - val_loss: 0.7293 - val_accuracy: 0.5032\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6601 - accuracy: 0.5502 - val_loss: 0.7278 - val_accuracy: 0.5034\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6598 - accuracy: 0.5502 - val_loss: 0.7296 - val_accuracy: 0.5031\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6596 - accuracy: 0.5505 - val_loss: 0.7287 - val_accuracy: 0.5026\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6598 - accuracy: 0.5504 - val_loss: 0.7301 - val_accuracy: 0.5041\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6592 - accuracy: 0.5508 - val_loss: 0.7294 - val_accuracy: 0.5032\n",
            "19/19 [==============================] - 1s 20ms/step - loss: 0.7393 - accuracy: 0.4999\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 69ms/step - loss: 0.6905 - accuracy: 0.5061 - val_loss: 0.6872 - val_accuracy: 0.5304\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6880 - accuracy: 0.5203 - val_loss: 0.6846 - val_accuracy: 0.5336\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6865 - accuracy: 0.5223 - val_loss: 0.6836 - val_accuracy: 0.5374\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6854 - accuracy: 0.5235 - val_loss: 0.6834 - val_accuracy: 0.5377\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6847 - accuracy: 0.5246 - val_loss: 0.6829 - val_accuracy: 0.5366\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6841 - accuracy: 0.5248 - val_loss: 0.6830 - val_accuracy: 0.5387\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6832 - accuracy: 0.5263 - val_loss: 0.6825 - val_accuracy: 0.5384\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6823 - accuracy: 0.5269 - val_loss: 0.6825 - val_accuracy: 0.5394\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6817 - accuracy: 0.5278 - val_loss: 0.6830 - val_accuracy: 0.5386\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6811 - accuracy: 0.5278 - val_loss: 0.6828 - val_accuracy: 0.5390\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6801 - accuracy: 0.5286 - val_loss: 0.6839 - val_accuracy: 0.5379\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6796 - accuracy: 0.5291 - val_loss: 0.6831 - val_accuracy: 0.5392\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6791 - accuracy: 0.5293 - val_loss: 0.6835 - val_accuracy: 0.5384\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6781 - accuracy: 0.5306 - val_loss: 0.6839 - val_accuracy: 0.5380\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6778 - accuracy: 0.5306 - val_loss: 0.6840 - val_accuracy: 0.5388\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6776 - accuracy: 0.5306 - val_loss: 0.6841 - val_accuracy: 0.5388\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6767 - accuracy: 0.5316 - val_loss: 0.6844 - val_accuracy: 0.5389\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6759 - accuracy: 0.5317 - val_loss: 0.6851 - val_accuracy: 0.5387\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6755 - accuracy: 0.5321 - val_loss: 0.6846 - val_accuracy: 0.5370\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6753 - accuracy: 0.5326 - val_loss: 0.6844 - val_accuracy: 0.5385\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6742 - accuracy: 0.5332 - val_loss: 0.6851 - val_accuracy: 0.5386\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6743 - accuracy: 0.5329 - val_loss: 0.6850 - val_accuracy: 0.5383\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6739 - accuracy: 0.5331 - val_loss: 0.6860 - val_accuracy: 0.5381\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6734 - accuracy: 0.5339 - val_loss: 0.6858 - val_accuracy: 0.5384\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6727 - accuracy: 0.5339 - val_loss: 0.6860 - val_accuracy: 0.5377\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6725 - accuracy: 0.5343 - val_loss: 0.6869 - val_accuracy: 0.5381\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6719 - accuracy: 0.5346 - val_loss: 0.6886 - val_accuracy: 0.5377\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6715 - accuracy: 0.5350 - val_loss: 0.6886 - val_accuracy: 0.5373\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6714 - accuracy: 0.5349 - val_loss: 0.6879 - val_accuracy: 0.5366\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6709 - accuracy: 0.5353 - val_loss: 0.6883 - val_accuracy: 0.5367\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6709 - accuracy: 0.5349 - val_loss: 0.6888 - val_accuracy: 0.5374\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6701 - accuracy: 0.5355 - val_loss: 0.6890 - val_accuracy: 0.5375\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6702 - accuracy: 0.5358 - val_loss: 0.6910 - val_accuracy: 0.5381\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6697 - accuracy: 0.5361 - val_loss: 0.6900 - val_accuracy: 0.5376\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 89ms/step - loss: 0.6697 - accuracy: 0.5363 - val_loss: 0.6891 - val_accuracy: 0.5372\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6691 - accuracy: 0.5363 - val_loss: 0.6920 - val_accuracy: 0.5374\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6687 - accuracy: 0.5365 - val_loss: 0.6906 - val_accuracy: 0.5371\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6687 - accuracy: 0.5367 - val_loss: 0.6903 - val_accuracy: 0.5376\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6688 - accuracy: 0.5374 - val_loss: 0.6918 - val_accuracy: 0.5367\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6680 - accuracy: 0.5375 - val_loss: 0.6919 - val_accuracy: 0.5371\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6680 - accuracy: 0.5375 - val_loss: 0.6913 - val_accuracy: 0.5366\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6674 - accuracy: 0.5376 - val_loss: 0.6908 - val_accuracy: 0.5369\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6675 - accuracy: 0.5377 - val_loss: 0.6921 - val_accuracy: 0.5367\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6670 - accuracy: 0.5379 - val_loss: 0.6935 - val_accuracy: 0.5365\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6674 - accuracy: 0.5377 - val_loss: 0.6936 - val_accuracy: 0.5370\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6668 - accuracy: 0.5384 - val_loss: 0.6932 - val_accuracy: 0.5369\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6662 - accuracy: 0.5386 - val_loss: 0.6925 - val_accuracy: 0.5366\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6659 - accuracy: 0.5387 - val_loss: 0.6942 - val_accuracy: 0.5369\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6657 - accuracy: 0.5390 - val_loss: 0.6941 - val_accuracy: 0.5369\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6657 - accuracy: 0.5391 - val_loss: 0.6957 - val_accuracy: 0.5365\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6657 - accuracy: 0.5389 - val_loss: 0.6961 - val_accuracy: 0.5366\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6653 - accuracy: 0.5392 - val_loss: 0.6950 - val_accuracy: 0.5369\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6650 - accuracy: 0.5393 - val_loss: 0.6959 - val_accuracy: 0.5373\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6646 - accuracy: 0.5397 - val_loss: 0.6955 - val_accuracy: 0.5366\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6647 - accuracy: 0.5393 - val_loss: 0.6957 - val_accuracy: 0.5369\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6645 - accuracy: 0.5398 - val_loss: 0.6971 - val_accuracy: 0.5368\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6643 - accuracy: 0.5397 - val_loss: 0.6965 - val_accuracy: 0.5373\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6644 - accuracy: 0.5397 - val_loss: 0.6979 - val_accuracy: 0.5369\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6641 - accuracy: 0.5401 - val_loss: 0.6981 - val_accuracy: 0.5371\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6636 - accuracy: 0.5404 - val_loss: 0.6965 - val_accuracy: 0.5367\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6642 - accuracy: 0.5400 - val_loss: 0.6964 - val_accuracy: 0.5371\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6636 - accuracy: 0.5403 - val_loss: 0.6979 - val_accuracy: 0.5369\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6635 - accuracy: 0.5404 - val_loss: 0.6984 - val_accuracy: 0.5365\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6631 - accuracy: 0.5405 - val_loss: 0.6993 - val_accuracy: 0.5368\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6634 - accuracy: 0.5405 - val_loss: 0.6974 - val_accuracy: 0.5370\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6632 - accuracy: 0.5405 - val_loss: 0.6990 - val_accuracy: 0.5364\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6629 - accuracy: 0.5407 - val_loss: 0.6981 - val_accuracy: 0.5369\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6627 - accuracy: 0.5406 - val_loss: 0.6996 - val_accuracy: 0.5362\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6628 - accuracy: 0.5410 - val_loss: 0.6987 - val_accuracy: 0.5365\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6623 - accuracy: 0.5411 - val_loss: 0.6998 - val_accuracy: 0.5371\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6625 - accuracy: 0.5410 - val_loss: 0.6998 - val_accuracy: 0.5365\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6625 - accuracy: 0.5412 - val_loss: 0.6982 - val_accuracy: 0.5369\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6621 - accuracy: 0.5415 - val_loss: 0.6985 - val_accuracy: 0.5364\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6623 - accuracy: 0.5412 - val_loss: 0.6970 - val_accuracy: 0.5373\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6621 - accuracy: 0.5411 - val_loss: 0.6987 - val_accuracy: 0.5368\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6621 - accuracy: 0.5416 - val_loss: 0.6983 - val_accuracy: 0.5368\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6618 - accuracy: 0.5415 - val_loss: 0.7003 - val_accuracy: 0.5369\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6621 - accuracy: 0.5414 - val_loss: 0.6994 - val_accuracy: 0.5370\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6616 - accuracy: 0.5411 - val_loss: 0.6991 - val_accuracy: 0.5367\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6614 - accuracy: 0.5417 - val_loss: 0.7011 - val_accuracy: 0.5368\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6612 - accuracy: 0.5419 - val_loss: 0.7017 - val_accuracy: 0.5370\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6613 - accuracy: 0.5421 - val_loss: 0.7022 - val_accuracy: 0.5366\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6616 - accuracy: 0.5417 - val_loss: 0.7011 - val_accuracy: 0.5368\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6613 - accuracy: 0.5420 - val_loss: 0.7003 - val_accuracy: 0.5366\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6609 - accuracy: 0.5424 - val_loss: 0.7017 - val_accuracy: 0.5365\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6608 - accuracy: 0.5422 - val_loss: 0.7031 - val_accuracy: 0.5367\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6607 - accuracy: 0.5423 - val_loss: 0.7013 - val_accuracy: 0.5367\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6604 - accuracy: 0.5426 - val_loss: 0.7030 - val_accuracy: 0.5362\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6609 - accuracy: 0.5423 - val_loss: 0.7011 - val_accuracy: 0.5370\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6604 - accuracy: 0.5425 - val_loss: 0.7034 - val_accuracy: 0.5370\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6609 - accuracy: 0.5424 - val_loss: 0.7011 - val_accuracy: 0.5369\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6605 - accuracy: 0.5417 - val_loss: 0.7024 - val_accuracy: 0.5362\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6608 - accuracy: 0.5421 - val_loss: 0.7021 - val_accuracy: 0.5370\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6605 - accuracy: 0.5425 - val_loss: 0.7028 - val_accuracy: 0.5371\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6599 - accuracy: 0.5426 - val_loss: 0.7026 - val_accuracy: 0.5369\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6599 - accuracy: 0.5427 - val_loss: 0.7059 - val_accuracy: 0.5372\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6602 - accuracy: 0.5427 - val_loss: 0.7065 - val_accuracy: 0.5366\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6601 - accuracy: 0.5427 - val_loss: 0.7049 - val_accuracy: 0.5371\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6597 - accuracy: 0.5430 - val_loss: 0.7063 - val_accuracy: 0.5371\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6601 - accuracy: 0.5428 - val_loss: 0.7022 - val_accuracy: 0.5368\n",
            "19/19 [==============================] - 1s 23ms/step - loss: 0.7061 - accuracy: 0.5357\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 8s 89ms/step - loss: 0.6901 - accuracy: 0.5178 - val_loss: 0.6874 - val_accuracy: 0.5116\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6869 - accuracy: 0.5269 - val_loss: 0.6871 - val_accuracy: 0.5121\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6856 - accuracy: 0.5292 - val_loss: 0.6865 - val_accuracy: 0.5127\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6848 - accuracy: 0.5297 - val_loss: 0.6864 - val_accuracy: 0.5127\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6833 - accuracy: 0.5316 - val_loss: 0.6876 - val_accuracy: 0.5119\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6833 - accuracy: 0.5314 - val_loss: 0.6864 - val_accuracy: 0.5131\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6820 - accuracy: 0.5329 - val_loss: 0.6869 - val_accuracy: 0.5132\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6811 - accuracy: 0.5336 - val_loss: 0.6875 - val_accuracy: 0.5125\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6804 - accuracy: 0.5337 - val_loss: 0.6870 - val_accuracy: 0.5135\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6797 - accuracy: 0.5345 - val_loss: 0.6876 - val_accuracy: 0.5133\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6791 - accuracy: 0.5347 - val_loss: 0.6879 - val_accuracy: 0.5137\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6785 - accuracy: 0.5354 - val_loss: 0.6888 - val_accuracy: 0.5133\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6777 - accuracy: 0.5358 - val_loss: 0.6884 - val_accuracy: 0.5132\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6769 - accuracy: 0.5363 - val_loss: 0.6896 - val_accuracy: 0.5141\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6766 - accuracy: 0.5365 - val_loss: 0.6895 - val_accuracy: 0.5107\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6762 - accuracy: 0.5370 - val_loss: 0.6901 - val_accuracy: 0.5135\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6756 - accuracy: 0.5374 - val_loss: 0.6889 - val_accuracy: 0.5129\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6751 - accuracy: 0.5377 - val_loss: 0.6896 - val_accuracy: 0.5131\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6741 - accuracy: 0.5383 - val_loss: 0.6905 - val_accuracy: 0.5127\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6742 - accuracy: 0.5379 - val_loss: 0.6903 - val_accuracy: 0.5097\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6744 - accuracy: 0.5379 - val_loss: 0.6907 - val_accuracy: 0.5124\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6736 - accuracy: 0.5386 - val_loss: 0.6911 - val_accuracy: 0.5132\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6726 - accuracy: 0.5394 - val_loss: 0.6910 - val_accuracy: 0.5118\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6725 - accuracy: 0.5393 - val_loss: 0.6930 - val_accuracy: 0.5122\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6721 - accuracy: 0.5397 - val_loss: 0.6913 - val_accuracy: 0.5123\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6722 - accuracy: 0.5392 - val_loss: 0.6923 - val_accuracy: 0.5127\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6711 - accuracy: 0.5402 - val_loss: 0.6919 - val_accuracy: 0.5131\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6711 - accuracy: 0.5404 - val_loss: 0.6928 - val_accuracy: 0.5118\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6706 - accuracy: 0.5406 - val_loss: 0.6935 - val_accuracy: 0.5123\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6699 - accuracy: 0.5412 - val_loss: 0.6936 - val_accuracy: 0.5134\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6701 - accuracy: 0.5409 - val_loss: 0.6932 - val_accuracy: 0.5119\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6695 - accuracy: 0.5412 - val_loss: 0.6954 - val_accuracy: 0.5142\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6698 - accuracy: 0.5410 - val_loss: 0.6935 - val_accuracy: 0.5123\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6692 - accuracy: 0.5415 - val_loss: 0.6946 - val_accuracy: 0.5128\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6688 - accuracy: 0.5420 - val_loss: 0.6954 - val_accuracy: 0.5120\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6683 - accuracy: 0.5420 - val_loss: 0.6962 - val_accuracy: 0.5122\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6680 - accuracy: 0.5421 - val_loss: 0.6957 - val_accuracy: 0.5116\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6680 - accuracy: 0.5421 - val_loss: 0.6978 - val_accuracy: 0.5130\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6674 - accuracy: 0.5426 - val_loss: 0.6974 - val_accuracy: 0.5125\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6673 - accuracy: 0.5427 - val_loss: 0.6986 - val_accuracy: 0.5122\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6669 - accuracy: 0.5431 - val_loss: 0.6987 - val_accuracy: 0.5120\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6670 - accuracy: 0.5428 - val_loss: 0.6970 - val_accuracy: 0.5124\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6667 - accuracy: 0.5430 - val_loss: 0.6981 - val_accuracy: 0.5122\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6662 - accuracy: 0.5435 - val_loss: 0.6972 - val_accuracy: 0.5112\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6665 - accuracy: 0.5435 - val_loss: 0.6998 - val_accuracy: 0.5129\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6663 - accuracy: 0.5435 - val_loss: 0.6990 - val_accuracy: 0.5126\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6654 - accuracy: 0.5439 - val_loss: 0.6998 - val_accuracy: 0.5131\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6658 - accuracy: 0.5438 - val_loss: 0.6991 - val_accuracy: 0.5127\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6656 - accuracy: 0.5443 - val_loss: 0.7008 - val_accuracy: 0.5106\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6653 - accuracy: 0.5443 - val_loss: 0.6997 - val_accuracy: 0.5120\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6646 - accuracy: 0.5445 - val_loss: 0.6984 - val_accuracy: 0.5133\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6649 - accuracy: 0.5447 - val_loss: 0.7045 - val_accuracy: 0.5140\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6647 - accuracy: 0.5443 - val_loss: 0.6997 - val_accuracy: 0.5121\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6643 - accuracy: 0.5444 - val_loss: 0.6997 - val_accuracy: 0.5127\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6640 - accuracy: 0.5451 - val_loss: 0.7014 - val_accuracy: 0.5115\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6645 - accuracy: 0.5449 - val_loss: 0.7013 - val_accuracy: 0.5126\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6640 - accuracy: 0.5446 - val_loss: 0.7003 - val_accuracy: 0.5124\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6637 - accuracy: 0.5451 - val_loss: 0.7017 - val_accuracy: 0.5122\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6636 - accuracy: 0.5454 - val_loss: 0.7032 - val_accuracy: 0.5130\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6634 - accuracy: 0.5455 - val_loss: 0.7023 - val_accuracy: 0.5138\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6634 - accuracy: 0.5456 - val_loss: 0.7031 - val_accuracy: 0.5135\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6631 - accuracy: 0.5455 - val_loss: 0.7026 - val_accuracy: 0.5124\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6632 - accuracy: 0.5459 - val_loss: 0.7022 - val_accuracy: 0.5123\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6629 - accuracy: 0.5457 - val_loss: 0.7030 - val_accuracy: 0.5124\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6628 - accuracy: 0.5459 - val_loss: 0.7034 - val_accuracy: 0.5136\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6626 - accuracy: 0.5459 - val_loss: 0.7034 - val_accuracy: 0.5117\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6623 - accuracy: 0.5459 - val_loss: 0.7031 - val_accuracy: 0.5131\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6627 - accuracy: 0.5390 - val_loss: 0.7020 - val_accuracy: 0.5122\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6624 - accuracy: 0.5440 - val_loss: 0.7033 - val_accuracy: 0.5126\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6623 - accuracy: 0.5467 - val_loss: 0.7048 - val_accuracy: 0.5136\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6623 - accuracy: 0.5462 - val_loss: 0.7038 - val_accuracy: 0.5121\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6615 - accuracy: 0.5468 - val_loss: 0.7053 - val_accuracy: 0.5138\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6618 - accuracy: 0.5464 - val_loss: 0.7028 - val_accuracy: 0.5130\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6619 - accuracy: 0.5466 - val_loss: 0.7053 - val_accuracy: 0.5133\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6613 - accuracy: 0.5469 - val_loss: 0.7058 - val_accuracy: 0.5127\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6619 - accuracy: 0.5465 - val_loss: 0.7025 - val_accuracy: 0.5124\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6615 - accuracy: 0.5467 - val_loss: 0.7056 - val_accuracy: 0.5138\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6611 - accuracy: 0.5470 - val_loss: 0.7039 - val_accuracy: 0.5123\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6615 - accuracy: 0.5467 - val_loss: 0.7052 - val_accuracy: 0.5133\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6609 - accuracy: 0.5473 - val_loss: 0.7059 - val_accuracy: 0.5139\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6609 - accuracy: 0.5472 - val_loss: 0.7069 - val_accuracy: 0.5141\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6603 - accuracy: 0.5475 - val_loss: 0.7079 - val_accuracy: 0.5141\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6606 - accuracy: 0.5473 - val_loss: 0.7066 - val_accuracy: 0.5139\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6603 - accuracy: 0.5477 - val_loss: 0.7052 - val_accuracy: 0.5131\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6601 - accuracy: 0.5475 - val_loss: 0.7083 - val_accuracy: 0.5135\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6602 - accuracy: 0.5476 - val_loss: 0.7060 - val_accuracy: 0.5128\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6607 - accuracy: 0.5472 - val_loss: 0.7056 - val_accuracy: 0.5137\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6601 - accuracy: 0.5477 - val_loss: 0.7098 - val_accuracy: 0.5139\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6599 - accuracy: 0.5477 - val_loss: 0.7077 - val_accuracy: 0.5139\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6602 - accuracy: 0.5476 - val_loss: 0.7073 - val_accuracy: 0.5133\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6607 - accuracy: 0.5399 - val_loss: 0.7064 - val_accuracy: 0.5133\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6597 - accuracy: 0.5480 - val_loss: 0.7091 - val_accuracy: 0.5141\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6597 - accuracy: 0.5479 - val_loss: 0.7065 - val_accuracy: 0.5120\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6602 - accuracy: 0.5476 - val_loss: 0.7081 - val_accuracy: 0.5138\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6596 - accuracy: 0.5479 - val_loss: 0.7094 - val_accuracy: 0.5132\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6595 - accuracy: 0.5479 - val_loss: 0.7076 - val_accuracy: 0.5139\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6595 - accuracy: 0.5481 - val_loss: 0.7120 - val_accuracy: 0.5144\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6594 - accuracy: 0.5481 - val_loss: 0.7087 - val_accuracy: 0.5133\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6590 - accuracy: 0.5484 - val_loss: 0.7088 - val_accuracy: 0.5141\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6596 - accuracy: 0.5447 - val_loss: 0.7080 - val_accuracy: 0.5134\n",
            "19/19 [==============================] - 1s 18ms/step - loss: 0.7087 - accuracy: 0.5129\n",
            "Best accuracy for dataset 3: 0.5373842716217041\n",
            "2308\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 77ms/step - loss: 0.6898 - accuracy: 0.5078 - val_loss: 0.6872 - val_accuracy: 0.5409\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6873 - accuracy: 0.5193 - val_loss: 0.6854 - val_accuracy: 0.5439\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6861 - accuracy: 0.5205 - val_loss: 0.6850 - val_accuracy: 0.5456\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6854 - accuracy: 0.5219 - val_loss: 0.6842 - val_accuracy: 0.5458\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6843 - accuracy: 0.5225 - val_loss: 0.6840 - val_accuracy: 0.5447\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6834 - accuracy: 0.5237 - val_loss: 0.6841 - val_accuracy: 0.5445\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6833 - accuracy: 0.5239 - val_loss: 0.6842 - val_accuracy: 0.5475\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6823 - accuracy: 0.5246 - val_loss: 0.6838 - val_accuracy: 0.5473\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 64ms/step - loss: 0.6816 - accuracy: 0.5251 - val_loss: 0.6841 - val_accuracy: 0.5471\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6804 - accuracy: 0.5260 - val_loss: 0.6837 - val_accuracy: 0.5471\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6806 - accuracy: 0.5262 - val_loss: 0.6843 - val_accuracy: 0.5469\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6791 - accuracy: 0.5269 - val_loss: 0.6854 - val_accuracy: 0.5466\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6787 - accuracy: 0.5275 - val_loss: 0.6851 - val_accuracy: 0.5469\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6778 - accuracy: 0.5277 - val_loss: 0.6853 - val_accuracy: 0.5468\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6774 - accuracy: 0.5280 - val_loss: 0.6856 - val_accuracy: 0.5466\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6767 - accuracy: 0.5287 - val_loss: 0.6858 - val_accuracy: 0.5470\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6762 - accuracy: 0.5294 - val_loss: 0.6872 - val_accuracy: 0.5469\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6754 - accuracy: 0.5296 - val_loss: 0.6869 - val_accuracy: 0.5458\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6751 - accuracy: 0.5297 - val_loss: 0.6871 - val_accuracy: 0.5468\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6744 - accuracy: 0.5304 - val_loss: 0.6896 - val_accuracy: 0.5468\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6739 - accuracy: 0.5309 - val_loss: 0.6896 - val_accuracy: 0.5460\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6737 - accuracy: 0.5310 - val_loss: 0.6882 - val_accuracy: 0.5470\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6732 - accuracy: 0.5312 - val_loss: 0.6908 - val_accuracy: 0.5472\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6728 - accuracy: 0.5317 - val_loss: 0.6896 - val_accuracy: 0.5458\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6723 - accuracy: 0.5318 - val_loss: 0.6916 - val_accuracy: 0.5463\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6718 - accuracy: 0.5317 - val_loss: 0.6920 - val_accuracy: 0.5464\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6716 - accuracy: 0.5322 - val_loss: 0.6907 - val_accuracy: 0.5458\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6713 - accuracy: 0.5325 - val_loss: 0.6911 - val_accuracy: 0.5468\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6707 - accuracy: 0.5327 - val_loss: 0.6913 - val_accuracy: 0.5461\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 5s 65ms/step - loss: 0.6709 - accuracy: 0.5329 - val_loss: 0.6918 - val_accuracy: 0.5455\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6704 - accuracy: 0.5330 - val_loss: 0.6934 - val_accuracy: 0.5468\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6698 - accuracy: 0.5335 - val_loss: 0.6957 - val_accuracy: 0.5458\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6695 - accuracy: 0.5338 - val_loss: 0.6930 - val_accuracy: 0.5462\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6691 - accuracy: 0.5338 - val_loss: 0.6955 - val_accuracy: 0.5464\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6689 - accuracy: 0.5342 - val_loss: 0.6948 - val_accuracy: 0.5458\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6692 - accuracy: 0.5338 - val_loss: 0.6933 - val_accuracy: 0.5462\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6684 - accuracy: 0.5344 - val_loss: 0.6951 - val_accuracy: 0.5457\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6679 - accuracy: 0.5348 - val_loss: 0.6950 - val_accuracy: 0.5461\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6678 - accuracy: 0.5349 - val_loss: 0.6946 - val_accuracy: 0.5460\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6679 - accuracy: 0.5350 - val_loss: 0.6953 - val_accuracy: 0.5465\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6676 - accuracy: 0.5348 - val_loss: 0.6959 - val_accuracy: 0.5461\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6673 - accuracy: 0.5354 - val_loss: 0.6958 - val_accuracy: 0.5457\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6666 - accuracy: 0.5359 - val_loss: 0.6963 - val_accuracy: 0.5457\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6670 - accuracy: 0.5351 - val_loss: 0.6985 - val_accuracy: 0.5467\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6666 - accuracy: 0.5362 - val_loss: 0.6974 - val_accuracy: 0.5453\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6662 - accuracy: 0.5328 - val_loss: 0.6993 - val_accuracy: 0.5454\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6659 - accuracy: 0.5361 - val_loss: 0.6991 - val_accuracy: 0.5457\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6657 - accuracy: 0.5362 - val_loss: 0.6992 - val_accuracy: 0.5457\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6657 - accuracy: 0.5367 - val_loss: 0.7037 - val_accuracy: 0.5451\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6657 - accuracy: 0.5363 - val_loss: 0.6991 - val_accuracy: 0.5456\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6651 - accuracy: 0.5369 - val_loss: 0.6986 - val_accuracy: 0.5452\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6650 - accuracy: 0.5369 - val_loss: 0.7013 - val_accuracy: 0.5457\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6650 - accuracy: 0.5370 - val_loss: 0.7008 - val_accuracy: 0.5459\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6647 - accuracy: 0.5374 - val_loss: 0.7028 - val_accuracy: 0.5456\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6643 - accuracy: 0.5374 - val_loss: 0.7000 - val_accuracy: 0.5455\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6643 - accuracy: 0.5375 - val_loss: 0.7011 - val_accuracy: 0.5460\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6640 - accuracy: 0.5375 - val_loss: 0.7029 - val_accuracy: 0.5458\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6640 - accuracy: 0.5375 - val_loss: 0.7012 - val_accuracy: 0.5452\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 7s 91ms/step - loss: 0.6634 - accuracy: 0.5380 - val_loss: 0.7045 - val_accuracy: 0.5452\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6636 - accuracy: 0.5379 - val_loss: 0.7030 - val_accuracy: 0.5449\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6633 - accuracy: 0.5381 - val_loss: 0.7028 - val_accuracy: 0.5455\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6632 - accuracy: 0.5379 - val_loss: 0.7023 - val_accuracy: 0.5450\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6630 - accuracy: 0.5384 - val_loss: 0.7040 - val_accuracy: 0.5454\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6631 - accuracy: 0.5381 - val_loss: 0.7051 - val_accuracy: 0.5458\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6630 - accuracy: 0.5381 - val_loss: 0.7051 - val_accuracy: 0.5454\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6625 - accuracy: 0.5385 - val_loss: 0.7049 - val_accuracy: 0.5457\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6631 - accuracy: 0.5384 - val_loss: 0.7059 - val_accuracy: 0.5456\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6648 - accuracy: 0.5385 - val_loss: 0.7062 - val_accuracy: 0.5456\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6624 - accuracy: 0.5388 - val_loss: 0.7036 - val_accuracy: 0.5450\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6626 - accuracy: 0.5389 - val_loss: 0.7055 - val_accuracy: 0.5458\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6625 - accuracy: 0.5386 - val_loss: 0.7063 - val_accuracy: 0.5452\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6624 - accuracy: 0.5387 - val_loss: 0.7090 - val_accuracy: 0.5454\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6626 - accuracy: 0.5352 - val_loss: 0.7077 - val_accuracy: 0.5456\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6621 - accuracy: 0.5387 - val_loss: 0.7066 - val_accuracy: 0.5450\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6621 - accuracy: 0.5389 - val_loss: 0.7072 - val_accuracy: 0.5457\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6618 - accuracy: 0.5394 - val_loss: 0.7092 - val_accuracy: 0.5455\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6617 - accuracy: 0.5392 - val_loss: 0.7093 - val_accuracy: 0.5456\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6614 - accuracy: 0.5396 - val_loss: 0.7077 - val_accuracy: 0.5459\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6617 - accuracy: 0.5392 - val_loss: 0.7089 - val_accuracy: 0.5455\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6617 - accuracy: 0.5391 - val_loss: 0.7086 - val_accuracy: 0.5457\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6613 - accuracy: 0.5395 - val_loss: 0.7100 - val_accuracy: 0.5451\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6614 - accuracy: 0.5394 - val_loss: 0.7082 - val_accuracy: 0.5455\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6610 - accuracy: 0.5397 - val_loss: 0.7081 - val_accuracy: 0.5458\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6607 - accuracy: 0.5399 - val_loss: 0.7101 - val_accuracy: 0.5455\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6609 - accuracy: 0.5397 - val_loss: 0.7099 - val_accuracy: 0.5459\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6609 - accuracy: 0.5398 - val_loss: 0.7106 - val_accuracy: 0.5457\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6605 - accuracy: 0.5401 - val_loss: 0.7076 - val_accuracy: 0.5446\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6608 - accuracy: 0.5397 - val_loss: 0.7076 - val_accuracy: 0.5447\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6607 - accuracy: 0.5401 - val_loss: 0.7113 - val_accuracy: 0.5455\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6602 - accuracy: 0.5401 - val_loss: 0.7101 - val_accuracy: 0.5456\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6599 - accuracy: 0.5405 - val_loss: 0.7110 - val_accuracy: 0.5455\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6604 - accuracy: 0.5378 - val_loss: 0.7129 - val_accuracy: 0.5457\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6601 - accuracy: 0.5404 - val_loss: 0.7122 - val_accuracy: 0.5455\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6600 - accuracy: 0.5406 - val_loss: 0.7131 - val_accuracy: 0.5453\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6601 - accuracy: 0.5403 - val_loss: 0.7097 - val_accuracy: 0.5455\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6601 - accuracy: 0.5403 - val_loss: 0.7085 - val_accuracy: 0.5452\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6604 - accuracy: 0.5404 - val_loss: 0.7091 - val_accuracy: 0.5457\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6599 - accuracy: 0.5406 - val_loss: 0.7122 - val_accuracy: 0.5454\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 89ms/step - loss: 0.6594 - accuracy: 0.5410 - val_loss: 0.7088 - val_accuracy: 0.5449\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6594 - accuracy: 0.5405 - val_loss: 0.7115 - val_accuracy: 0.5457\n",
            "19/19 [==============================] - 1s 23ms/step - loss: 0.7181 - accuracy: 0.5434\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 73ms/step - loss: 0.6904 - accuracy: 0.5048 - val_loss: 0.6890 - val_accuracy: 0.5293\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6869 - accuracy: 0.5206 - val_loss: 0.6881 - val_accuracy: 0.5337\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 6s 89ms/step - loss: 0.6854 - accuracy: 0.5234 - val_loss: 0.6875 - val_accuracy: 0.5343\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6840 - accuracy: 0.5244 - val_loss: 0.6875 - val_accuracy: 0.5352\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6833 - accuracy: 0.5255 - val_loss: 0.6874 - val_accuracy: 0.5349\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 7s 97ms/step - loss: 0.6831 - accuracy: 0.5262 - val_loss: 0.6876 - val_accuracy: 0.5366\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6814 - accuracy: 0.5270 - val_loss: 0.6879 - val_accuracy: 0.5361\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6806 - accuracy: 0.5280 - val_loss: 0.6891 - val_accuracy: 0.5360\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6801 - accuracy: 0.5287 - val_loss: 0.6901 - val_accuracy: 0.5358\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6792 - accuracy: 0.5290 - val_loss: 0.6922 - val_accuracy: 0.5354\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6783 - accuracy: 0.5292 - val_loss: 0.6896 - val_accuracy: 0.5364\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6776 - accuracy: 0.5300 - val_loss: 0.6910 - val_accuracy: 0.5358\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6772 - accuracy: 0.5300 - val_loss: 0.6925 - val_accuracy: 0.5357\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6771 - accuracy: 0.5299 - val_loss: 0.6921 - val_accuracy: 0.5363\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6760 - accuracy: 0.5311 - val_loss: 0.6933 - val_accuracy: 0.5353\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6754 - accuracy: 0.5315 - val_loss: 0.6949 - val_accuracy: 0.5362\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6751 - accuracy: 0.5317 - val_loss: 0.6941 - val_accuracy: 0.5349\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6742 - accuracy: 0.5325 - val_loss: 0.6941 - val_accuracy: 0.5352\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6743 - accuracy: 0.5323 - val_loss: 0.6989 - val_accuracy: 0.5360\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6736 - accuracy: 0.5327 - val_loss: 0.6971 - val_accuracy: 0.5343\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6736 - accuracy: 0.5330 - val_loss: 0.6961 - val_accuracy: 0.5346\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6724 - accuracy: 0.5338 - val_loss: 0.7015 - val_accuracy: 0.5353\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6721 - accuracy: 0.5339 - val_loss: 0.7014 - val_accuracy: 0.5344\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6718 - accuracy: 0.5340 - val_loss: 0.7049 - val_accuracy: 0.5353\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6712 - accuracy: 0.5346 - val_loss: 0.7013 - val_accuracy: 0.5341\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6709 - accuracy: 0.5348 - val_loss: 0.7021 - val_accuracy: 0.5344\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6705 - accuracy: 0.5352 - val_loss: 0.7021 - val_accuracy: 0.5350\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6701 - accuracy: 0.5352 - val_loss: 0.7020 - val_accuracy: 0.5323\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6700 - accuracy: 0.5357 - val_loss: 0.7059 - val_accuracy: 0.5347\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6694 - accuracy: 0.5359 - val_loss: 0.7022 - val_accuracy: 0.5336\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6689 - accuracy: 0.5364 - val_loss: 0.7058 - val_accuracy: 0.5338\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6690 - accuracy: 0.5363 - val_loss: 0.7076 - val_accuracy: 0.5350\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6686 - accuracy: 0.5368 - val_loss: 0.7067 - val_accuracy: 0.5348\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6680 - accuracy: 0.5366 - val_loss: 0.7079 - val_accuracy: 0.5344\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6678 - accuracy: 0.5370 - val_loss: 0.7072 - val_accuracy: 0.5329\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6671 - accuracy: 0.5374 - val_loss: 0.7096 - val_accuracy: 0.5345\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6672 - accuracy: 0.5375 - val_loss: 0.7112 - val_accuracy: 0.5348\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6674 - accuracy: 0.5374 - val_loss: 0.7069 - val_accuracy: 0.5345\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6669 - accuracy: 0.5378 - val_loss: 0.7100 - val_accuracy: 0.5337\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6667 - accuracy: 0.5377 - val_loss: 0.7099 - val_accuracy: 0.5339\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6660 - accuracy: 0.5382 - val_loss: 0.7094 - val_accuracy: 0.5331\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6662 - accuracy: 0.5385 - val_loss: 0.7093 - val_accuracy: 0.5330\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6656 - accuracy: 0.5385 - val_loss: 0.7113 - val_accuracy: 0.5336\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6655 - accuracy: 0.5389 - val_loss: 0.7152 - val_accuracy: 0.5341\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6655 - accuracy: 0.5390 - val_loss: 0.7114 - val_accuracy: 0.5335\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6652 - accuracy: 0.5389 - val_loss: 0.7123 - val_accuracy: 0.5334\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6648 - accuracy: 0.5394 - val_loss: 0.7141 - val_accuracy: 0.5334\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6644 - accuracy: 0.5395 - val_loss: 0.7153 - val_accuracy: 0.5341\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6647 - accuracy: 0.5392 - val_loss: 0.7160 - val_accuracy: 0.5342\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6646 - accuracy: 0.5386 - val_loss: 0.7153 - val_accuracy: 0.5334\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6640 - accuracy: 0.5399 - val_loss: 0.7159 - val_accuracy: 0.5343\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6637 - accuracy: 0.5400 - val_loss: 0.7158 - val_accuracy: 0.5330\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6639 - accuracy: 0.5397 - val_loss: 0.7172 - val_accuracy: 0.5341\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6634 - accuracy: 0.5400 - val_loss: 0.7162 - val_accuracy: 0.5343\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6630 - accuracy: 0.5405 - val_loss: 0.7165 - val_accuracy: 0.5341\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6632 - accuracy: 0.5396 - val_loss: 0.7184 - val_accuracy: 0.5342\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6631 - accuracy: 0.5402 - val_loss: 0.7186 - val_accuracy: 0.5341\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6626 - accuracy: 0.5408 - val_loss: 0.7185 - val_accuracy: 0.5343\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6629 - accuracy: 0.5404 - val_loss: 0.7181 - val_accuracy: 0.5338\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6627 - accuracy: 0.5408 - val_loss: 0.7173 - val_accuracy: 0.5330\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6621 - accuracy: 0.5413 - val_loss: 0.7226 - val_accuracy: 0.5341\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6617 - accuracy: 0.5415 - val_loss: 0.7172 - val_accuracy: 0.5333\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6622 - accuracy: 0.5409 - val_loss: 0.7205 - val_accuracy: 0.5331\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6622 - accuracy: 0.5412 - val_loss: 0.7210 - val_accuracy: 0.5333\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6622 - accuracy: 0.5411 - val_loss: 0.7230 - val_accuracy: 0.5336\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6622 - accuracy: 0.5406 - val_loss: 0.7192 - val_accuracy: 0.5332\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6618 - accuracy: 0.5415 - val_loss: 0.7213 - val_accuracy: 0.5338\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6615 - accuracy: 0.5416 - val_loss: 0.7218 - val_accuracy: 0.5335\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6617 - accuracy: 0.5415 - val_loss: 0.7206 - val_accuracy: 0.5338\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6611 - accuracy: 0.5416 - val_loss: 0.7227 - val_accuracy: 0.5333\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6608 - accuracy: 0.5420 - val_loss: 0.7202 - val_accuracy: 0.5327\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6609 - accuracy: 0.5420 - val_loss: 0.7239 - val_accuracy: 0.5332\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6607 - accuracy: 0.5411 - val_loss: 0.7248 - val_accuracy: 0.5333\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6606 - accuracy: 0.5420 - val_loss: 0.7306 - val_accuracy: 0.5340\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6606 - accuracy: 0.5421 - val_loss: 0.7240 - val_accuracy: 0.5335\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6607 - accuracy: 0.5419 - val_loss: 0.7232 - val_accuracy: 0.5333\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6602 - accuracy: 0.5424 - val_loss: 0.7262 - val_accuracy: 0.5337\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6602 - accuracy: 0.5424 - val_loss: 0.7250 - val_accuracy: 0.5336\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6602 - accuracy: 0.5425 - val_loss: 0.7242 - val_accuracy: 0.5336\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6602 - accuracy: 0.5425 - val_loss: 0.7247 - val_accuracy: 0.5338\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6601 - accuracy: 0.5420 - val_loss: 0.7281 - val_accuracy: 0.5333\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6599 - accuracy: 0.5425 - val_loss: 0.7266 - val_accuracy: 0.5335\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6596 - accuracy: 0.5407 - val_loss: 0.7235 - val_accuracy: 0.5325\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6599 - accuracy: 0.5427 - val_loss: 0.7256 - val_accuracy: 0.5339\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6596 - accuracy: 0.5427 - val_loss: 0.7287 - val_accuracy: 0.5334\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6596 - accuracy: 0.5430 - val_loss: 0.7262 - val_accuracy: 0.5334\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6591 - accuracy: 0.5431 - val_loss: 0.7286 - val_accuracy: 0.5340\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6595 - accuracy: 0.5431 - val_loss: 0.7293 - val_accuracy: 0.5340\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6591 - accuracy: 0.5432 - val_loss: 0.7268 - val_accuracy: 0.5330\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6589 - accuracy: 0.5434 - val_loss: 0.7275 - val_accuracy: 0.5325\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6592 - accuracy: 0.5424 - val_loss: 0.7287 - val_accuracy: 0.5330\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6592 - accuracy: 0.5434 - val_loss: 0.7297 - val_accuracy: 0.5335\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 7s 93ms/step - loss: 0.6590 - accuracy: 0.5430 - val_loss: 0.7306 - val_accuracy: 0.5345\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6584 - accuracy: 0.5435 - val_loss: 0.7308 - val_accuracy: 0.5338\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6592 - accuracy: 0.5430 - val_loss: 0.7276 - val_accuracy: 0.5339\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6590 - accuracy: 0.5429 - val_loss: 0.7279 - val_accuracy: 0.5343\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6581 - accuracy: 0.5438 - val_loss: 0.7286 - val_accuracy: 0.5335\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6584 - accuracy: 0.5436 - val_loss: 0.7284 - val_accuracy: 0.5336\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6586 - accuracy: 0.5440 - val_loss: 0.7309 - val_accuracy: 0.5331\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6581 - accuracy: 0.5432 - val_loss: 0.7294 - val_accuracy: 0.5334\n",
            "19/19 [==============================] - 1s 28ms/step - loss: 0.7338 - accuracy: 0.5317\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 7s 74ms/step - loss: 0.6909 - accuracy: 0.5104 - val_loss: 0.6876 - val_accuracy: 0.5032\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6875 - accuracy: 0.5291 - val_loss: 0.6868 - val_accuracy: 0.5029\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6864 - accuracy: 0.5306 - val_loss: 0.6861 - val_accuracy: 0.5039\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6848 - accuracy: 0.5323 - val_loss: 0.6865 - val_accuracy: 0.5033\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6840 - accuracy: 0.5333 - val_loss: 0.6864 - val_accuracy: 0.5040\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6834 - accuracy: 0.5339 - val_loss: 0.6879 - val_accuracy: 0.5005\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6824 - accuracy: 0.5354 - val_loss: 0.6865 - val_accuracy: 0.5033\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6815 - accuracy: 0.5353 - val_loss: 0.6877 - val_accuracy: 0.5005\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6810 - accuracy: 0.5364 - val_loss: 0.6878 - val_accuracy: 0.5002\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6803 - accuracy: 0.5368 - val_loss: 0.6886 - val_accuracy: 0.5025\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6795 - accuracy: 0.5376 - val_loss: 0.6880 - val_accuracy: 0.4999\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 7s 90ms/step - loss: 0.6786 - accuracy: 0.5379 - val_loss: 0.6883 - val_accuracy: 0.5006\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6777 - accuracy: 0.5387 - val_loss: 0.6878 - val_accuracy: 0.5018\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6769 - accuracy: 0.5394 - val_loss: 0.6888 - val_accuracy: 0.5029\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 7s 89ms/step - loss: 0.6765 - accuracy: 0.5397 - val_loss: 0.6888 - val_accuracy: 0.5006\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6760 - accuracy: 0.5401 - val_loss: 0.6896 - val_accuracy: 0.4995\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6752 - accuracy: 0.5405 - val_loss: 0.6890 - val_accuracy: 0.5007\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6748 - accuracy: 0.5407 - val_loss: 0.6892 - val_accuracy: 0.5007\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6744 - accuracy: 0.5411 - val_loss: 0.6901 - val_accuracy: 0.4997\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6738 - accuracy: 0.5414 - val_loss: 0.6895 - val_accuracy: 0.5020\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6736 - accuracy: 0.5420 - val_loss: 0.6891 - val_accuracy: 0.4999\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6729 - accuracy: 0.5422 - val_loss: 0.6908 - val_accuracy: 0.5021\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6728 - accuracy: 0.5422 - val_loss: 0.6911 - val_accuracy: 0.5015\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6723 - accuracy: 0.5422 - val_loss: 0.6898 - val_accuracy: 0.5010\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6725 - accuracy: 0.5424 - val_loss: 0.6906 - val_accuracy: 0.5006\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6715 - accuracy: 0.5430 - val_loss: 0.6921 - val_accuracy: 0.4991\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6713 - accuracy: 0.5435 - val_loss: 0.6928 - val_accuracy: 0.4984\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6706 - accuracy: 0.5438 - val_loss: 0.6937 - val_accuracy: 0.5012\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6702 - accuracy: 0.5439 - val_loss: 0.6925 - val_accuracy: 0.5003\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6697 - accuracy: 0.5443 - val_loss: 0.6936 - val_accuracy: 0.4985\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6697 - accuracy: 0.5443 - val_loss: 0.6930 - val_accuracy: 0.4994\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6695 - accuracy: 0.5444 - val_loss: 0.6939 - val_accuracy: 0.5003\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6690 - accuracy: 0.5445 - val_loss: 0.6921 - val_accuracy: 0.5011\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6687 - accuracy: 0.5448 - val_loss: 0.6943 - val_accuracy: 0.5008\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6684 - accuracy: 0.5452 - val_loss: 0.6933 - val_accuracy: 0.5013\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6682 - accuracy: 0.5454 - val_loss: 0.6946 - val_accuracy: 0.5009\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6679 - accuracy: 0.5456 - val_loss: 0.6934 - val_accuracy: 0.5004\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6676 - accuracy: 0.5457 - val_loss: 0.6933 - val_accuracy: 0.5002\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6672 - accuracy: 0.5460 - val_loss: 0.6947 - val_accuracy: 0.5011\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6672 - accuracy: 0.5458 - val_loss: 0.6952 - val_accuracy: 0.5017\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6672 - accuracy: 0.5462 - val_loss: 0.6939 - val_accuracy: 0.5014\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6668 - accuracy: 0.5462 - val_loss: 0.6961 - val_accuracy: 0.5008\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6663 - accuracy: 0.5465 - val_loss: 0.6969 - val_accuracy: 0.5006\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6661 - accuracy: 0.5467 - val_loss: 0.6967 - val_accuracy: 0.5023\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6658 - accuracy: 0.5467 - val_loss: 0.6952 - val_accuracy: 0.5013\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6663 - accuracy: 0.5466 - val_loss: 0.6950 - val_accuracy: 0.5008\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6656 - accuracy: 0.5470 - val_loss: 0.7017 - val_accuracy: 0.4999\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6659 - accuracy: 0.5471 - val_loss: 0.6965 - val_accuracy: 0.5000\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6657 - accuracy: 0.5471 - val_loss: 0.6971 - val_accuracy: 0.4999\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6650 - accuracy: 0.5477 - val_loss: 0.6952 - val_accuracy: 0.5006\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6653 - accuracy: 0.5473 - val_loss: 0.6981 - val_accuracy: 0.5013\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6644 - accuracy: 0.5480 - val_loss: 0.6977 - val_accuracy: 0.4998\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6648 - accuracy: 0.5477 - val_loss: 0.6972 - val_accuracy: 0.5017\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6643 - accuracy: 0.5481 - val_loss: 0.6957 - val_accuracy: 0.5014\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6641 - accuracy: 0.5479 - val_loss: 0.6976 - val_accuracy: 0.5015\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6639 - accuracy: 0.5485 - val_loss: 0.6987 - val_accuracy: 0.4998\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6635 - accuracy: 0.5486 - val_loss: 0.6983 - val_accuracy: 0.5008\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6635 - accuracy: 0.5487 - val_loss: 0.6995 - val_accuracy: 0.5007\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6634 - accuracy: 0.5488 - val_loss: 0.7005 - val_accuracy: 0.5025\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6635 - accuracy: 0.5446 - val_loss: 0.7008 - val_accuracy: 0.5004\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6630 - accuracy: 0.5488 - val_loss: 0.7009 - val_accuracy: 0.5013\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6628 - accuracy: 0.5491 - val_loss: 0.7000 - val_accuracy: 0.5025\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6628 - accuracy: 0.5492 - val_loss: 0.6985 - val_accuracy: 0.5022\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 5s 67ms/step - loss: 0.6625 - accuracy: 0.5493 - val_loss: 0.7002 - val_accuracy: 0.5004\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6631 - accuracy: 0.5494 - val_loss: 0.7011 - val_accuracy: 0.5017\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6625 - accuracy: 0.5493 - val_loss: 0.7004 - val_accuracy: 0.5014\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6621 - accuracy: 0.5494 - val_loss: 0.7008 - val_accuracy: 0.5019\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6620 - accuracy: 0.5494 - val_loss: 0.6996 - val_accuracy: 0.5015\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6622 - accuracy: 0.5494 - val_loss: 0.7011 - val_accuracy: 0.5019\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6617 - accuracy: 0.5498 - val_loss: 0.7011 - val_accuracy: 0.5022\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6619 - accuracy: 0.5496 - val_loss: 0.7022 - val_accuracy: 0.5010\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6615 - accuracy: 0.5498 - val_loss: 0.7012 - val_accuracy: 0.5005\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6624 - accuracy: 0.5497 - val_loss: 0.7002 - val_accuracy: 0.5010\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6613 - accuracy: 0.5502 - val_loss: 0.7010 - val_accuracy: 0.5015\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6613 - accuracy: 0.5498 - val_loss: 0.7016 - val_accuracy: 0.5016\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6610 - accuracy: 0.5502 - val_loss: 0.7026 - val_accuracy: 0.5024\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6610 - accuracy: 0.5501 - val_loss: 0.7024 - val_accuracy: 0.5017\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6608 - accuracy: 0.5503 - val_loss: 0.7034 - val_accuracy: 0.5014\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6606 - accuracy: 0.5504 - val_loss: 0.7015 - val_accuracy: 0.5015\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6607 - accuracy: 0.5505 - val_loss: 0.7023 - val_accuracy: 0.5014\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6607 - accuracy: 0.5504 - val_loss: 0.7020 - val_accuracy: 0.5004\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6605 - accuracy: 0.5507 - val_loss: 0.7033 - val_accuracy: 0.5012\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6605 - accuracy: 0.5502 - val_loss: 0.7019 - val_accuracy: 0.5017\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6602 - accuracy: 0.5505 - val_loss: 0.7050 - val_accuracy: 0.5005\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6602 - accuracy: 0.5506 - val_loss: 0.7034 - val_accuracy: 0.5012\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6603 - accuracy: 0.5505 - val_loss: 0.7060 - val_accuracy: 0.5027\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6605 - accuracy: 0.5504 - val_loss: 0.7022 - val_accuracy: 0.5012\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6596 - accuracy: 0.5511 - val_loss: 0.7053 - val_accuracy: 0.5017\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6603 - accuracy: 0.5507 - val_loss: 0.7042 - val_accuracy: 0.5023\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6597 - accuracy: 0.5510 - val_loss: 0.7046 - val_accuracy: 0.5010\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6595 - accuracy: 0.5509 - val_loss: 0.7054 - val_accuracy: 0.5011\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6593 - accuracy: 0.5512 - val_loss: 0.7066 - val_accuracy: 0.5015\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6589 - accuracy: 0.5514 - val_loss: 0.7059 - val_accuracy: 0.5019\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6592 - accuracy: 0.5516 - val_loss: 0.7057 - val_accuracy: 0.5021\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6598 - accuracy: 0.5508 - val_loss: 0.7055 - val_accuracy: 0.5013\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6591 - accuracy: 0.5514 - val_loss: 0.7063 - val_accuracy: 0.5005\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 66ms/step - loss: 0.6591 - accuracy: 0.5513 - val_loss: 0.7052 - val_accuracy: 0.5013\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6591 - accuracy: 0.5514 - val_loss: 0.7062 - val_accuracy: 0.5015\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6589 - accuracy: 0.5516 - val_loss: 0.7068 - val_accuracy: 0.5012\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6590 - accuracy: 0.5513 - val_loss: 0.7088 - val_accuracy: 0.5021\n",
            "19/19 [==============================] - 1s 20ms/step - loss: 0.7154 - accuracy: 0.5007\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 11s 134ms/step - loss: 0.6901 - accuracy: 0.5058 - val_loss: 0.6878 - val_accuracy: 0.5286\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6873 - accuracy: 0.5217 - val_loss: 0.6865 - val_accuracy: 0.5339\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 9s 130ms/step - loss: 0.6860 - accuracy: 0.5237 - val_loss: 0.6858 - val_accuracy: 0.5344\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6847 - accuracy: 0.5246 - val_loss: 0.6854 - val_accuracy: 0.5352\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6839 - accuracy: 0.5260 - val_loss: 0.6853 - val_accuracy: 0.5359\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6825 - accuracy: 0.5272 - val_loss: 0.6859 - val_accuracy: 0.5359\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6821 - accuracy: 0.5270 - val_loss: 0.6858 - val_accuracy: 0.5358\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6812 - accuracy: 0.5282 - val_loss: 0.6859 - val_accuracy: 0.5358\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6806 - accuracy: 0.5286 - val_loss: 0.6866 - val_accuracy: 0.5365\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6793 - accuracy: 0.5291 - val_loss: 0.6867 - val_accuracy: 0.5345\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6792 - accuracy: 0.5297 - val_loss: 0.6867 - val_accuracy: 0.5361\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6783 - accuracy: 0.5303 - val_loss: 0.6871 - val_accuracy: 0.5362\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6777 - accuracy: 0.5307 - val_loss: 0.6869 - val_accuracy: 0.5355\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6769 - accuracy: 0.5310 - val_loss: 0.6894 - val_accuracy: 0.5354\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6760 - accuracy: 0.5317 - val_loss: 0.6884 - val_accuracy: 0.5347\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6759 - accuracy: 0.5319 - val_loss: 0.6891 - val_accuracy: 0.5353\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6748 - accuracy: 0.5327 - val_loss: 0.6901 - val_accuracy: 0.5355\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6747 - accuracy: 0.5329 - val_loss: 0.6904 - val_accuracy: 0.5351\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6745 - accuracy: 0.5330 - val_loss: 0.6908 - val_accuracy: 0.5353\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6737 - accuracy: 0.5334 - val_loss: 0.6898 - val_accuracy: 0.5338\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6732 - accuracy: 0.5340 - val_loss: 0.6926 - val_accuracy: 0.5351\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6725 - accuracy: 0.5342 - val_loss: 0.6924 - val_accuracy: 0.5351\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6721 - accuracy: 0.5347 - val_loss: 0.6918 - val_accuracy: 0.5332\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6720 - accuracy: 0.5352 - val_loss: 0.6922 - val_accuracy: 0.5341\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6714 - accuracy: 0.5355 - val_loss: 0.6935 - val_accuracy: 0.5343\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6710 - accuracy: 0.5356 - val_loss: 0.6961 - val_accuracy: 0.5341\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6704 - accuracy: 0.5359 - val_loss: 0.6936 - val_accuracy: 0.5324\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6701 - accuracy: 0.5362 - val_loss: 0.6958 - val_accuracy: 0.5325\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6701 - accuracy: 0.5363 - val_loss: 0.6952 - val_accuracy: 0.5337\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6692 - accuracy: 0.5371 - val_loss: 0.6959 - val_accuracy: 0.5335\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6691 - accuracy: 0.5371 - val_loss: 0.6956 - val_accuracy: 0.5334\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6685 - accuracy: 0.5374 - val_loss: 0.6990 - val_accuracy: 0.5344\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6686 - accuracy: 0.5377 - val_loss: 0.6980 - val_accuracy: 0.5334\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6680 - accuracy: 0.5376 - val_loss: 0.6972 - val_accuracy: 0.5320\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6681 - accuracy: 0.5379 - val_loss: 0.6996 - val_accuracy: 0.5333\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6674 - accuracy: 0.5381 - val_loss: 0.7010 - val_accuracy: 0.5338\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6676 - accuracy: 0.5379 - val_loss: 0.7016 - val_accuracy: 0.5337\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6671 - accuracy: 0.5381 - val_loss: 0.6997 - val_accuracy: 0.5328\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6666 - accuracy: 0.5387 - val_loss: 0.7009 - val_accuracy: 0.5327\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6668 - accuracy: 0.5385 - val_loss: 0.6997 - val_accuracy: 0.5331\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6660 - accuracy: 0.5393 - val_loss: 0.7036 - val_accuracy: 0.5334\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6658 - accuracy: 0.5392 - val_loss: 0.7025 - val_accuracy: 0.5334\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6655 - accuracy: 0.5394 - val_loss: 0.7019 - val_accuracy: 0.5323\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6655 - accuracy: 0.5396 - val_loss: 0.7025 - val_accuracy: 0.5328\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6655 - accuracy: 0.5395 - val_loss: 0.7032 - val_accuracy: 0.5323\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6645 - accuracy: 0.5399 - val_loss: 0.7057 - val_accuracy: 0.5335\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6649 - accuracy: 0.5400 - val_loss: 0.7027 - val_accuracy: 0.5331\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6646 - accuracy: 0.5399 - val_loss: 0.7068 - val_accuracy: 0.5339\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6648 - accuracy: 0.5399 - val_loss: 0.7059 - val_accuracy: 0.5335\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6641 - accuracy: 0.5405 - val_loss: 0.7047 - val_accuracy: 0.5333\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6639 - accuracy: 0.5405 - val_loss: 0.7056 - val_accuracy: 0.5334\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6636 - accuracy: 0.5409 - val_loss: 0.7060 - val_accuracy: 0.5326\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6636 - accuracy: 0.5405 - val_loss: 0.7068 - val_accuracy: 0.5334\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6636 - accuracy: 0.5399 - val_loss: 0.7093 - val_accuracy: 0.5337\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6631 - accuracy: 0.5411 - val_loss: 0.7081 - val_accuracy: 0.5332\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6633 - accuracy: 0.5408 - val_loss: 0.7097 - val_accuracy: 0.5332\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6626 - accuracy: 0.5398 - val_loss: 0.7094 - val_accuracy: 0.5331\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6630 - accuracy: 0.5411 - val_loss: 0.7085 - val_accuracy: 0.5331\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6624 - accuracy: 0.5417 - val_loss: 0.7115 - val_accuracy: 0.5336\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6626 - accuracy: 0.5412 - val_loss: 0.7067 - val_accuracy: 0.5328\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6623 - accuracy: 0.5416 - val_loss: 0.7091 - val_accuracy: 0.5328\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6621 - accuracy: 0.5415 - val_loss: 0.7127 - val_accuracy: 0.5331\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6628 - accuracy: 0.5414 - val_loss: 0.7081 - val_accuracy: 0.5328\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6619 - accuracy: 0.5412 - val_loss: 0.7092 - val_accuracy: 0.5334\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6616 - accuracy: 0.5420 - val_loss: 0.7099 - val_accuracy: 0.5326\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6615 - accuracy: 0.5420 - val_loss: 0.7082 - val_accuracy: 0.5329\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6616 - accuracy: 0.5421 - val_loss: 0.7086 - val_accuracy: 0.5327\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6612 - accuracy: 0.5425 - val_loss: 0.7095 - val_accuracy: 0.5334\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6617 - accuracy: 0.5422 - val_loss: 0.7108 - val_accuracy: 0.5327\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6611 - accuracy: 0.5425 - val_loss: 0.7106 - val_accuracy: 0.5329\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6613 - accuracy: 0.5423 - val_loss: 0.7101 - val_accuracy: 0.5325\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6606 - accuracy: 0.5423 - val_loss: 0.7123 - val_accuracy: 0.5327\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6609 - accuracy: 0.5428 - val_loss: 0.7155 - val_accuracy: 0.5340\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 68ms/step - loss: 0.6610 - accuracy: 0.5424 - val_loss: 0.7110 - val_accuracy: 0.5332\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6603 - accuracy: 0.5426 - val_loss: 0.7107 - val_accuracy: 0.5329\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6608 - accuracy: 0.5425 - val_loss: 0.7105 - val_accuracy: 0.5321\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6607 - accuracy: 0.5426 - val_loss: 0.7110 - val_accuracy: 0.5323\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6602 - accuracy: 0.5428 - val_loss: 0.7130 - val_accuracy: 0.5330\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6604 - accuracy: 0.5429 - val_loss: 0.7114 - val_accuracy: 0.5335\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6601 - accuracy: 0.5432 - val_loss: 0.7117 - val_accuracy: 0.5321\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6604 - accuracy: 0.5425 - val_loss: 0.7118 - val_accuracy: 0.5327\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6604 - accuracy: 0.5429 - val_loss: 0.7123 - val_accuracy: 0.5333\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6600 - accuracy: 0.5430 - val_loss: 0.7137 - val_accuracy: 0.5324\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6599 - accuracy: 0.5433 - val_loss: 0.7142 - val_accuracy: 0.5330\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6599 - accuracy: 0.5432 - val_loss: 0.7111 - val_accuracy: 0.5328\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6597 - accuracy: 0.5430 - val_loss: 0.7132 - val_accuracy: 0.5330\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6596 - accuracy: 0.5434 - val_loss: 0.7128 - val_accuracy: 0.5325\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6596 - accuracy: 0.5434 - val_loss: 0.7124 - val_accuracy: 0.5331\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6595 - accuracy: 0.5435 - val_loss: 0.7126 - val_accuracy: 0.5329\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6593 - accuracy: 0.5434 - val_loss: 0.7122 - val_accuracy: 0.5330\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6591 - accuracy: 0.5436 - val_loss: 0.7121 - val_accuracy: 0.5326\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6592 - accuracy: 0.5436 - val_loss: 0.7130 - val_accuracy: 0.5328\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6591 - accuracy: 0.5433 - val_loss: 0.7138 - val_accuracy: 0.5327\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6590 - accuracy: 0.5437 - val_loss: 0.7158 - val_accuracy: 0.5333\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6588 - accuracy: 0.5440 - val_loss: 0.7146 - val_accuracy: 0.5331\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6586 - accuracy: 0.5441 - val_loss: 0.7144 - val_accuracy: 0.5335\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6587 - accuracy: 0.5441 - val_loss: 0.7156 - val_accuracy: 0.5335\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6586 - accuracy: 0.5438 - val_loss: 0.7173 - val_accuracy: 0.5335\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6586 - accuracy: 0.5438 - val_loss: 0.7125 - val_accuracy: 0.5325\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6587 - accuracy: 0.5440 - val_loss: 0.7142 - val_accuracy: 0.5331\n",
            "19/19 [==============================] - 1s 20ms/step - loss: 0.7165 - accuracy: 0.5320\n",
            "2309\n",
            "Epoch 1/100\n",
            "73/73 [==============================] - 12s 138ms/step - loss: 0.6907 - accuracy: 0.5093 - val_loss: 0.6876 - val_accuracy: 0.5098\n",
            "Epoch 2/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6874 - accuracy: 0.5265 - val_loss: 0.6871 - val_accuracy: 0.5145\n",
            "Epoch 3/100\n",
            "73/73 [==============================] - 5s 69ms/step - loss: 0.6857 - accuracy: 0.5282 - val_loss: 0.6865 - val_accuracy: 0.5148\n",
            "Epoch 4/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6845 - accuracy: 0.5295 - val_loss: 0.6867 - val_accuracy: 0.5146\n",
            "Epoch 5/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6834 - accuracy: 0.5303 - val_loss: 0.6870 - val_accuracy: 0.5144\n",
            "Epoch 6/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6832 - accuracy: 0.5313 - val_loss: 0.6864 - val_accuracy: 0.5156\n",
            "Epoch 7/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6817 - accuracy: 0.5323 - val_loss: 0.6868 - val_accuracy: 0.5153\n",
            "Epoch 8/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6809 - accuracy: 0.5334 - val_loss: 0.6873 - val_accuracy: 0.5151\n",
            "Epoch 9/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6807 - accuracy: 0.5331 - val_loss: 0.6871 - val_accuracy: 0.5148\n",
            "Epoch 10/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6800 - accuracy: 0.5340 - val_loss: 0.6880 - val_accuracy: 0.5159\n",
            "Epoch 11/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6789 - accuracy: 0.5345 - val_loss: 0.6885 - val_accuracy: 0.5156\n",
            "Epoch 12/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6784 - accuracy: 0.5351 - val_loss: 0.6887 - val_accuracy: 0.5162\n",
            "Epoch 13/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6776 - accuracy: 0.5355 - val_loss: 0.6895 - val_accuracy: 0.5155\n",
            "Epoch 14/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6774 - accuracy: 0.5356 - val_loss: 0.6894 - val_accuracy: 0.5141\n",
            "Epoch 15/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6766 - accuracy: 0.5361 - val_loss: 0.6905 - val_accuracy: 0.5162\n",
            "Epoch 16/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6763 - accuracy: 0.5366 - val_loss: 0.6905 - val_accuracy: 0.5153\n",
            "Epoch 17/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6755 - accuracy: 0.5369 - val_loss: 0.6914 - val_accuracy: 0.5131\n",
            "Epoch 18/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6752 - accuracy: 0.5372 - val_loss: 0.6923 - val_accuracy: 0.5154\n",
            "Epoch 19/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6747 - accuracy: 0.5376 - val_loss: 0.6915 - val_accuracy: 0.5155\n",
            "Epoch 20/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6741 - accuracy: 0.5379 - val_loss: 0.6928 - val_accuracy: 0.5147\n",
            "Epoch 21/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6738 - accuracy: 0.5382 - val_loss: 0.6926 - val_accuracy: 0.5148\n",
            "Epoch 22/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6737 - accuracy: 0.5383 - val_loss: 0.6940 - val_accuracy: 0.5154\n",
            "Epoch 23/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6728 - accuracy: 0.5387 - val_loss: 0.6930 - val_accuracy: 0.5138\n",
            "Epoch 24/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6723 - accuracy: 0.5394 - val_loss: 0.6952 - val_accuracy: 0.5147\n",
            "Epoch 25/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6723 - accuracy: 0.5394 - val_loss: 0.6958 - val_accuracy: 0.5146\n",
            "Epoch 26/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6719 - accuracy: 0.5396 - val_loss: 0.6971 - val_accuracy: 0.5145\n",
            "Epoch 27/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6712 - accuracy: 0.5400 - val_loss: 0.6990 - val_accuracy: 0.5151\n",
            "Epoch 28/100\n",
            "73/73 [==============================] - 6s 89ms/step - loss: 0.6711 - accuracy: 0.5401 - val_loss: 0.6980 - val_accuracy: 0.5150\n",
            "Epoch 29/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6705 - accuracy: 0.5403 - val_loss: 0.6971 - val_accuracy: 0.5135\n",
            "Epoch 30/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6702 - accuracy: 0.5408 - val_loss: 0.6968 - val_accuracy: 0.5139\n",
            "Epoch 31/100\n",
            "73/73 [==============================] - 6s 88ms/step - loss: 0.6700 - accuracy: 0.5410 - val_loss: 0.6994 - val_accuracy: 0.5141\n",
            "Epoch 32/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6696 - accuracy: 0.5414 - val_loss: 0.6994 - val_accuracy: 0.5149\n",
            "Epoch 33/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6702 - accuracy: 0.5359 - val_loss: 0.6983 - val_accuracy: 0.5145\n",
            "Epoch 34/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6690 - accuracy: 0.5414 - val_loss: 0.6997 - val_accuracy: 0.5133\n",
            "Epoch 35/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6686 - accuracy: 0.5421 - val_loss: 0.7008 - val_accuracy: 0.5120\n",
            "Epoch 36/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6686 - accuracy: 0.5421 - val_loss: 0.6982 - val_accuracy: 0.5129\n",
            "Epoch 37/100\n",
            "73/73 [==============================] - 7s 90ms/step - loss: 0.6685 - accuracy: 0.5421 - val_loss: 0.7000 - val_accuracy: 0.5140\n",
            "Epoch 38/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6681 - accuracy: 0.5422 - val_loss: 0.7009 - val_accuracy: 0.5129\n",
            "Epoch 39/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6682 - accuracy: 0.5424 - val_loss: 0.7014 - val_accuracy: 0.5129\n",
            "Epoch 40/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6675 - accuracy: 0.5427 - val_loss: 0.7023 - val_accuracy: 0.5142\n",
            "Epoch 41/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6675 - accuracy: 0.5428 - val_loss: 0.7030 - val_accuracy: 0.5147\n",
            "Epoch 42/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6671 - accuracy: 0.5429 - val_loss: 0.7009 - val_accuracy: 0.5126\n",
            "Epoch 43/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6667 - accuracy: 0.5432 - val_loss: 0.7080 - val_accuracy: 0.5115\n",
            "Epoch 44/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6681 - accuracy: 0.5428 - val_loss: 0.7010 - val_accuracy: 0.5129\n",
            "Epoch 45/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6668 - accuracy: 0.5430 - val_loss: 0.7029 - val_accuracy: 0.5123\n",
            "Epoch 46/100\n",
            "73/73 [==============================] - 6s 87ms/step - loss: 0.6662 - accuracy: 0.5436 - val_loss: 0.7040 - val_accuracy: 0.5124\n",
            "Epoch 47/100\n",
            "73/73 [==============================] - 5s 75ms/step - loss: 0.6662 - accuracy: 0.5436 - val_loss: 0.7051 - val_accuracy: 0.5142\n",
            "Epoch 48/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6654 - accuracy: 0.5441 - val_loss: 0.7057 - val_accuracy: 0.5139\n",
            "Epoch 49/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6655 - accuracy: 0.5440 - val_loss: 0.7045 - val_accuracy: 0.5137\n",
            "Epoch 50/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6653 - accuracy: 0.5443 - val_loss: 0.7050 - val_accuracy: 0.5141\n",
            "Epoch 51/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6649 - accuracy: 0.5446 - val_loss: 0.7043 - val_accuracy: 0.5134\n",
            "Epoch 52/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6647 - accuracy: 0.5447 - val_loss: 0.7047 - val_accuracy: 0.5141\n",
            "Epoch 53/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6646 - accuracy: 0.5448 - val_loss: 0.7051 - val_accuracy: 0.5136\n",
            "Epoch 54/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6641 - accuracy: 0.5450 - val_loss: 0.7076 - val_accuracy: 0.5140\n",
            "Epoch 55/100\n",
            "73/73 [==============================] - 6s 86ms/step - loss: 0.6643 - accuracy: 0.5449 - val_loss: 0.7059 - val_accuracy: 0.5138\n",
            "Epoch 56/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6645 - accuracy: 0.5447 - val_loss: 0.7075 - val_accuracy: 0.5141\n",
            "Epoch 57/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6641 - accuracy: 0.5449 - val_loss: 0.7067 - val_accuracy: 0.5143\n",
            "Epoch 58/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6637 - accuracy: 0.5452 - val_loss: 0.7064 - val_accuracy: 0.5136\n",
            "Epoch 59/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6638 - accuracy: 0.5454 - val_loss: 0.7073 - val_accuracy: 0.5133\n",
            "Epoch 60/100\n",
            "73/73 [==============================] - 6s 78ms/step - loss: 0.6637 - accuracy: 0.5456 - val_loss: 0.7090 - val_accuracy: 0.5134\n",
            "Epoch 61/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6632 - accuracy: 0.5455 - val_loss: 0.7075 - val_accuracy: 0.5134\n",
            "Epoch 62/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6634 - accuracy: 0.5454 - val_loss: 0.7070 - val_accuracy: 0.5140\n",
            "Epoch 63/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6631 - accuracy: 0.5456 - val_loss: 0.7092 - val_accuracy: 0.5137\n",
            "Epoch 64/100\n",
            "73/73 [==============================] - 6s 79ms/step - loss: 0.6629 - accuracy: 0.5459 - val_loss: 0.7116 - val_accuracy: 0.5146\n",
            "Epoch 65/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6629 - accuracy: 0.5457 - val_loss: 0.7078 - val_accuracy: 0.5132\n",
            "Epoch 66/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6625 - accuracy: 0.5461 - val_loss: 0.7091 - val_accuracy: 0.5137\n",
            "Epoch 67/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6630 - accuracy: 0.5456 - val_loss: 0.7079 - val_accuracy: 0.5127\n",
            "Epoch 68/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6627 - accuracy: 0.5457 - val_loss: 0.7104 - val_accuracy: 0.5144\n",
            "Epoch 69/100\n",
            "73/73 [==============================] - 6s 77ms/step - loss: 0.6621 - accuracy: 0.5463 - val_loss: 0.7103 - val_accuracy: 0.5138\n",
            "Epoch 70/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6622 - accuracy: 0.5462 - val_loss: 0.7100 - val_accuracy: 0.5142\n",
            "Epoch 71/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6622 - accuracy: 0.5465 - val_loss: 0.7082 - val_accuracy: 0.5132\n",
            "Epoch 72/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6620 - accuracy: 0.5464 - val_loss: 0.7101 - val_accuracy: 0.5133\n",
            "Epoch 73/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6618 - accuracy: 0.5465 - val_loss: 0.7098 - val_accuracy: 0.5136\n",
            "Epoch 74/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6619 - accuracy: 0.5465 - val_loss: 0.7109 - val_accuracy: 0.5134\n",
            "Epoch 75/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6617 - accuracy: 0.5464 - val_loss: 0.7119 - val_accuracy: 0.5132\n",
            "Epoch 76/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6617 - accuracy: 0.5467 - val_loss: 0.7112 - val_accuracy: 0.5139\n",
            "Epoch 77/100\n",
            "73/73 [==============================] - 5s 73ms/step - loss: 0.6613 - accuracy: 0.5468 - val_loss: 0.7159 - val_accuracy: 0.5140\n",
            "Epoch 78/100\n",
            "73/73 [==============================] - 7s 92ms/step - loss: 0.6613 - accuracy: 0.5469 - val_loss: 0.7135 - val_accuracy: 0.5139\n",
            "Epoch 79/100\n",
            "73/73 [==============================] - 7s 93ms/step - loss: 0.6612 - accuracy: 0.5471 - val_loss: 0.7133 - val_accuracy: 0.5134\n",
            "Epoch 80/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6613 - accuracy: 0.5469 - val_loss: 0.7142 - val_accuracy: 0.5140\n",
            "Epoch 81/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6611 - accuracy: 0.5471 - val_loss: 0.7148 - val_accuracy: 0.5148\n",
            "Epoch 82/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6610 - accuracy: 0.5470 - val_loss: 0.7133 - val_accuracy: 0.5136\n",
            "Epoch 83/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6607 - accuracy: 0.5474 - val_loss: 0.7139 - val_accuracy: 0.5143\n",
            "Epoch 84/100\n",
            "73/73 [==============================] - 6s 84ms/step - loss: 0.6609 - accuracy: 0.5466 - val_loss: 0.7140 - val_accuracy: 0.5135\n",
            "Epoch 85/100\n",
            "73/73 [==============================] - 6s 82ms/step - loss: 0.6605 - accuracy: 0.5474 - val_loss: 0.7128 - val_accuracy: 0.5128\n",
            "Epoch 86/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6606 - accuracy: 0.5474 - val_loss: 0.7133 - val_accuracy: 0.5135\n",
            "Epoch 87/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6604 - accuracy: 0.5473 - val_loss: 0.7152 - val_accuracy: 0.5133\n",
            "Epoch 88/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6604 - accuracy: 0.5476 - val_loss: 0.7138 - val_accuracy: 0.5129\n",
            "Epoch 89/100\n",
            "73/73 [==============================] - 5s 72ms/step - loss: 0.6599 - accuracy: 0.5473 - val_loss: 0.7165 - val_accuracy: 0.5133\n",
            "Epoch 90/100\n",
            "73/73 [==============================] - 6s 85ms/step - loss: 0.6604 - accuracy: 0.5478 - val_loss: 0.7141 - val_accuracy: 0.5139\n",
            "Epoch 91/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6602 - accuracy: 0.5477 - val_loss: 0.7136 - val_accuracy: 0.5136\n",
            "Epoch 92/100\n",
            "73/73 [==============================] - 5s 71ms/step - loss: 0.6604 - accuracy: 0.5474 - val_loss: 0.7167 - val_accuracy: 0.5142\n",
            "Epoch 93/100\n",
            "73/73 [==============================] - 6s 83ms/step - loss: 0.6602 - accuracy: 0.5477 - val_loss: 0.7156 - val_accuracy: 0.5142\n",
            "Epoch 94/100\n",
            "73/73 [==============================] - 6s 80ms/step - loss: 0.6598 - accuracy: 0.5463 - val_loss: 0.7150 - val_accuracy: 0.5132\n",
            "Epoch 95/100\n",
            "73/73 [==============================] - 5s 70ms/step - loss: 0.6597 - accuracy: 0.5480 - val_loss: 0.7154 - val_accuracy: 0.5128\n",
            "Epoch 96/100\n",
            "73/73 [==============================] - 7s 90ms/step - loss: 0.6598 - accuracy: 0.5479 - val_loss: 0.7158 - val_accuracy: 0.5131\n",
            "Epoch 97/100\n",
            "73/73 [==============================] - 5s 74ms/step - loss: 0.6598 - accuracy: 0.5479 - val_loss: 0.7140 - val_accuracy: 0.5133\n",
            "Epoch 98/100\n",
            "73/73 [==============================] - 6s 76ms/step - loss: 0.6595 - accuracy: 0.5480 - val_loss: 0.7147 - val_accuracy: 0.5129\n",
            "Epoch 99/100\n",
            "73/73 [==============================] - 6s 81ms/step - loss: 0.6596 - accuracy: 0.5481 - val_loss: 0.7149 - val_accuracy: 0.5135\n",
            "Epoch 100/100\n",
            "73/73 [==============================] - 6s 75ms/step - loss: 0.6594 - accuracy: 0.5481 - val_loss: 0.7148 - val_accuracy: 0.5136\n",
            "19/19 [==============================] - 1s 22ms/step - loss: 0.7178 - accuracy: 0.5128\n",
            "Best accuracy for dataset 4: 0.5434162616729736\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize dictionary to store best models and their performances\n",
        "best_models = {}\n",
        "\n",
        "for df_idx, df in enumerate(dfs):\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Prepare data\n",
        "    X_emb = pad_sequences(df['emb'], dtype='float32', padding='post')\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    for fold_idx, (train_index, val_index) in enumerate(kf.split(X_emb)):\n",
        "        X_emb_train, X_emb_val = X_emb[train_index], X_emb[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        model = create_model()\n",
        "        print(len((y_train.reshape(-1, 1))))\n",
        "        history = model.fit(X_emb_train, y_train.reshape(-1, 1), epochs=100, batch_size=32, validation_data=(X_emb_val, y_val.reshape(-1, 1)))\n",
        "\n",
        "        # Evaluate model performance\n",
        "        _, accuracy = model.evaluate(X_emb_val, y_val)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model\n",
        "\n",
        "    # Save the best model for this dataset\n",
        "    best_models[f'df_{df_idx}'] = best_model\n",
        "    print(f\"Best accuracy for dataset {df_idx}: {best_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFwkcgJmoUv3",
        "outputId": "f236a092-16bf-438a-8bf0-2fa2b1554c04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "for idx, (name, model) in enumerate(best_models.items()):\n",
        "    model.save(f'/content/drive/MyDrive/best_model_bbbp{name}.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DIvTuGoETnp"
      },
      "source": [
        "load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J9QKaH5dESs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1671346e-43dd-44aa-f50e-286b77848e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "best_models = []\n",
        "for cnt in range(5):\n",
        "  best_models.append(load_model('/content/drive/MyDrive/best_model_bbbpdf_0' + '.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LqXD5WXxTc"
      },
      "source": [
        "Evaluating the performances of the best models using ROC\n",
        "fare file log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_EVM5KRu---_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "41270847-5246-4ab7-8851-e14b3b3e0d15"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dfs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8a71b736372e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nModel {idx + 1}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mX_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open a file to write the results\n",
        "with open(\"/content/drive/MyDrive/metrics_results_bbbp_models.txt\", \"w\") as file:\n",
        "\n",
        "    # Loop through best models\n",
        "    for idx, model in enumerate(best_models):\n",
        "        file.write(f\"\\nModel {idx + 1}\\n\")\n",
        "\n",
        "        df = dfs[idx]\n",
        "        X_emb = pad_sequences(df['emb'], dtype='float32', padding='post')\n",
        "        y = np.array(df['label'])\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_emb_train, X_emb_test, y_train, y_test = train_test_split(X_emb, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        # Pad sequences if necessary\n",
        "        X_emb_train = pad_sequences(X_emb_train, dtype='float32', padding='post')\n",
        "        X_emb_test = pad_sequences(X_emb_test, dtype='float32', padding='post')\n",
        "\n",
        "        # Reshape labels if necessary\n",
        "        y_train_reshaped = y_train.reshape(-1, 1)\n",
        "        y_test_reshaped = y_test.reshape(-1, 1)\n",
        "\n",
        "        # predict probabilities for test set\n",
        "        yhat_probs = model.predict(X_emb_test, verbose=0)\n",
        "        # predict crisp classes for test set\n",
        "        y_classes = np.argmax(yhat_probs, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_classes)\n",
        "        precision = precision_score(y_test, y_classes, labels=[1] , average = 'weighted')\n",
        "        recall = recall_score(y_test, y_classes, labels=[1], average = 'weighted')\n",
        "        f1 = f1_score(y_test, y_classes, labels=[1], average = 'weighted')\n",
        "        conf_matrix = confusion_matrix(y_test, y_classes)\n",
        "\n",
        "        file.write(\"Accuracy: {}\\n\".format(accuracy))\n",
        "        file.write(\"Precision: {}\\n\".format(precision))\n",
        "        file.write(\"Recall: {}\\n\".format(recall))\n",
        "        file.write(\"F1 Score: {}\\n\".format(f1))\n",
        "\n",
        "        # Calculate ROC AUC\n",
        "        roc_auc = roc_auc_score(y_test, y_classes)\n",
        "        file.write(\"ROC AUC: {}\\n\".format(roc_auc))\n",
        "\n",
        "        # Plot ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_classes)\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC)')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig(\"ROC_Model{}.png\".format(idx + 1))  # Save ROC curve plot\n",
        "        plt.close()\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plot_confusion_matrix(y_test, y_classes, class_names=['Class 0', 'Class 1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOsT4DIMB35f"
      },
      "source": [
        "Integrated Gradients - on the classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb4mR4N3mhhi",
        "outputId": "1ecab155-82b8-4d51-ff20-92a3ca14f4ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 15ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def integrated_gradients(input_data, model):\n",
        "    \"\"\"\n",
        "    Calculate integrated gradients for a given input_data and model\n",
        "    \"\"\"\n",
        "    # Define the baseline as all zeros\n",
        "    baseline = np.zeros_like(input_data)\n",
        "\n",
        "    # Create a linear interpolation path from baseline to the actual input\n",
        "    steps = 50\n",
        "    interpolated_points = [baseline + (i/steps) * (input_data - baseline) for i in range(steps+1)]\n",
        "\n",
        "    # Convert to numpy array\n",
        "    interpolated_points = np.array(interpolated_points)\n",
        "\n",
        "    # Convert to TensorFlow tensor\n",
        "    interpolated_points = tf.convert_to_tensor(interpolated_points, dtype=tf.float32)\n",
        "\n",
        "    # Get model predictions for the interpolated points\n",
        "    preds = model.predict(interpolated_points)\n",
        "\n",
        "    # Create a GradientTape to record operations for automatic differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_points)\n",
        "        predictions = model(interpolated_points)\n",
        "\n",
        "    # Calculate gradients with respect to input\n",
        "    gradients = tape.gradient(predictions, interpolated_points)\n",
        "\n",
        "    # Define the integrated gradients\n",
        "    integrated_grads = np.mean(gradients.numpy(), axis=0) * (input_data - baseline)\n",
        "\n",
        "    # Sum along the path to approximate the integral\n",
        "    integrated_grads = np.sum(integrated_grads, axis=1)\n",
        "\n",
        "    return integrated_grads\n",
        "\n",
        "# Example usage\n",
        "index_to_explain = 4  # Choose an index to explain\n",
        "embedding_to_explain = X_emb[index_to_explain]\n",
        "integrated_grads_result = integrated_gradients(embedding_to_explain, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "o1FqpJ0Q-5bR",
        "outputId": "3b8c6564-e35b-40f5-884d-2071d187b77b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an array of indices to use as x-axis\n",
        "indices = np.arange(len(integrated_grads_result))\n",
        "\n",
        "# Filter out non-zero values and their corresponding indices\n",
        "non_zero_indices = [i for i, val in enumerate(integrated_grads_result) if val != 0]\n",
        "non_zero_values = [val for val in integrated_grads_result if val != 0]\n",
        "\n",
        "# Plot non-zero integrated gradients\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(non_zero_indices, non_zero_values, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Integrated Gradients (Non-zero)')\n",
        "plt.title('Non-zero Integrated Gradients for Explaining Model Prediction')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SDNE"
      ],
      "metadata": {
        "id": "-Bs0fWiRsIlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch import nn, optim\n",
        "\n",
        "# Assuming 'all_data' is already defined and contains the graph data\n",
        "\n",
        "def create_networkx_graph(edge_index, edge_attr, x, y, num_nodes):\n",
        "    # Create an empty NetworkX graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with features\n",
        "    for i in range(num_nodes):\n",
        "        node_features = {f\"feat_{j}\": x[i, j].item() for j in range(x.shape[1])}\n",
        "        G.add_node(i, **node_features)\n",
        "\n",
        "    # Add edges with attributes\n",
        "    for j in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[:, j].tolist()\n",
        "        edge_attributes = {f\"attr_{k}\": edge_attr[j, k].item() for k in range(edge_attr.shape[1])}\n",
        "        G.add_edge(src, dst, **edge_attributes)\n",
        "\n",
        "    return G, y.item()\n",
        "\n",
        "class SDNE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, alpha=1e-5, beta=5):\n",
        "        super(SDNE, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # Encode\n",
        "        y = self.encoder(x)\n",
        "        # Decode\n",
        "        x_hat = self.decoder(y)\n",
        "        return x_hat, y\n",
        "\n",
        "    def loss_function(self, x, x_hat, y, adj):\n",
        "        # Reconstruction loss\n",
        "        mse = nn.MSELoss()\n",
        "        L_1st = mse(x_hat, x)\n",
        "\n",
        "        # Laplacian regularization\n",
        "        L_2nd = torch.sum(adj * torch.norm(y.unsqueeze(1) - y, dim=2))\n",
        "\n",
        "        return L_1st + self.alpha * L_1st + self.beta * L_2nd\n",
        "\n",
        "def train_sdne(graph, hidden_dims=[128, 64], epochs=100, lr=0.01):\n",
        "    # Create adjacency matrix\n",
        "    adj = nx.adjacency_matrix(graph).todense()\n",
        "    adj = torch.tensor(adj, dtype=torch.float32)\n",
        "\n",
        "    # Get node features\n",
        "    node_features = np.array([list(graph.nodes[i].values()) for i in range(graph.number_of_nodes())])\n",
        "\n",
        "    # Check if node features are empty\n",
        "    if node_features.shape[1] == 0:\n",
        "        raise ValueError(\"Node features are empty. Ensure nodes have features.\")\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    node_features = scaler.fit_transform(node_features)\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float32)\n",
        "\n",
        "    # Initialize model\n",
        "    input_dim = node_features.shape[1]\n",
        "    model = SDNE(input_dim, hidden_dims)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, y = model(node_features, adj)\n",
        "        loss = model.loss_function(node_features, x_hat, y, adj)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #if (epoch + 1) % 10 == 0:\n",
        "            #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return model, y\n",
        "\n",
        "def balance_dataset(all_data, seed=42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    label_0_indices = [i for i, data in enumerate(all_data) if data.y.item() == 0]\n",
        "    label_1_indices = [i for i, data in enumerate(all_data) if data.y.item() == 1]\n",
        "\n",
        "    min_size_dataset = min(len(label_0_indices), len(label_1_indices))\n",
        "\n",
        "    random.shuffle(label_0_indices)\n",
        "    random.shuffle(label_1_indices)\n",
        "\n",
        "    label_0_indices = label_0_indices[:min_size_dataset]\n",
        "    label_1_indices = label_1_indices[:min_size_dataset]\n",
        "    print(len(label_0_indices))\n",
        "    print(len(label_1_indices))\n",
        "\n",
        "    balanced_indices = label_0_indices + label_1_indices\n",
        "    random.shuffle(balanced_indices)\n",
        "\n",
        "    balanced_data = [all_data[i] for i in balanced_indices]\n",
        "\n",
        "    return balanced_data\n",
        "\n",
        "# Generate and save embeddings for the balanced datasets\n",
        "if __name__ == \"__main__\":\n",
        "    all_graph_embeddings = []\n",
        "\n",
        "    for i in range(1):  # Loop to generate 5 different datasets\n",
        "        print(f'Generating dataset {i + 1}')\n",
        "        balanced_data = balance_dataset(all_data)\n",
        "\n",
        "        for data in balanced_data:\n",
        "            num_nodes = data.num_nodes\n",
        "            edge_index = data.edge_index\n",
        "            edge_attr = data.edge_attr\n",
        "            x = data.x\n",
        "            y = data.y\n",
        "\n",
        "            # Create graph\n",
        "            G, y_label = create_networkx_graph(edge_index, edge_attr, x, y, num_nodes)\n",
        "\n",
        "            # Train SDNE\n",
        "            model, embeddings = train_sdne(G, epochs=100)\n",
        "\n",
        "            # Collect node embeddings and aggregate them to obtain a graph-level embedding\n",
        "            embeddings_np = embeddings.detach().numpy()\n",
        "            graph_embedding = np.mean(embeddings_np, axis=0)  # Averaging node embeddings\n",
        "            all_graph_embeddings.append((graph_embedding.tolist(), y_label))\n",
        "\n",
        "    # Convert graph-level embeddings to DataFrame\n",
        "    df_graph_embeddings = pd.DataFrame(all_graph_embeddings, columns=[\"embedding\", \"label\"])\n",
        "\n",
        "    # Save the DataFrame to a CSV file (optional)\n",
        "    df_graph_embeddings.to_csv(\"graph_embeddings.csv\", index=False)\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(df_graph_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxLnsdjK-Y2p",
        "outputId": "165529cf-1bf6-4c0e-e873-59e818cb7c81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating dataset 1\n",
            "1443\n",
            "1443\n",
            "                                              embedding  label\n",
            "0     [-0.058513201773166656, -0.018007565289735794,...      1\n",
            "1     [-0.02449730411171913, -0.014300267212092876, ...      0\n",
            "2     [-0.05463476851582527, 0.052841268479824066, -...      0\n",
            "3     [-0.09562081843614578, 0.02884604223072529, -0...      0\n",
            "4     [0.05240524560213089, 0.09314791858196259, -0....      1\n",
            "...                                                 ...    ...\n",
            "2881  [-0.016942720860242844, 0.007493229117244482, ...      0\n",
            "2882  [-0.04370803013443947, -0.1348639577627182, 0....      0\n",
            "2883  [-0.06855199486017227, -0.04499823972582817, 0...      0\n",
            "2884  [-0.009918007999658585, 0.08222417533397675, 0...      1\n",
            "2885  [-0.02948945201933384, 0.07188456505537033, 0....      1\n",
            "\n",
            "[2886 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_graph_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-07XH-30TkTh",
        "outputId": "6da0ccdf-d54c-4a56-dedf-01206bfd2e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              embedding  label\n",
              "0     [0.012872357852756977, 0.036167778074741364, 0...      1\n",
              "1     [0.02220860868692398, -0.0034232004545629025, ...      1\n",
              "2     [-0.034402377903461456, 0.03993811085820198, 0...      0\n",
              "3     [0.03250516951084137, -0.041709255427122116, 0...      1\n",
              "4     [0.0890597328543663, -0.018752433359622955, 0....      1\n",
              "...                                                 ...    ...\n",
              "2881  [-0.07019953429698944, 0.03325891122221947, 0....      0\n",
              "2882  [0.007876122370362282, 0.04717765375971794, 0....      1\n",
              "2883  [-0.11123344302177429, 0.02893875353038311, -0...      0\n",
              "2884  [-0.000383681443054229, -0.043122611939907074,...      1\n",
              "2885  [0.049928534775972366, -0.03744585067033768, -...      0\n",
              "\n",
              "[2886 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c053596-ad0c-4b57-b6af-476571d3179a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>embedding</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.012872357852756977, 0.036167778074741364, 0...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.02220860868692398, -0.0034232004545629025, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.034402377903461456, 0.03993811085820198, 0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.03250516951084137, -0.041709255427122116, 0...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.0890597328543663, -0.018752433359622955, 0....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2881</th>\n",
              "      <td>[-0.07019953429698944, 0.03325891122221947, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2882</th>\n",
              "      <td>[0.007876122370362282, 0.04717765375971794, 0....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2883</th>\n",
              "      <td>[-0.11123344302177429, 0.02893875353038311, -0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2884</th>\n",
              "      <td>[-0.000383681443054229, -0.043122611939907074,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2885</th>\n",
              "      <td>[0.049928534775972366, -0.03744585067033768, -...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2886 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c053596-ad0c-4b57-b6af-476571d3179a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2c053596-ad0c-4b57-b6af-476571d3179a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2c053596-ad0c-4b57-b6af-476571d3179a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e573a062-bab1-4d84-ab47-6d0c9a91a315\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e573a062-bab1-4d84-ab47-6d0c9a91a315')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e573a062-bab1-4d84-ab47-6d0c9a91a315 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ba8ef02c-a0c1-4783-a174-2db693a7dc6a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_graph_embeddings')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ba8ef02c-a0c1-4783-a174-2db693a7dc6a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_graph_embeddings');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_graph_embeddings",
              "summary": "{\n  \"name\": \"df_graph_embeddings\",\n  \"rows\": 2886,\n  \"fields\": [\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import ast\n",
        "\n",
        "# Load the balanced dataset\n",
        "df_graph_embeddings = pd.read_csv(\"graph_embeddings.csv\")\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    embeddings = []\n",
        "    labels = df['label'].tolist()\n",
        "\n",
        "    for emb in df['embedding']:\n",
        "        if isinstance(emb, str):\n",
        "            # Convert string representation of list back to list\n",
        "            emb = ast.literal_eval(emb)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    X = np.array(embeddings)\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "X, y = extract_features_labels(df_graph_embeddings)\n",
        "\n",
        "# Check if data is loaded correctly\n",
        "if X.shape[0] == 0 or X.shape[1] == 0:\n",
        "    raise ValueError(\"No valid embeddings found. Please check the DataFrame for formatting issues.\")\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Perform 5-fold cross-validation on the entire dataset\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "roc_aucs = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "histories = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f'Fold {fold_no}')\n",
        "\n",
        "    X_train_val, X_test = X[train_index], X[test_index]\n",
        "    y_train_val, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Perform an additional split to get a validation set from the training+validation set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val)\n",
        "\n",
        "    # Create and train the neural network\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = create_model(input_dim)\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f'Fold {fold_no} Validation Accuracy: {val_accuracy:.4f}')\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    histories.append(history.history)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Fold {fold_no} Test Accuracy: {test_accuracy:.4f}')\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Predict probabilities and calculate metrics for the test set\n",
        "    y_test_pred_proba = model.predict(X_test).flatten()\n",
        "    y_test_pred = (y_test_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "    roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
        "    precision = precision_score(y_test, y_test_pred)\n",
        "    recall = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f'Fold {fold_no} ROC-AUC: {roc_auc:.4f}')\n",
        "    print(f'Fold {fold_no} Precision: {precision:.4f}')\n",
        "    print(f'Fold {fold_no} Recall: {recall:.4f}')\n",
        "    print(f'Fold {fold_no} F1-Score: {f1:.4f}')\n",
        "\n",
        "    roc_aucs.append(roc_auc)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Plot confusion matrix for each fold\n",
        "    plot_confusion_matrix(y_test, y_test_pred, class_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "# Calculate and print the average validation accuracy and variance\n",
        "average_val_accuracy = np.mean(val_accuracies)\n",
        "variance_val_accuracy = np.var(val_accuracies)\n",
        "print(f'Average Validation Accuracy: {average_val_accuracy:.4f}')\n",
        "print(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}')\n",
        "\n",
        "# Calculate and print the average test accuracy and variance\n",
        "average_test_accuracy = np.mean(test_accuracies)\n",
        "variance_test_accuracy = np.var(test_accuracies)\n",
        "print(f'Average Test Accuracy: {average_test_accuracy:.4f}')\n",
        "print(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}')\n",
        "\n",
        "# Calculate and print the average ROC-AUC, Precision, Recall, and F1-Score\n",
        "average_roc_auc = np.mean(roc_aucs)\n",
        "average_precision = np.mean(precisions)\n",
        "average_recall = np.mean(recalls)\n",
        "average_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(f'Average ROC-AUC: {average_roc_auc:.4f}')\n",
        "print(f'Average Precision: {average_precision:.4f}')\n",
        "print(f'Average Recall: {average_recall:.4f}')\n",
        "print(f'Average F1-Score: {average_f1_score:.4f}')\n",
        "\n",
        "# Save the average validation accuracy, variance, test accuracy, and metrics to a file\n",
        "with open(\"model_accuracies.txt\", \"w\") as f:\n",
        "    f.write(f'Average Validation Accuracy: {average_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Average Test Accuracy: {average_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Average ROC-AUC: {average_roc_auc:.4f}\\n')\n",
        "    f.write(f'Average Precision: {average_precision:.4f}\\n')\n",
        "    f.write(f'Average Recall: {average_recall:.4f}\\n')\n",
        "    f.write(f'Average F1-Score: {average_f1_score:.4f}\\n')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Save training history for each fold (optional)\n",
        "with open(\"training_histories.txt\", \"w\") as f:\n",
        "    for i, history in enumerate(histories):\n",
        "        f.write(f'\\nFold {i + 1} History:\\n')\n",
        "        f.write(str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EAqVMiFSQA_I",
        "outputId": "d8c187a3-ac95-4f04-f71a-eeedb8a8e258"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5255 - loss: 0.6936 - val_accuracy: 0.4187 - val_loss: 0.6950\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5421 - loss: 0.6915 - val_accuracy: 0.4533 - val_loss: 0.6957\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5477 - loss: 0.6899 - val_accuracy: 0.4187 - val_loss: 0.6969\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5598 - loss: 0.6874 - val_accuracy: 0.4464 - val_loss: 0.6997\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5567 - loss: 0.6871 - val_accuracy: 0.4394 - val_loss: 0.7006\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5809 - loss: 0.6797 - val_accuracy: 0.4498 - val_loss: 0.7063\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6114 - loss: 0.6709 - val_accuracy: 0.4394 - val_loss: 0.7187\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6605 - loss: 0.6533 - val_accuracy: 0.4291 - val_loss: 0.7341\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6321 - loss: 0.6494 - val_accuracy: 0.4671 - val_loss: 0.7451\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6671 - loss: 0.6348 - val_accuracy: 0.4464 - val_loss: 0.7664\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6763 - loss: 0.6127 - val_accuracy: 0.4637 - val_loss: 0.7793\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7129 - loss: 0.5873 - val_accuracy: 0.4671 - val_loss: 0.7941\n",
            "Epoch 13/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7250 - loss: 0.5796 - val_accuracy: 0.4775 - val_loss: 0.8090\n",
            "Epoch 14/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7323 - loss: 0.5497 - val_accuracy: 0.4810 - val_loss: 0.8307\n",
            "Epoch 15/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7559 - loss: 0.5255 - val_accuracy: 0.4740 - val_loss: 0.8493\n",
            "Epoch 16/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7503 - loss: 0.5116 - val_accuracy: 0.5052 - val_loss: 0.8747\n",
            "Epoch 17/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7722 - loss: 0.4799 - val_accuracy: 0.4948 - val_loss: 0.8695\n",
            "Epoch 18/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7913 - loss: 0.4668 - val_accuracy: 0.4775 - val_loss: 0.9175\n",
            "Epoch 19/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8072 - loss: 0.4218 - val_accuracy: 0.4706 - val_loss: 0.9813\n",
            "Epoch 20/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8102 - loss: 0.4268 - val_accuracy: 0.4637 - val_loss: 0.9505\n",
            "Epoch 21/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8563 - loss: 0.3746 - val_accuracy: 0.4810 - val_loss: 1.0738\n",
            "Epoch 22/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8402 - loss: 0.3665 - val_accuracy: 0.4775 - val_loss: 1.0708\n",
            "Epoch 23/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8917 - loss: 0.2920 - val_accuracy: 0.4706 - val_loss: 1.1298\n",
            "Epoch 24/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8773 - loss: 0.3051 - val_accuracy: 0.4740 - val_loss: 1.1923\n",
            "Epoch 25/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8964 - loss: 0.2866 - val_accuracy: 0.4879 - val_loss: 1.1976\n",
            "Epoch 26/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8945 - loss: 0.2816 - val_accuracy: 0.4637 - val_loss: 1.2545\n",
            "Epoch 27/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8797 - loss: 0.2763 - val_accuracy: 0.4983 - val_loss: 1.2623\n",
            "Epoch 28/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9005 - loss: 0.2472 - val_accuracy: 0.4983 - val_loss: 1.2945\n",
            "Epoch 29/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.2198 - val_accuracy: 0.4844 - val_loss: 1.3794\n",
            "Epoch 30/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9030 - loss: 0.2224 - val_accuracy: 0.4844 - val_loss: 1.4011\n",
            "Epoch 31/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9138 - loss: 0.2186 - val_accuracy: 0.4913 - val_loss: 1.4023\n",
            "Epoch 32/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2226 - val_accuracy: 0.4810 - val_loss: 1.4092\n",
            "Epoch 33/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9187 - loss: 0.2134 - val_accuracy: 0.4775 - val_loss: 1.4314\n",
            "Epoch 34/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9502 - loss: 0.1612 - val_accuracy: 0.4983 - val_loss: 1.4744\n",
            "Epoch 35/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9251 - loss: 0.1809 - val_accuracy: 0.5017 - val_loss: 1.5134\n",
            "Epoch 36/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9305 - loss: 0.1830 - val_accuracy: 0.5052 - val_loss: 1.5672\n",
            "Epoch 37/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9308 - loss: 0.1751 - val_accuracy: 0.4983 - val_loss: 1.5681\n",
            "Epoch 38/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9425 - loss: 0.1746 - val_accuracy: 0.4879 - val_loss: 1.6229\n",
            "Epoch 39/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9274 - loss: 0.1769 - val_accuracy: 0.4740 - val_loss: 1.6035\n",
            "Epoch 40/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9242 - loss: 0.1832 - val_accuracy: 0.4810 - val_loss: 1.5816\n",
            "Epoch 41/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9485 - loss: 0.1405 - val_accuracy: 0.4913 - val_loss: 1.6087\n",
            "Epoch 42/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9557 - loss: 0.1334 - val_accuracy: 0.4913 - val_loss: 1.6975\n",
            "Epoch 43/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9603 - loss: 0.1187 - val_accuracy: 0.4810 - val_loss: 1.7777\n",
            "Epoch 44/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9562 - loss: 0.1232 - val_accuracy: 0.4913 - val_loss: 1.7691\n",
            "Epoch 45/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9452 - loss: 0.1581 - val_accuracy: 0.4983 - val_loss: 1.7447\n",
            "Epoch 46/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9661 - loss: 0.0959 - val_accuracy: 0.4983 - val_loss: 1.8248\n",
            "Epoch 47/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1135 - val_accuracy: 0.4913 - val_loss: 1.8223\n",
            "Epoch 48/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9382 - loss: 0.1294 - val_accuracy: 0.5017 - val_loss: 1.8193\n",
            "Epoch 49/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9683 - loss: 0.1152 - val_accuracy: 0.4913 - val_loss: 1.8407\n",
            "Epoch 50/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9651 - loss: 0.1042 - val_accuracy: 0.4844 - val_loss: 1.9642\n",
            "Fold 1 Validation Accuracy: 0.4844\n",
            "Fold 1 Test Accuracy: 0.4969\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Fold 1 ROC-AUC: 0.4869\n",
            "Fold 1 Precision: 0.4895\n",
            "Fold 1 Recall: 0.4909\n",
            "Fold 1 F1-Score: 0.4902\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5246 - loss: 0.6939 - val_accuracy: 0.5052 - val_loss: 0.6922\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5122 - loss: 0.6931 - val_accuracy: 0.4983 - val_loss: 0.6928\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5263 - loss: 0.6900 - val_accuracy: 0.5225 - val_loss: 0.6931\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5233 - loss: 0.6907 - val_accuracy: 0.5121 - val_loss: 0.6936\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5606 - loss: 0.6880 - val_accuracy: 0.4879 - val_loss: 0.6953\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5654 - loss: 0.6821 - val_accuracy: 0.4948 - val_loss: 0.6959\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5586 - loss: 0.6770 - val_accuracy: 0.4948 - val_loss: 0.6990\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6025 - loss: 0.6622 - val_accuracy: 0.5017 - val_loss: 0.7069\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6305 - loss: 0.6542 - val_accuracy: 0.4740 - val_loss: 0.7091\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6645 - loss: 0.6413 - val_accuracy: 0.5052 - val_loss: 0.7232\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6379 - loss: 0.6291 - val_accuracy: 0.4913 - val_loss: 0.7276\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7129 - loss: 0.6054 - val_accuracy: 0.4844 - val_loss: 0.7418\n",
            "Epoch 13/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7262 - loss: 0.5749 - val_accuracy: 0.4879 - val_loss: 0.7540\n",
            "Epoch 14/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7661 - loss: 0.5554 - val_accuracy: 0.4879 - val_loss: 0.7639\n",
            "Epoch 15/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7267 - loss: 0.5419 - val_accuracy: 0.5017 - val_loss: 0.7834\n",
            "Epoch 16/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7587 - loss: 0.5126 - val_accuracy: 0.4810 - val_loss: 0.8178\n",
            "Epoch 17/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7824 - loss: 0.4930 - val_accuracy: 0.5087 - val_loss: 0.8389\n",
            "Epoch 18/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8141 - loss: 0.4117 - val_accuracy: 0.5190 - val_loss: 0.8591\n",
            "Epoch 19/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8345 - loss: 0.4044 - val_accuracy: 0.5294 - val_loss: 0.8995\n",
            "Epoch 20/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8344 - loss: 0.3889 - val_accuracy: 0.5398 - val_loss: 0.8805\n",
            "Epoch 21/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8387 - loss: 0.3765 - val_accuracy: 0.5190 - val_loss: 0.8973\n",
            "Epoch 22/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8554 - loss: 0.3306 - val_accuracy: 0.5329 - val_loss: 0.9357\n",
            "Epoch 23/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8725 - loss: 0.3266 - val_accuracy: 0.5121 - val_loss: 0.9698\n",
            "Epoch 24/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8568 - loss: 0.3285 - val_accuracy: 0.5121 - val_loss: 1.0232\n",
            "Epoch 25/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8839 - loss: 0.2801 - val_accuracy: 0.5571 - val_loss: 1.0451\n",
            "Epoch 26/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8833 - loss: 0.2811 - val_accuracy: 0.5294 - val_loss: 1.1163\n",
            "Epoch 27/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8968 - loss: 0.2543 - val_accuracy: 0.5433 - val_loss: 1.1264\n",
            "Epoch 28/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9147 - loss: 0.2188 - val_accuracy: 0.5190 - val_loss: 1.1310\n",
            "Epoch 29/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9179 - loss: 0.2063 - val_accuracy: 0.5363 - val_loss: 1.1767\n",
            "Epoch 30/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9292 - loss: 0.2132 - val_accuracy: 0.5329 - val_loss: 1.2374\n",
            "Epoch 31/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9383 - loss: 0.1802 - val_accuracy: 0.5052 - val_loss: 1.2736\n",
            "Epoch 32/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9396 - loss: 0.1806 - val_accuracy: 0.5121 - val_loss: 1.2957\n",
            "Epoch 33/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9541 - loss: 0.1439 - val_accuracy: 0.5017 - val_loss: 1.3917\n",
            "Epoch 34/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9280 - loss: 0.1878 - val_accuracy: 0.5294 - val_loss: 1.3782\n",
            "Epoch 35/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9349 - loss: 0.1658 - val_accuracy: 0.5363 - val_loss: 1.3647\n",
            "Epoch 36/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9435 - loss: 0.1432 - val_accuracy: 0.5294 - val_loss: 1.4041\n",
            "Epoch 37/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9503 - loss: 0.1412 - val_accuracy: 0.5433 - val_loss: 1.4155\n",
            "Epoch 38/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9532 - loss: 0.1351 - val_accuracy: 0.5398 - val_loss: 1.4388\n",
            "Epoch 39/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9594 - loss: 0.1280 - val_accuracy: 0.5260 - val_loss: 1.4680\n",
            "Epoch 40/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9503 - loss: 0.1344 - val_accuracy: 0.5260 - val_loss: 1.5123\n",
            "Epoch 41/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9377 - loss: 0.1621 - val_accuracy: 0.5190 - val_loss: 1.4665\n",
            "Epoch 42/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9495 - loss: 0.1331 - val_accuracy: 0.5294 - val_loss: 1.5015\n",
            "Epoch 43/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9437 - loss: 0.1475 - val_accuracy: 0.5363 - val_loss: 1.5035\n",
            "Epoch 44/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9487 - loss: 0.1257 - val_accuracy: 0.5087 - val_loss: 1.5809\n",
            "Epoch 45/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9614 - loss: 0.1159 - val_accuracy: 0.5190 - val_loss: 1.5197\n",
            "Epoch 46/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9322 - loss: 0.1655 - val_accuracy: 0.5225 - val_loss: 1.4624\n",
            "Epoch 47/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9621 - loss: 0.1194 - val_accuracy: 0.5190 - val_loss: 1.5619\n",
            "Epoch 48/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9605 - loss: 0.1136 - val_accuracy: 0.5294 - val_loss: 1.6030\n",
            "Epoch 49/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9620 - loss: 0.1025 - val_accuracy: 0.5121 - val_loss: 1.6612\n",
            "Epoch 50/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.1199 - val_accuracy: 0.5052 - val_loss: 1.6656\n",
            "Fold 2 Validation Accuracy: 0.5052\n",
            "Fold 2 Test Accuracy: 0.5003\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Fold 2 ROC-AUC: 0.5101\n",
            "Fold 2 Precision: 0.5096\n",
            "Fold 2 Recall: 0.3975\n",
            "Fold 2 F1-Score: 0.4467\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Accuracy: 0.4948\n",
            "Variance of Validation Accuracy: 0.0001\n",
            "Average Test Accuracy: 0.4986\n",
            "Variance of Test Accuracy: 0.0000\n",
            "Average ROC-AUC: 0.4985\n",
            "Average Precision: 0.4996\n",
            "Average Recall: 0.4442\n",
            "Average F1-Score: 0.4684\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m8,320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,069\u001b[0m (219.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,069</span> (219.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,689\u001b[0m (73.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,689</span> (73.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m37,380\u001b[0m (146.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,380</span> (146.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the balanced dataset\n",
        "#df_embeddings = pd.read_csv(\"sdne_embeddings.csv\")\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    embeddings = []\n",
        "    labels = df['label'].tolist()\n",
        "\n",
        "    for emb in df['embedding']:\n",
        "        if isinstance(emb, str):\n",
        "            emb = ast.literal_eval(emb)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    X = np.array(embeddings)\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "X, y = extract_features_labels(df_graph_embeddings)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets (60% train, 20% validation, 20% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Load the trained model\n",
        "model = keras.models.load_model(\"node_classification_model_final.h5\")\n",
        "\n",
        "# Integrated Gradients function\n",
        "def integrated_gradients(inputs, model, baseline=None, steps=50):\n",
        "    if baseline is None:\n",
        "        baseline = np.zeros(inputs.shape)\n",
        "\n",
        "    # Scale inputs and compute gradients\n",
        "    scaled_inputs = np.array([baseline + (float(i) / steps) * (inputs - baseline) for i in range(steps + 1)], dtype=np.float32)\n",
        "    grads = np.zeros_like(inputs, dtype=np.float32)\n",
        "\n",
        "    for i, input_batch in enumerate(scaled_inputs):\n",
        "        input_batch_tensor = tf.convert_to_tensor(input_batch, dtype=tf.float32)\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(input_batch_tensor)\n",
        "            predictions = model(input_batch_tensor)\n",
        "            loss = tf.reduce_sum(predictions, axis=0)\n",
        "        grads += tape.gradient(loss, input_batch_tensor).numpy()\n",
        "\n",
        "    integrated_grads = (inputs - baseline) * grads / steps\n",
        "    return integrated_grads\n",
        "\n",
        "# Example usage of Integrated Gradients\n",
        "index_to_explain = 0  # Choose an index to explain\n",
        "embedding_to_explain = X_test[index_to_explain:index_to_explain+1]\n",
        "\n",
        "integrated_grads_result = integrated_gradients(embedding_to_explain, model)\n",
        "\n",
        "# Visualize the Integrated Gradients\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(integrated_grads_result[0])), integrated_grads_result[0], color='b')\n",
        "plt.xlabel(\"Node Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Integrated Gradients - Node Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "# Save Integrated Gradients to a file\n",
        "np.savetxt(\"integrated_gradients.csv\", integrated_grads_result, delimiter=\",\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "wZHLfs8AOpHp",
        "outputId": "a4984e1b-8781-4202-8342-fea96a033c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HOPE"
      ],
      "metadata": {
        "id": "oNeNbrv3xjgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "def create_networkx_graph(edge_index, edge_attr, x, y, num_nodes):\n",
        "    # Create an empty NetworkX graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with features\n",
        "    for i in range(num_nodes):\n",
        "        node_features = {f\"feat_{j}\": x[i, j].item() for j in range(x.shape[1])}\n",
        "        G.add_node(i, **node_features)\n",
        "\n",
        "    # Add edges with attributes\n",
        "    for j in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[:, j].tolist()\n",
        "        edge_attributes = {f\"attr_{k}\": edge_attr[j, k].item() for k in range(edge_attr.shape[1])}\n",
        "        G.add_edge(src, dst, **edge_attributes)\n",
        "\n",
        "    return G, y.item()\n",
        "\n",
        "def balance_dataset(all_data, seed=42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    label_0_indices = [i for i, data in enumerate(all_data) if data.y.item() == 0]\n",
        "    label_1_indices = [i for i, data in enumerate(all_data) if data.y.item() == 1]\n",
        "\n",
        "    min_size_dataset = min(len(label_0_indices), len(label_1_indices))\n",
        "\n",
        "    random.shuffle(label_0_indices)\n",
        "    random.shuffle(label_1_indices)\n",
        "\n",
        "    label_0_indices = label_0_indices[:min_size_dataset]\n",
        "    label_1_indices = label_1_indices[:min_size_dataset]\n",
        "    print(len(label_0_indices))\n",
        "    print(len(label_1_indices))\n",
        "\n",
        "    balanced_indices = label_0_indices + label_1_indices\n",
        "    random.shuffle(balanced_indices)\n",
        "\n",
        "    balanced_data = [all_data[i] for i in balanced_indices]\n",
        "\n",
        "    return balanced_data\n",
        "\n",
        "def compute_hope_embedding(G, d=128, beta=0.01):\n",
        "    # Create adjacency matrix\n",
        "    A = nx.to_numpy_array(G)\n",
        "\n",
        "    # Compute the Katz similarity matrix\n",
        "    I = np.eye(A.shape[0])\n",
        "    S = np.linalg.inv(I - beta * A) - I\n",
        "\n",
        "    # Ensure k is valid for SVD\n",
        "    k = min(d // 2, min(S.shape) - 1)\n",
        "\n",
        "    if k <= 0:\n",
        "        # If k is invalid, return trivial embeddings\n",
        "        return np.zeros((S.shape[0], d))\n",
        "\n",
        "    # Compute SVD\n",
        "    U, s, Vt = svds(S, k=k)\n",
        "    S_sqrt = np.diag(np.sqrt(s))\n",
        "    X1 = np.dot(U, S_sqrt)\n",
        "    X2 = np.dot(Vt.T, S_sqrt)\n",
        "    X = np.concatenate((X1, X2), axis=1)\n",
        "\n",
        "    # Ensure the final embedding dimension matches d\n",
        "    if X.shape[1] < d:\n",
        "        X = np.pad(X, ((0, 0), (0, d - X.shape[1])), 'constant')\n",
        "    elif X.shape[1] > d:\n",
        "        X = X[:, :d]\n",
        "\n",
        "    return X\n",
        "\n",
        "# Generate and save embeddings for the balanced datasets\n",
        "if __name__ == \"__main__\":\n",
        "    all_graph_embeddings = []\n",
        "\n",
        "    for i in range(1):  # Loop to generate embeddings from 5 different balanced datasets\n",
        "        print(f'Generating embeddings for dataset {i + 1}')\n",
        "        balanced_data = balance_dataset(all_data)\n",
        "\n",
        "        for data in balanced_data:\n",
        "            num_nodes = data.num_nodes\n",
        "            edge_index = data.edge_index\n",
        "            edge_attr = data.edge_attr\n",
        "            x = data.x\n",
        "            y = data.y\n",
        "\n",
        "            # Create graph\n",
        "            G, y_label = create_networkx_graph(edge_index, edge_attr, x, y, num_nodes)\n",
        "\n",
        "            # Compute HOPE embeddings\n",
        "            embeddings = compute_hope_embedding(G, d=128, beta=0.01)\n",
        "\n",
        "            # Collect node embeddings and aggregate them to obtain a graph-level embedding\n",
        "            graph_embedding = np.mean(embeddings, axis=0)  # Averaging node embeddings\n",
        "            all_graph_embeddings.append((graph_embedding.tolist(), y_label))\n",
        "\n",
        "    # Convert graph-level embeddings to DataFrame\n",
        "    df_graph_embeddings = pd.DataFrame(all_graph_embeddings, columns=[\"embedding\", \"label\"])\n",
        "\n",
        "    # Save the DataFrame to a CSV file (optional)\n",
        "    df_graph_embeddings.to_csv(\"graph_embeddings.csv\", index=False)\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(df_graph_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEJnUWgaxksk",
        "outputId": "c8be0670-70fc-4e17-da91-e7b732daddfb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for dataset 1\n",
            "1443\n",
            "1443\n",
            "                                              embedding  label\n",
            "0     [1.002956492720546e-24, 4.714929491552258e-24,...      1\n",
            "1     [-2.1231698688769175e-10, 2.1902841275780873e-...      0\n",
            "2     [2.525745618875066e-11, -0.0002107770586235419...      0\n",
            "3     [3.4131242588287545e-11, -0.000992377263743873...      0\n",
            "4     [-7.874016842597724e-11, -0.000245918838967187...      1\n",
            "...                                                 ...    ...\n",
            "2881  [5.416063534573396e-26, 2.714186384939622e-25,...      0\n",
            "2882  [-5.460148667158279e-11, -0.000262492124382778...      0\n",
            "2883  [7.931097886001548e-24, -0.0001840439135499003...      0\n",
            "2884  [9.947703184636709e-11, 0.0004371147622382399,...      1\n",
            "2885  [-0.0002665343343851301, -3.206927317328855e-0...      1\n",
            "\n",
            "[2886 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import ast\n",
        "\n",
        "# Load the balanced dataset\n",
        "# df_graph_embeddings = pd.read_csv(\"graph_embeddings.csv\")\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    embeddings = []\n",
        "    labels = df['label'].tolist()\n",
        "\n",
        "    for emb in df['embedding']:\n",
        "        if isinstance(emb, str):\n",
        "            # Convert string representation of list back to list\n",
        "            emb = ast.literal_eval(emb)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    X = np.array(embeddings)\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "# Assuming df_graph_embeddings is already defined and loaded\n",
        "X, y = extract_features_labels(df_graph_embeddings)\n",
        "\n",
        "# Check if data is loaded correctly\n",
        "if X.shape[0] == 0 or X.shape[1] == 0:\n",
        "    raise ValueError(\"No valid embeddings found. Please check the DataFrame for formatting issues.\")\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Perform 5-fold cross-validation on the entire dataset\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "histories = []\n",
        "\n",
        "roc_aucs = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f'Fold {fold_no}')\n",
        "\n",
        "    X_train_val, X_test = X[train_index], X[test_index]\n",
        "    y_train_val, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Perform an additional split to get a validation set from the training+validation set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val)\n",
        "\n",
        "    # Create and train the neural network\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = create_model(input_dim)\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f'Fold {fold_no} Validation Accuracy: {val_accuracy:.4f}')\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    histories.append(history.history)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Fold {fold_no} Test Accuracy: {test_accuracy:.4f}')\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred_proba = model.predict(X_test).flatten()\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    # Compute and store metrics\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    roc_aucs.append(roc_auc)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Plot confusion matrix for each fold\n",
        "    plot_confusion_matrix(y_test, y_test_pred, class_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    print(f'Fold {fold_no} ROC AUC: {roc_auc:.4f}')\n",
        "    print(f'Fold {fold_no} Precision: {precision:.4f}')\n",
        "    print(f'Fold {fold_no} Recall: {recall:.4f}')\n",
        "    print(f'Fold {fold_no} F1-Score: {f1:.4f}')\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "# Calculate and print the average metrics\n",
        "average_val_accuracy = np.mean(val_accuracies)\n",
        "variance_val_accuracy = np.var(val_accuracies)\n",
        "print(f'Average Validation Accuracy: {average_val_accuracy:.4f}')\n",
        "print(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}')\n",
        "\n",
        "average_test_accuracy = np.mean(test_accuracies)\n",
        "variance_test_accuracy = np.var(test_accuracies)\n",
        "print(f'Average Test Accuracy: {average_test_accuracy:.4f}')\n",
        "print(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}')\n",
        "\n",
        "average_roc_auc = np.mean(roc_aucs)\n",
        "average_precision = np.mean(precisions)\n",
        "average_recall = np.mean(recalls)\n",
        "average_f1 = np.mean(f1_scores)\n",
        "\n",
        "print(f'Average ROC AUC: {average_roc_auc:.4f}')\n",
        "print(f'Average Precision: {average_precision:.4f}')\n",
        "print(f'Average Recall: {average_recall:.4f}')\n",
        "print(f'Average F1-Score: {average_f1:.4f}')\n",
        "\n",
        "# Save the average metrics to a file\n",
        "with open(\"model_accuracies.txt\", \"w\") as f:\n",
        "    f.write(f'Average Validation Accuracy: {average_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Average Test Accuracy: {average_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Average ROC AUC: {average_roc_auc:.4f}\\n')\n",
        "    f.write(f'Average Precision: {average_precision:.4f}\\n')\n",
        "    f.write(f'Average Recall: {average_recall:.4f}\\n')\n",
        "    f.write(f'Average F1-Score: {average_f1:.4f}\\n')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Save training history for each fold (optional)\n",
        "with open(\"training_histories.txt\", \"w\") as f:\n",
        "    for i, history in enumerate(histories):\n",
        "        f.write(f'\\nFold {i + 1} History:\\n')\n",
        "        f.write(str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B6Zb2A9Q1QJB",
        "outputId": "b0c63889-8e16-4bab-f114-46a4e8528bb1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.4828 - loss: 0.6934 - val_accuracy: 0.5087 - val_loss: 0.6930\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5133 - loss: 0.6928 - val_accuracy: 0.5017 - val_loss: 0.6932\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5390 - loss: 0.6931 - val_accuracy: 0.5087 - val_loss: 0.6929\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5121 - loss: 0.6928 - val_accuracy: 0.5121 - val_loss: 0.6928\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5039 - loss: 0.6929 - val_accuracy: 0.4844 - val_loss: 0.6931\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5166 - loss: 0.6921 - val_accuracy: 0.5260 - val_loss: 0.6927\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5070 - loss: 0.6923 - val_accuracy: 0.5225 - val_loss: 0.6928\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5084 - loss: 0.6911 - val_accuracy: 0.5294 - val_loss: 0.6929\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5474 - loss: 0.6897 - val_accuracy: 0.4706 - val_loss: 0.6932\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5243 - loss: 0.6918 - val_accuracy: 0.5225 - val_loss: 0.6930\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5583 - loss: 0.6854 - val_accuracy: 0.5017 - val_loss: 0.6933\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5573 - loss: 0.6831 - val_accuracy: 0.4879 - val_loss: 0.7083\n",
            "Epoch 13/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5644 - loss: 0.6755 - val_accuracy: 0.5156 - val_loss: 0.6950\n",
            "Epoch 14/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5876 - loss: 0.6727 - val_accuracy: 0.4948 - val_loss: 0.6969\n",
            "Epoch 15/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6071 - loss: 0.6703 - val_accuracy: 0.4913 - val_loss: 0.7199\n",
            "Epoch 16/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5394 - loss: 0.6925 - val_accuracy: 0.4810 - val_loss: 0.6988\n",
            "Epoch 17/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5804 - loss: 0.6727 - val_accuracy: 0.4810 - val_loss: 0.7017\n",
            "Epoch 18/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6229 - loss: 0.6675 - val_accuracy: 0.5087 - val_loss: 0.7041\n",
            "Epoch 19/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5978 - loss: 0.6615 - val_accuracy: 0.5121 - val_loss: 0.7034\n",
            "Epoch 20/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6145 - loss: 0.6606 - val_accuracy: 0.5329 - val_loss: 0.6998\n",
            "Epoch 21/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6098 - loss: 0.6596 - val_accuracy: 0.5156 - val_loss: 0.7078\n",
            "Epoch 22/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6443 - loss: 0.6434 - val_accuracy: 0.5190 - val_loss: 0.7092\n",
            "Epoch 23/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6734 - loss: 0.6240 - val_accuracy: 0.5260 - val_loss: 0.7100\n",
            "Epoch 24/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6438 - loss: 0.6411 - val_accuracy: 0.5329 - val_loss: 0.7142\n",
            "Epoch 25/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6254 - loss: 0.6520 - val_accuracy: 0.5606 - val_loss: 0.6994\n",
            "Epoch 26/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6848 - loss: 0.6181 - val_accuracy: 0.5640 - val_loss: 0.7011\n",
            "Epoch 27/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6653 - loss: 0.6200 - val_accuracy: 0.5606 - val_loss: 0.7066\n",
            "Epoch 28/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6131 - loss: 0.6496 - val_accuracy: 0.4983 - val_loss: 0.7195\n",
            "Epoch 29/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6263 - loss: 0.6460 - val_accuracy: 0.5675 - val_loss: 0.7039\n",
            "Epoch 30/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6482 - loss: 0.6200 - val_accuracy: 0.5709 - val_loss: 0.7083\n",
            "Epoch 31/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7085 - loss: 0.5999 - val_accuracy: 0.5675 - val_loss: 0.7141\n",
            "Epoch 32/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6978 - loss: 0.6051 - val_accuracy: 0.5675 - val_loss: 0.7088\n",
            "Epoch 33/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6894 - loss: 0.6051 - val_accuracy: 0.5260 - val_loss: 0.7140\n",
            "Epoch 34/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6656 - loss: 0.6132 - val_accuracy: 0.5744 - val_loss: 0.7060\n",
            "Epoch 35/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6647 - loss: 0.6112 - val_accuracy: 0.5640 - val_loss: 0.7138\n",
            "Epoch 36/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6944 - loss: 0.5963 - val_accuracy: 0.5709 - val_loss: 0.7156\n",
            "Epoch 37/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6715 - loss: 0.6165 - val_accuracy: 0.5744 - val_loss: 0.7322\n",
            "Epoch 38/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7064 - loss: 0.5835 - val_accuracy: 0.5363 - val_loss: 0.7341\n",
            "Epoch 39/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6913 - loss: 0.5996 - val_accuracy: 0.5813 - val_loss: 0.7184\n",
            "Epoch 40/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7011 - loss: 0.5770 - val_accuracy: 0.5640 - val_loss: 0.7193\n",
            "Epoch 41/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6838 - loss: 0.5835 - val_accuracy: 0.5813 - val_loss: 0.7092\n",
            "Epoch 42/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6835 - loss: 0.5966 - val_accuracy: 0.5848 - val_loss: 0.7222\n",
            "Epoch 43/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6988 - loss: 0.5942 - val_accuracy: 0.5917 - val_loss: 0.7189\n",
            "Epoch 44/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6489 - loss: 0.6157 - val_accuracy: 0.5640 - val_loss: 0.7175\n",
            "Epoch 45/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6938 - loss: 0.5942 - val_accuracy: 0.5882 - val_loss: 0.7151\n",
            "Epoch 46/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7037 - loss: 0.5909 - val_accuracy: 0.5571 - val_loss: 0.7288\n",
            "Epoch 47/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6837 - loss: 0.5929 - val_accuracy: 0.5709 - val_loss: 0.7198\n",
            "Epoch 48/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7240 - loss: 0.5509 - val_accuracy: 0.5779 - val_loss: 0.7216\n",
            "Epoch 49/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7123 - loss: 0.5587 - val_accuracy: 0.5779 - val_loss: 0.7156\n",
            "Epoch 50/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7059 - loss: 0.5648 - val_accuracy: 0.5606 - val_loss: 0.7197\n",
            "Fold 1 Validation Accuracy: 0.5606\n",
            "Fold 1 Test Accuracy: 0.5710\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 ROC AUC: 0.6109\n",
            "Fold 1 Precision: 0.5635\n",
            "Fold 1 Recall: 0.5738\n",
            "Fold 1 F1-Score: 0.5686\n",
            "Fold 2\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4903 - loss: 0.6933 - val_accuracy: 0.5087 - val_loss: 0.6929\n",
            "Epoch 2/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5525 - loss: 0.6919 - val_accuracy: 0.5087 - val_loss: 0.6929\n",
            "Epoch 3/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5311 - loss: 0.6915 - val_accuracy: 0.5087 - val_loss: 0.6928\n",
            "Epoch 4/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4981 - loss: 0.6927 - val_accuracy: 0.5087 - val_loss: 0.6925\n",
            "Epoch 5/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4974 - loss: 0.6932 - val_accuracy: 0.5087 - val_loss: 0.6924\n",
            "Epoch 6/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5080 - loss: 0.6929 - val_accuracy: 0.5087 - val_loss: 0.6921\n",
            "Epoch 7/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5073 - loss: 0.6915 - val_accuracy: 0.5087 - val_loss: 0.6921\n",
            "Epoch 8/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5311 - loss: 0.6910 - val_accuracy: 0.5225 - val_loss: 0.6915\n",
            "Epoch 9/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5535 - loss: 0.6911 - val_accuracy: 0.5294 - val_loss: 0.6905\n",
            "Epoch 10/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5804 - loss: 0.6864 - val_accuracy: 0.5225 - val_loss: 0.6902\n",
            "Epoch 11/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5734 - loss: 0.6868 - val_accuracy: 0.5363 - val_loss: 0.6900\n",
            "Epoch 12/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5427 - loss: 0.6861 - val_accuracy: 0.5433 - val_loss: 0.6882\n",
            "Epoch 13/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5873 - loss: 0.6817 - val_accuracy: 0.5121 - val_loss: 0.6978\n",
            "Epoch 14/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5896 - loss: 0.6791 - val_accuracy: 0.5225 - val_loss: 0.6925\n",
            "Epoch 15/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5707 - loss: 0.6771 - val_accuracy: 0.5260 - val_loss: 0.6872\n",
            "Epoch 16/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5817 - loss: 0.6712 - val_accuracy: 0.5260 - val_loss: 0.6904\n",
            "Epoch 17/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5927 - loss: 0.6670 - val_accuracy: 0.5260 - val_loss: 0.6871\n",
            "Epoch 18/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6086 - loss: 0.6502 - val_accuracy: 0.5329 - val_loss: 0.6944\n",
            "Epoch 19/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6284 - loss: 0.6546 - val_accuracy: 0.5467 - val_loss: 0.6950\n",
            "Epoch 20/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6136 - loss: 0.6477 - val_accuracy: 0.5363 - val_loss: 0.6900\n",
            "Epoch 21/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6640 - loss: 0.6341 - val_accuracy: 0.5467 - val_loss: 0.6901\n",
            "Epoch 22/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6692 - loss: 0.6316 - val_accuracy: 0.5398 - val_loss: 0.6908\n",
            "Epoch 23/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6372 - loss: 0.6317 - val_accuracy: 0.5467 - val_loss: 0.6920\n",
            "Epoch 24/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6659 - loss: 0.6203 - val_accuracy: 0.5329 - val_loss: 0.6941\n",
            "Epoch 25/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6627 - loss: 0.6144 - val_accuracy: 0.5190 - val_loss: 0.7024\n",
            "Epoch 26/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6430 - loss: 0.6479 - val_accuracy: 0.5502 - val_loss: 0.6988\n",
            "Epoch 27/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6371 - loss: 0.6294 - val_accuracy: 0.5744 - val_loss: 0.7095\n",
            "Epoch 28/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6735 - loss: 0.6118 - val_accuracy: 0.5329 - val_loss: 0.7003\n",
            "Epoch 29/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6639 - loss: 0.6133 - val_accuracy: 0.5398 - val_loss: 0.7040\n",
            "Epoch 30/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6541 - loss: 0.6204 - val_accuracy: 0.5779 - val_loss: 0.7138\n",
            "Epoch 31/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6509 - loss: 0.6304 - val_accuracy: 0.5363 - val_loss: 0.7056\n",
            "Epoch 32/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6643 - loss: 0.6214 - val_accuracy: 0.5606 - val_loss: 0.7053\n",
            "Epoch 33/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6961 - loss: 0.5860 - val_accuracy: 0.5294 - val_loss: 0.7106\n",
            "Epoch 34/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6993 - loss: 0.5927 - val_accuracy: 0.5294 - val_loss: 0.7278\n",
            "Epoch 35/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6791 - loss: 0.5947 - val_accuracy: 0.5779 - val_loss: 0.7039\n",
            "Epoch 36/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7117 - loss: 0.5855 - val_accuracy: 0.5813 - val_loss: 0.7110\n",
            "Epoch 37/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6720 - loss: 0.5959 - val_accuracy: 0.5709 - val_loss: 0.7131\n",
            "Epoch 38/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6953 - loss: 0.5943 - val_accuracy: 0.5744 - val_loss: 0.7180\n",
            "Epoch 39/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6943 - loss: 0.5849 - val_accuracy: 0.5744 - val_loss: 0.7149\n",
            "Epoch 40/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6872 - loss: 0.5702 - val_accuracy: 0.5744 - val_loss: 0.7182\n",
            "Epoch 41/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7153 - loss: 0.5574 - val_accuracy: 0.5779 - val_loss: 0.7217\n",
            "Epoch 42/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6927 - loss: 0.5716 - val_accuracy: 0.5779 - val_loss: 0.7190\n",
            "Epoch 43/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7267 - loss: 0.5606 - val_accuracy: 0.5744 - val_loss: 0.7164\n",
            "Epoch 44/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7072 - loss: 0.5653 - val_accuracy: 0.5709 - val_loss: 0.7156\n",
            "Epoch 45/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7263 - loss: 0.5634 - val_accuracy: 0.5813 - val_loss: 0.7210\n",
            "Epoch 46/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6976 - loss: 0.5681 - val_accuracy: 0.5675 - val_loss: 0.7305\n",
            "Epoch 47/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7099 - loss: 0.5621 - val_accuracy: 0.5917 - val_loss: 0.7357\n",
            "Epoch 48/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7096 - loss: 0.5710 - val_accuracy: 0.5813 - val_loss: 0.7382\n",
            "Epoch 49/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6841 - loss: 0.5839 - val_accuracy: 0.5744 - val_loss: 0.7337\n",
            "Epoch 50/50\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7173 - loss: 0.5611 - val_accuracy: 0.5606 - val_loss: 0.7355\n",
            "Fold 2 Validation Accuracy: 0.5606\n",
            "Fold 2 Test Accuracy: 0.5807\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxEAAAJwCAYAAAD2uOwtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZAklEQVR4nO3deXhN997+8XsnZCOjqCSmmIvUTKuhVTMRSukphyNpSw0ntBqlTVFTiRpqKAed0EFpKVU1BQdtTSlSMVRLadqHCA2JMSHZvz/87LN3E3ZWJdkJ79e51nXJd02fvc7z5OSz7/Vdy2SxWCwCAAAAgBxycXYBAAAAAAoXmggAAAAAhtBEAAAAADCEJgIAAACAITQRAAAAAAyhiQAAAABgCE0EAAAAAENoIgAAAAAYQhMBAAAAwBCaCADIxi+//KJ27drJ29tbJpNJq1atytXjnzx5UiaTSYsWLcrV4xZmLVq0UIsWLZxdBgAgB2giABRYx48f14ABA1SlShUVK1ZMXl5eatasmWbNmqWrV6/m6bnDw8MVHx+viRMn6uOPP1bjxo3z9Hz56dlnn5XJZJKXl1e21/GXX36RyWSSyWTStGnTDB//1KlTGjt2rOLi4nKhWgBAQVTE2QUAQHa++eYb/eMf/5DZbFZYWJhq166t9PR0fffddxo+fLgOHTqkd999N0/OffXqVe3cuVMjR47U4MGD8+QcFStW1NWrV1W0aNE8Ob4jRYoU0ZUrV/T111/rmWeesVv36aefqlixYrp27drfOvapU6c0btw4VapUSfXr18/xfhs3bvxb5wMA5D+aCAAFzokTJ9SzZ09VrFhRW7ZsUZkyZazrIiIidOzYMX3zzTd5dv6zZ89Kknx8fPLsHCaTScWKFcuz4ztiNpvVrFkzffbZZ1maiCVLlig0NFQrVqzIl1quXLmiEiVKyM3NLV/OBwC4e9zOBKDAmTJlii5duqQPPvjAroG4pVq1anrppZesP9+4cUMTJkxQ1apVZTabValSJb3++utKS0uz269SpUrq1KmTvvvuOz3yyCMqVqyYqlSpoo8++si6zdixY1WxYkVJ0vDhw2UymVSpUiVJN28DuvVvW2PHjpXJZLIbi4mJ0WOPPSYfHx95eHioRo0aev31163rbzcnYsuWLXr88cfl7u4uHx8fdenSRUeOHMn2fMeOHdOzzz4rHx8feXt767nnntOVK1duf2H/olevXlq3bp0uXLhgHYuNjdUvv/yiXr16Zdk+OTlZr7zyiurUqSMPDw95eXkpJCREP/74o3WbrVu36uGHH5YkPffcc9bbom59zhYtWqh27drau3evmjdvrhIlSlivy1/nRISHh6tYsWJZPn/79u1VsmRJnTp1KsefFQCQu2giABQ4X3/9tapUqaKmTZvmaPt+/frpjTfeUMOGDTVjxgw98cQTio6OVs+ePbNse+zYMT399NNq27atpk+frpIlS+rZZ5/VoUOHJEndunXTjBkzJEn//Oc/9fHHH2vmzJmG6j906JA6deqktLQ0jR8/XtOnT9eTTz6p77///o77bdq0Se3bt1dSUpLGjh2ryMhI7dixQ82aNdPJkyezbP/MM8/o4sWLio6O1jPPPKNFixZp3LhxOa6zW7duMplM+vLLL61jS5YsUc2aNdWwYcMs2//6669atWqVOnXqpLffflvDhw9XfHy8nnjiCesf9LVq1dL48eMlSf3799fHH3+sjz/+WM2bN7ce588//1RISIjq16+vmTNnqmXLltnWN2vWLJUuXVrh4eHKyMiQJC1YsEAbN27UO++8o7Jly+b4swIAcpkFAAqQlJQUiyRLly5dcrR9XFycRZKlX79+duOvvPKKRZJly5Yt1rGKFStaJFm2b99uHUtKSrKYzWbLsGHDrGMnTpywSLJMnTrV7pjh4eGWihUrZqlhzJgxFttfpzNmzLBIspw9e/a2dd86x8KFC61j9evXt/j5+Vn+/PNP69iPP/5ocXFxsYSFhWU53/PPP293zKeeespSqlSp257T9nO4u7tbLBaL5emnn7a0bt3aYrFYLBkZGZaAgADLuHHjsr0G165ds2RkZGT5HGaz2TJ+/HjrWGxsbJbPdssTTzxhkWSZP39+tuueeOIJu7ENGzZYJFnefPNNy6+//mrx8PCwdO3a1eFnBADkLZIIAAVKamqqJMnT0zNH269du1aSFBkZaTc+bNgwScoydyIoKEiPP/649efSpUurRo0a+vXXX/92zX91ay7FV199pczMzBztc/r0acXFxenZZ5+Vr6+vdbxu3bpq27at9XPaGjhwoN3Pjz/+uP7880/rNcyJXr16aevWrUpMTNSWLVuUmJiY7a1M0s15FC4uN/9nIyMjQ3/++af1Vq19+/bl+Jxms1nPPfdcjrZt166dBgwYoPHjx6tbt24qVqyYFixYkONzAQDyBk0EgALFy8tLknTx4sUcbf/bb7/JxcVF1apVsxsPCAiQj4+PfvvtN7vxwMDALMcoWbKkzp8//zcrzqpHjx5q1qyZ+vXrJ39/f/Xs2VOff/75HRuKW3XWqFEjy7patWrp3Llzunz5st34Xz9LyZIlJcnQZ+nYsaM8PT21bNkyffrpp3r44YezXMtbMjMzNWPGDFWvXl1ms1kPPPCASpcurQMHDiglJSXH5yxXrpyhSdTTpk2Tr6+v4uLiNHv2bPn5+eV4XwBA3qCJAFCgeHl5qWzZsjp48KCh/f46sfl2XF1dsx23WCx/+xy37te/pXjx4tq+fbs2bdqkPn366MCBA+rRo4fatm2bZdu7cTef5Raz2axu3bpp8eLFWrly5W1TCEmaNGmSIiMj1bx5c33yySfasGGDYmJi9NBDD+U4cZFuXh8j9u/fr6SkJElSfHy8oX0BAHmDJgJAgdOpUycdP35cO3fudLhtxYoVlZmZqV9++cVu/MyZM7pw4YL1SUu5oWTJknZPMrrlr2mHJLm4uKh169Z6++23dfjwYU2cOFFbtmzRf//732yPfavOo0ePZln3008/6YEHHpC7u/vdfYDb6NWrl/bv36+LFy9mOxn9luXLl6tly5b64IMP1LNnT7Vr105t2rTJck1y2tDlxOXLl/Xcc88pKChI/fv315QpUxQbG5trxwcA/D00EQAKnBEjRsjd3V39+vXTmTNnsqw/fvy4Zs2aJenm7TiSsjxB6e2335YkhYaG5lpdVatWVUpKig4cOGAdO336tFauXGm3XXJycpZ9b7107a+Pnb2lTJkyql+/vhYvXmz3R/nBgwe1ceNG6+fMCy1bttSECRM0Z84cBQQE3HY7V1fXLCnHF198of/7v/+zG7vV7GTXcBn16quvKiEhQYsXL9bbb7+tSpUqKTw8/LbXEQCQP3jZHIACp2rVqlqyZIl69OihWrVq2b2xeseOHfriiy/07LPPSpLq1aun8PBwvfvuu7pw4YKeeOIJ7dmzR4sXL1bXrl1v+/jQv6Nnz5569dVX9dRTT+nFF1/UlStXNG/ePD344IN2E4vHjx+v7du3KzQ0VBUrVlRSUpL+85//qHz58nrsscdue/ypU6cqJCREwcHB6tu3r65evap33nlH3t7eGjt2bK59jr9ycXHRqFGjHG7XqVMnjR8/Xs8995yaNm2q+Ph4ffrpp6pSpYrddlWrVpWPj4/mz58vT09Pubu7q0mTJqpcubKhurZs2aL//Oc/GjNmjPWRswsXLlSLFi00evRoTZkyxdDxAAC5hyQCQIH05JNP6sCBA3r66af11VdfKSIiQq+99ppOnjyp6dOna/bs2dZt33//fY0bN06xsbEaOnSotmzZoqioKC1dujRXaypVqpRWrlypEiVKaMSIEVq8eLGio6PVuXPnLLUHBgbqww8/VEREhObOnavmzZtry5Yt8vb2vu3x27Rpo/Xr16tUqVJ64403NG3aND366KP6/vvvDf8Bnhdef/11DRs2TBs2bNBLL72kffv26ZtvvlGFChXstitatKgWL14sV1dXDRw4UP/85z+1bds2Q+e6ePGinn/+eTVo0EAjR460jj/++ON66aWXNH36dO3atStXPhcAwDiTxcgMPAAAAAD3PZIIAAAAAIbQRAAAAAAwhCYCAAAAgCE0EQAAAAAMoYkAAAAAYAhNBAAAAABDaCIAAACAQmby5MkymUwaOnRolnUWi0UhISEymUxatWqV3bqEhASFhoaqRIkS8vPz0/Dhw3Xjxg3D578n31hdvOUEZ5cAALnqfMxoZ5cAALmqWAH+K7R4g8H5dq6r++cY3ic2NlYLFixQ3bp1s10/c+ZMmUymLOMZGRkKDQ1VQECAduzYodOnTyssLExFixbVpEmTDNVAEgEAAAAUEpcuXVLv3r313nvvqWTJklnWx8XFafr06frwww+zrNu4caMOHz6sTz75RPXr11dISIgmTJiguXPnKj093VAdNBEAAACALZNLvi1paWlKTU21W9LS0m5bWkREhEJDQ9WmTZss665cuaJevXpp7ty5CggIyLJ+586dqlOnjvz9/a1j7du3V2pqqg4dOmToEtFEAAAAAE4SHR0tb29vuyU6OjrbbZcuXap9+/bddv3LL7+spk2bqkuXLtmuT0xMtGsgJFl/TkxMNFR3Ab4bDQAAAHCCbOYT5JWoqChFRkbajZnN5izb/f7773rppZcUExOjYsWKZVm/evVqbdmyRfv378+zWm2RRAAAAABOYjab5eXlZbdk10Ts3btXSUlJatiwoYoUKaIiRYpo27Ztmj17tooUKaKYmBgdP35cPj4+1vWS1L17d7Vo0UKSFBAQoDNnztgd99bP2d3+dCckEQAAAIAtU8H7nr1169aKj4+3G3vuuedUs2ZNvfrqq3rggQc0YMAAu/V16tTRjBkz1LlzZ0lScHCwJk6cqKSkJPn5+UmSYmJi5OXlpaCgIEP10EQAAAAABZynp6dq165tN+bu7q5SpUpZx7NLEwIDA1W5cmVJUrt27RQUFKQ+ffpoypQpSkxM1KhRoxQREZFt+nEnBa/NAgAAAJzJZMq/JR+5urpqzZo1cnV1VXBwsP71r38pLCxM48ePN3wskggAAACgENq6desd11sslixjFStW1Nq1a+/63DQRAAAAgK0COCeioOEKAQAAADCEJAIAAACwlc9zFQojkggAAAAAhpBEAAAAALaYE+EQVwgAAACAITQRAAAAAAzhdiYAAADAFhOrHSKJAAAAAGAISQQAAABgi4nVDnGFAAAAABhCEgEAAADYYk6EQyQRAAAAAAwhiQAAAABsMSfCIa4QAAAAAENIIgAAAABbzIlwiCQCAAAAgCEkEQAAAIAt5kQ4xBUCAAAAYAhJBAAAAGCLJMIhrhAAAAAAQ0giAAAAAFsuPJ3JEZIIAAAAAIaQRAAAAAC2mBPhEFcIAAAAgCE0EQAAAAAM4XYmAAAAwJaJidWOkEQAAAAAMIQkAgAAALDFxGqHuEIAAAAADCGJAAAAAGwxJ8IhkggAAAAAhpBEAAAAALaYE+EQVwgAAACAISQRAAAAgC3mRDhEEgEAAADAEJIIAAAAwBZzIhziCgEAAAAwhCQCAAAAsMWcCIdIIgAAAAAYQhIBAAAA2GJOhENcIQAAAACGkEQAAAAAtpgT4RBJBAAAAABDSCIAAAAAW8yJcIgrBAAAAMAQmggAAAAAhnA7EwAAAGCL25kc4goBAAAAMIQkAgAAALDFI14dIokAAAAAYAhJBAAAAGCLOREOcYUAAAAAGEISAQAAANhiToRDJBEAAAAADCGJAAAAAGwxJ8IhrhAAAABQyEyePFkmk0lDhw6VJCUnJ2vIkCGqUaOGihcvrsDAQL344otKSUmx2y8hIUGhoaEqUaKE/Pz8NHz4cN24ccPw+UkiAAAAAFsFfE5EbGysFixYoLp161rHTp06pVOnTmnatGkKCgrSb7/9poEDB+rUqVNavny5JCkjI0OhoaEKCAjQjh07dPr0aYWFhalo0aKaNGmSoRpIIgAAAIBC4tKlS+rdu7fee+89lSxZ0jpeu3ZtrVixQp07d1bVqlXVqlUrTZw4UV9//bU1adi4caMOHz6sTz75RPXr11dISIgmTJiguXPnKj093VAdNBEAAACADZPJlG9LWlqaUlNT7Za0tLTb1hYREaHQ0FC1adPG4edISUmRl5eXihS5efPRzp07VadOHfn7+1u3ad++vVJTU3Xo0CFD14gmAgAAAHCS6OhoeXt72y3R0dHZbrt06VLt27fvtuttnTt3ThMmTFD//v2tY4mJiXYNhCTrz4mJiYbqZk4EAAAAYMOUj3MioqKiFBkZaTdmNpuzbPf777/rpZdeUkxMjIoVK3bHY6ampio0NFRBQUEaO3ZsbpZrRRMBAAAAOInZbM62afirvXv3KikpSQ0bNrSOZWRkaPv27ZozZ47S0tLk6uqqixcvqkOHDvL09NTKlStVtGhR6/YBAQHas2eP3XHPnDljXWcEtzMBAAAAtkz5uORQ69atFR8fr7i4OOvSuHFj9e7dW3FxcXJ1dVVqaqratWsnNzc3rV69OktiERwcrPj4eCUlJVnHYmJi5OXlpaCgIEOXiCQCAAAAKOA8PT1Vu3ZtuzF3d3eVKlVKtWvXtjYQV65c0SeffGKdpC1JpUuXlqurq9q1a6egoCD16dNHU6ZMUWJiokaNGqWIiIgcpSG2aCIAAACAQm7fvn3avXu3JKlatWp2606cOKFKlSrJ1dVVa9as0aBBgxQcHCx3d3eFh4dr/Pjxhs9HEwEAAADYyM+J1Xdj69at1n+3aNFCFovF4T4VK1bU2rVr7/rczIkAAAAAYAhJBAAAAGCjsCQRzkQSAQAAAMAQkggAAADABkmEYyQRAAAAAAwhiQAAAABskEQ4RhIBAAAAwBCSCAAAAMAWQYRDJBEAAAAADCGJAAAAAGwwJ8IxkggAAAAAhpBEAAAAADZIIhwjiQAAAABgCEkEAAAAYIMkwjGSCAAAAACGkEQAAAAANkgiHCOJAAAAAGAISQQAAABgiyDCIZIIAAAAAIbQRAAAAAAwhNuZAAAAABtMrHaMJAIAAACAISQRAAAAgA2SCMdIIgAAAAAYQhIBAAAA2CCJcIwkAgAAAIAhJBEAAACALYIIh0giAAAAABhCEgEAAADYYE6EYyQRAAAAAAwhiQAAAABskEQ4RhIBAAAAwBCSCAAAAMAGSYRjJBEAAAAADCGJAAAAAGyQRDjm1CYiPT1dq1at0s6dO5WYmChJCggIUNOmTdWlSxe5ubk5szwAAAAA2XDa7UzHjh1TrVq1FB4erv379yszM1OZmZnav3+/wsLC9NBDD+nYsWPOKg8AAAD3K1M+LoWU05KIQYMGqU6dOtq/f7+8vLzs1qWmpiosLEwRERHasGGDkyoEAAAAkB2nNRHff/+99uzZk6WBkCQvLy9NmDBBTZo0cUJlAAAAAO7Eabcz+fj46OTJk7ddf/LkSfn4+ORbPQAAAIB0c2J1fi2FldOSiH79+iksLEyjR49W69at5e/vL0k6c+aMNm/erDfffFNDhgxxVnkAAAAAbsNpTcT48ePl7u6uqVOnatiwYdZOzGKxKCAgQK+++qpGjBjhrPIAAABwnyrMCUF+ceojXl999VW9+uqrOnHihN0jXitXruzMsgAAAADcQYF42VzlypVpHAAAAFAgkEQ45rSJ1QAAAAAKpwKRRAAAAAAFBkGEQyQRAAAAAAwhiQAAAABsMCfCMacnEevXr9d3331n/Xnu3LmqX7++evXqpfPnzzuxMgAAAADZcXoTMXz4cKWmpkqS4uPjNWzYMHXs2FEnTpxQZGSkk6sDAADA/YY3Vjvm9NuZTpw4oaCgIEnSihUr1KlTJ02aNEn79u1Tx44dnVwdAAAAgL9yehPh5uamK1euSJI2bdqksLAwSZKvr681oQAAAADyS2FOCPKL05uIxx57TJGRkWrWrJn27NmjZcuWSZJ+/vlnlS9f3snV4X73yj+bakL/1pqzfLeGz90oSXonsqNaNaysMg946tLVdO069IdGLdisn3//07rf9CHt9WjtCnqoUmn9lHBOj77wnrM+AoD73AfvLdDmmI06ceJXmYsVU/36DTQ08hVVqlzFus25s2f19vQp2rVjhy5fuaxKlSrrhf4D1aZde+s2KRcuaPKkCdq29b9ycXFR67bt9OprI1XC3d0ZHwuAkzl9TsScOXNUpEgRLV++XPPmzVO5cuUkSevWrVOHDh2cXB3uZ41qlFHfzg114PgZu/H9P59W/ylfq374PD05YolMMmnN1N5ycbH/1uKjdXFavvVwfpYMAFn8ELtHPf7ZWx9/9rkWvLdQN27c0MAX+lrvApCkka+/qpMnTmjWnHlasfJrtW7TVsOHDdWRI//7HRb16is6fuyY5r+/ULPnzte+H37Q+LFvOOMjAXmOORGOmSwWi8XZReS24i0nOLsEFHLuxYpq57sv6KWZ6/Ran8d04NgZaxLxV7Wr+Cn2gwEK6j1HJ07ZP1FsZHhzdX6sBkkE7tr5mNHOLgH3iOTkZLV8PFgfLv5EjRo/LEl6tHEDjXxjjDo/2dW6XfOmTTQ08hV1e/of+vX4cT31ZEctWbZcD9WuI0n6/tvtihjUXxu3bJOfn78zPgoKuWJOvx/m9ioP/SbfznViZmi+nSs3OT2J2Ldvn+Lj460/f/XVV+ratatef/11paenO7Ey3M9mDg3R+l2/6L/7TtxxuxLFiiqsQz2dOHVefySl5FN1APD3Xbp4UZLk5e1tHavXoIE2rF+nlAsXlJmZqXVrv1FaepoaP/yIJOnHH/fL08vL2kBIUpPgpnJxcVH8gQP5+wGA/GDKx6WQcnoTMWDAAP3888+SpF9//VU9e/ZUiRIl9MUXX2jEiBEO909LS1NqaqrdYsm8kddl4x72j5YPqX71Mhr93pbbbtO/SyOdXfuq/lz3mto1qabQ4Z/q+o3MfKwSAIzLzMzUlLcmqX6Dhqpe/UHr+NTpM3Xj+g01b9ZEDzeoozfHvaEZs+YosGJFSdKf587J19fX7lhFihSRl7e3/jx3Nl8/A4CbJk+eLJPJpKFDh1rHrl27poiICJUqVUoeHh7q3r27zpyxvy07ISFBoaGhKlGihPz8/DR8+HDduGH8b2enNxE///yz6tevL0n64osv1Lx5cy1ZskSLFi3SihUrHO4fHR0tb29vu+XGb9vzuGrcq8qX9tLUwe303MSVSruecdvtlm46qEdfeE9tXlqsX35P1idjustc1DUfKwUA4ya9OU7Hf/lFU6bNsBuf+84sXbyYqnc/WKQly1aoT/hzGjFsqH75+aiTKgWcq6DPiYiNjdWCBQtUt25du/GXX35ZX3/9tb744gtt27ZNp06dUrdu3azrMzIyFBoaqvT0dO3YsUOLFy/WokWL9MYbxuc3Ob2JsFgsysy8+Q3upk2brO+GqFChgs6dO+dw/6ioKKWkpNgtRSo2z9Oace9q8GAZ+ft6aOe7L+jippG6uGmkmtevpH93e0QXN420Tp5OvZym4/+XrO8PJKjX2C9Uo0IpdXm8ppOrB4Dbm/TmeG3ftlXvLVws/4AA6/jvCQlauuQTjXtzkpo8GqwaNWtq4L8HK+ih2lr62aeSpFIPPKDk5GS74924cUOpKSkq9UDpfP0cwP3u0qVL6t27t9577z2VLFnSOp6SkqIPPvhAb7/9tlq1aqVGjRpp4cKF2rFjh3bt2iVJ2rhxow4fPqxPPvlE9evXV0hIiCZMmKC5c+cankbg9CaicePGevPNN/Xxxx9r27ZtCg29ObnkxIkT8vd3PFHLbDbLy8vLbjG5FOCZOijQ/rvvhBo9N19N+r1rXfb+dEpLN8WrSb93lZmZ9TkEt75JcCOJAFAAWSwWTXpzvLZsjtF7Hy5W+fIV7NZfu3ZVkuRisv+TwMXFVZb//zuvXr0GupiaqsOHDlrX79m9S5mZmarzl29CARiT3a35aWlpt90+IiJCoaGhatOmjd343r17df36dbvxmjVrKjAwUDt37pQk7dy5U3Xq1LH7G7t9+/ZKTU3VoUOHDNXt9CZi5syZ2rdvnwYPHqyRI0eqWrVqkqTly5eradOmTq4O95tLV9N1+ORZu+XytXQlp17V4ZNnVamMj17p1UwNHgxQBT8vPfpQeX065mldTbuuDbuPWY9TpWxJ1a3qL39fDxV3K6q6Vf1Vt6q/ihZx+v/LAbjPTJowTmvXrNbkKdPlXsJd586e1bmzZ3Xt2jVJUqXKVRQYWFETxr2h+AMH9HtCghYv+lC7dn6vlq1v/jFSpWpVNXvscY0bM1rxBw5o/769ip44QR1CQnkyE+5J+Xk7U3a35kdHR2db19KlS7Vv375s1ycmJsrNzU0+Pj524/7+/kpMTLRu89cv6W/9fGubnHL6V/Z169a1ezrTLVOnTpWrK9/somBJS7+hZnUqaHD3R1TSs7iSzl/SdwcS1HLIIp298L9nrs8b3knN61ey/rz7/f6SpBo9ZyvhDE9xApB/Pl/2mSSp77N97MbHvxmtLk91U9GiRTVn/rua9fZ0vTh4oK5cuaLACoGaMGmyHm/+hHX76LemKXriBPXvG2592dxrUaPy9bMA96KoqChFRkbajZnN5izb/f7773rppZcUExOjYsWK5Vd5t+X0JuJ2CsLFASSp/csfW/99+s9LeipqqaF9AMCZfjzkeHJ0xYqV9Pasd+64jbePjyZPnZ5bZQEFWn6+A85sNmfbNPzV3r17lZSUpIYNG1rHMjIytH37ds2ZM0cbNmxQenq6Lly4YJdGnDlzRgH/fx5UQECA9uzZY3fcW09vCrCZK5UTTr+3IiMjQ9OmTdMjjzyigIAA+fr62i0AAADA/a5169aKj49XXFycdWncuLF69+5t/XfRokW1efNm6z5Hjx5VQkKCgoODJUnBwcGKj49XUlKSdZuYmBh5eXkpKCjIUD1OTyLGjRun999/X8OGDdOoUaM0cuRInTx5UqtWrfpbj5sCAAAA7sbfffRqXvL09FTt2rXtxtzd3VWqVCnreN++fRUZGSlfX195eXlpyJAhCg4O1qOPPipJateunYKCgtSnTx9NmTJFiYmJGjVqlCIiInKUhthyehLx6aef6r333tOwYcNUpEgR/fOf/9T777+vN954w/o4KgAAAAB3NmPGDHXq1Endu3dX8+bNFRAQoC+//NK63tXVVWvWrJGrq6uCg4P1r3/9S2FhYRo/frzhc5ksFkvWZ1bmI3d3dx05ckSBgYEqU6aMvvnmGzVs2FC//vqrGjRooJQU45NQi7eckAeVAoDznI8Z7ewSACBXFXP6/TC39+CI9fl2rp+ndMi3c+UmpycR5cuX1+nTpyVJVatW1caNGyXdfBOf0VgFAAAAQN5zehPx1FNPWSeADBkyRKNHj1b16tUVFham559/3snVAQAA4H6Tn++JKKycHiRNnjzZ+u8ePXpY36pXvXp1de7c2YmVAQAAAMiO05uIvwoODrY+hgoAAADIb4U4IMg3TmkiVq9eneNtn3zyyTysBAAAAIBRTmkiunbtmqPtTCaTMjIy8rYYAAAAwIaLC1GEI05pIjIzM51xWgAAAAC5oMDNiQAAAACciTkRjjntEa9btmxRUFCQUlNTs6xLSUnRQw89pO3btzuhMgAAAAB34rQmYubMmXrhhRfk5eWVZZ23t7cGDBigGTNmOKEyAAAA3M94T4RjTmsifvzxR3XocPvXfLdr10579+7Nx4oAAAAA5ITTmogzZ86oaNGit11fpEgRnT17Nh8rAgAAAJATTmsiypUrp4MHD952/YEDB1SmTJl8rAgAAAC4ObE6v5bCymlNRMeOHTV69Ghdu3Yty7qrV69qzJgx6tSpkxMqAwAAAHAnTnvE66hRo/Tll1/qwQcf1ODBg1WjRg1J0k8//aS5c+cqIyNDI0eOdFZ5AAAAuE8V5gnP+cVpTYS/v7927NihQYMGKSoqShaLRdLN/9Lat2+vuXPnyt/f31nlAQAAALgNp75srmLFilq7dq3Onz+vY8eOyWKxqHr16ipZsqQzywIAAMB9jCTCsQLxxuqSJUvq4YcfdnYZAAAAAHKgQDQRAAAAQEFBEOGY057OBAAAAKBwIokAAAAAbDAnwjGSCAAAAACGkEQAAAAANggiHCOJAAAAAGAISQQAAABggzkRjpFEAAAAADCEJAIAAACwQRDhGEkEAAAAAENIIgAAAAAbzIlwjCQCAAAAgCEkEQAAAIANggjHSCIAAAAAGEITAQAAAMAQbmcCAAAAbDCx2jGSCAAAAACGkEQAAAAANggiHCOJAAAAAGAISQQAAABggzkRjpFEAAAAADCEJAIAAACwQRDhGEkEAAAAAENIIgAAAAAbzIlwjCQCAAAAgCEkEQAAAIANggjHSCIAAAAAGEISAQAAANhgToRjJBEAAAAADCGJAAAAAGyQRDhGEgEAAADAEJIIAAAAwAZBhGMkEQAAAAAMoYkAAAAAYAi3MwEAAAA2mFjtGEkEAAAAAENIIgAAAAAbBBGOkUQAAAAAMIQmAgAAALBhMpnybTFi3rx5qlu3rry8vOTl5aXg4GCtW7fOuj4xMVF9+vRRQECA3N3d1bBhQ61YscLuGMnJyerdu7e8vLzk4+Ojvn376tKlS4avEU0EAAAAUAiUL19ekydP1t69e/XDDz+oVatW6tKliw4dOiRJCgsL09GjR7V69WrFx8erW7dueuaZZ7R//37rMXr37q1Dhw4pJiZGa9as0fbt29W/f3/DtZgsFosl1z5ZAVG85QRnlwAAuep8zGhnlwAAuapYAZ6Z2/qdnfl2rs1Dgu9qf19fX02dOlV9+/aVh4eH5s2bpz59+ljXlypVSm+99Zb69eunI0eOKCgoSLGxsWrcuLEkaf369erYsaP++OMPlS1bNsfnJYkAAAAAnCQtLU2pqal2S1pamsP9MjIytHTpUl2+fFnBwTcbkaZNm2rZsmVKTk5WZmamli5dqmvXrqlFixaSpJ07d8rHx8faQEhSmzZt5OLiot27dxuqmyYCAAAAsOFiMuXbEh0dLW9vb7slOjr6trXFx8fLw8NDZrNZAwcO1MqVKxUUFCRJ+vzzz3X9+nWVKlVKZrNZAwYM0MqVK1WtWjVJN+dM+Pn52R2vSJEi8vX1VWJioqFrVICDJAAAAODeFhUVpcjISLsxs9l82+1r1KihuLg4paSkaPny5QoPD9e2bdsUFBSk0aNH68KFC9q0aZMeeOABrVq1Ss8884y+/fZb1alTJ1frpokAAAAAbOTneyLMZvMdm4a/cnNzsyYLjRo1UmxsrGbNmqURI0Zozpw5OnjwoB566CFJUr169fTtt99q7ty5mj9/vgICApSUlGR3vBs3big5OVkBAQGG6uZ2JgAAAKCQyszMVFpamq5cuSJJcnGx//Pe1dVVmZmZkqTg4GBduHBBe/futa7fsmWLMjMz1aRJE0PnJYkAAAAAbBh9f0N+iYqKUkhIiAIDA3Xx4kUtWbJEW7du1YYNG1SzZk1Vq1ZNAwYM0LRp01SqVCmtWrXK+ihXSapVq5Y6dOigF154QfPnz9f169c1ePBg9ezZ09CTmSSaCAAAAKBQSEpKUlhYmE6fPi1vb2/VrVtXGzZsUNu2bSVJa9eu1WuvvabOnTvr0qVLqlatmhYvXqyOHTtaj/Hpp59q8ODBat26tVxcXNS9e3fNnj3bcC28JwIACgHeEwHgXlOQ3xMRMs/Y407vxrpBxm4jKiiYEwEAAADAkALcAwIAAAD5r6DOiShISCIAAAAAGEISAQAAANggiHCMJAIAAACAITQRAAAAAAzhdiYAAADAhkncz+QISQQAAAAAQ0giAAAAABsuBBEOkUQAAAAAMIQkAgAAALDBy+YcI4kAAAAAYAhJBAAAAGCDIMIxkggAAAAAhpBEAAAAADZciCIcIokAAAAAYAhJBAAAAGCDIMIxkggAAAAAhpBEAAAAADZ4T4RjJBEAAAAADCGJAAAAAGwQRDhGEgEAAADAEJIIAAAAwAbviXCMJAIAAACAITQRAAAAAAzhdiYAAADABjczOUYSAQAAAMAQkggAAADABi+bc4wkAgAAAIAhJBEAAACADReCCIdIIgAAAAAYQhIBAAAA2GBOhGMkEQAAAAAMIYkAAAAAbBBEOEYSAQAAAMAQkggAAADABnMiHCOJAAAAAGAISQQAAABgg/dEOEYSAQAAAMAQkggAAADABnMiHMtRE7F69eocH/DJJ5/828UAAAAAKPhy1ER07do1RwczmUzKyMi4m3oAAAAApyKHcCxHTURmZmZe1wEAAACgkGBOBAAAAGDDhTkRDv2tJuLy5cvatm2bEhISlJ6ebrfuxRdfzJXCAAAAABRMhpuI/fv3q2PHjrpy5YouX74sX19fnTt3TiVKlJCfnx9NBAAAAHCPM/yeiJdfflmdO3fW+fPnVbx4ce3atUu//fabGjVqpGnTpuVFjQAAAEC+MZnybymsDDcRcXFxGjZsmFxcXOTq6qq0tDRVqFBBU6ZM0euvv54XNQIAAAAoQAw3EUWLFpWLy83d/Pz8lJCQIEny9vbW77//nrvVAQAAAPnMZDLl21JYGZ4T0aBBA8XGxqp69ep64okn9MYbb+jcuXP6+OOPVbt27byoEQAAAEABYjiJmDRpksqUKSNJmjhxokqWLKlBgwbp7Nmzevfdd3O9QAAAACA/MSfCMcNJROPGja3/9vPz0/r163O1IAAAAAAFGy+bAwAAAGzwsjnHDDcRlStXvuMkkF9//fWuCgIAAABQsBluIoYOHWr38/Xr17V//36tX79ew4cPz626AAAAAKcgiHDMcBPx0ksvZTs+d+5c/fDDD3ddEAAAAICCzfDTmW4nJCREK1asyK3DAQAAAE5RUN8TMW/ePNWtW1deXl7y8vJScHCw1q1bZ7fNzp071apVK7m7u8vLy0vNmzfX1atXreuTk5PVu3dveXl5ycfHR3379tWlS5cMX6NcayKWL18uX1/f3DocAAAAABvly5fX5MmTtXfvXv3www9q1aqVunTpokOHDkm62UB06NBB7dq10549exQbG6vBgwdbXxQtSb1799ahQ4cUExOjNWvWaPv27erfv7/hWkwWi8ViZIcGDRrYdU0Wi0WJiYk6e/as/vOf//ytInJb8QaDnV0CAOSqhO0znV0CAOSq0p4F9yGhQ1YeybdzvfNUrbva39fXV1OnTlXfvn316KOPqm3btpowYUK22x45ckRBQUGKjY21vrZh/fr16tixo/744w+VLVs2x+c1/N9ely5d7JoIFxcXlS5dWi1atFDNmjWNHg4AAAC4b6WlpSktLc1uzGw2y2w233G/jIwMffHFF7p8+bKCg4OVlJSk3bt3q3fv3mratKmOHz+umjVrauLEiXrsscck3UwqfHx87N771qZNG7m4uGj37t166qmncly34SZi7NixRncBAAAACg2jcxXuRnR0tMaNG2c3NmbMmNv+zR0fH6/g4GBdu3ZNHh4eWrlypYKCgrRr1y5JN/9WnzZtmurXr6+PPvpIrVu31sGDB1W9enUlJibKz8/P7nhFihSRr6+vEhMTDdVtuIlwdXXV6dOnsxTw559/ys/PTxkZGUYPCQAAANyXoqKiFBkZaTd2pxSiRo0aiouLU0pKipYvX67w8HBt27ZNmZmZkqQBAwboueeek3RzGsLmzZv14YcfKjo6OlfrNtxE3G4KRVpamtzc3O66IAAAAMCZXPLxPRE5uXXJlpubm6pVqyZJatSokWJjYzVr1iy99tprkqSgoCC77WvVqqWEhARJUkBAgJKSkuzW37hxQ8nJyQoICDBUd46biNmzZ0u6Ge+8//778vDwsK7LyMjQ9u3bmRMBAAAA5KPMzEylpaWpUqVKKlu2rI4ePWq3/ueff1ZISIgkKTg4WBcuXNDevXvVqFEjSdKWLVuUmZmpJk2aGDpvjpuIGTNmSLqZRMyfP1+urq7WdW5ubqpUqZLmz59v6OQAAAAAciYqKkohISEKDAzUxYsXtWTJEm3dulUbNmyQyWTS8OHDNWbMGNWrV0/169fX4sWL9dNPP2n58uWSbqYSHTp00AsvvKD58+fr+vXrGjx4sHr27GnoyUySgSbixIkTkqSWLVvqyy+/VMmSJQ2dCAAAACgM8vN2JiOSkpIUFham06dPy9vbW3Xr1tWGDRvUtm1bSdLQoUN17do1vfzyy0pOTla9evUUExOjqlWrWo/x6aefavDgwWrdurVcXFzUvXt36x1HRhh+T0RhwHsiANxreE8EgHtNQX5PROTqn/LtXG8/WTinAxh+Y3X37t311ltvZRmfMmWK/vGPf+RKUQAAAICzmEymfFsKK8NNxPbt29WxY8cs4yEhIdq+fXuuFAUAAACg4DKcI126dCnbR7kWLVpUqampuVIUAAAA4CwFdU5EQWI4iahTp46WLVuWZXzp0qVZnksLAAAA4N5jOIkYPXq0unXrpuPHj6tVq1aSpM2bN2vJkiXWx0cBAAAAhVUhnqqQbww3EZ07d9aqVas0adIkLV++XMWLF1e9evW0ZcsW+fr65kWNAAAAAAqQv/VsrdDQUIWGhkqSUlNT9dlnn+mVV17R3r17lZGRkasFAgAAAPnJhSjCIcNzIm7Zvn27wsPDVbZsWU2fPl2tWrXSrl27crM2AAAAAAWQoSQiMTFRixYt0gcffKDU1FQ988wzSktL06pVq5hUDQAAgHvC3/6W/T6S42vUuXNn1ahRQwcOHNDMmTN16tQpvfPOO3lZGwAAAIACKMdJxLp16/Tiiy9q0KBBql69el7WBAAAADgNUyIcy3ES8d133+nixYtq1KiRmjRpojlz5ujcuXN5WRsAAACAAijHTcSjjz6q9957T6dPn9aAAQO0dOlSlS1bVpmZmYqJidHFixfzsk4AAAAgX7iYTPm2FFaG5424u7vr+eef13fffaf4+HgNGzZMkydPlp+fn5588sm8qBEAAABAAXJXk89r1KihKVOm6I8//tBnn32WWzUBAAAATmMy5d9SWOXKE6xcXV3VtWtXrV69OjcOBwAAAKAA+1tvrAYAAADuVS6FOCHIL7xLAwAAAIAhNBEAAAAADOF2JgAAAMBGYX70an4hiQAAAABgCEkEAAAAYIMgwjGSCAAAAACGkEQAAAAANnjEq2MkEQAAAAAMIYkAAAAAbJhEFOEISQQAAAAAQ0giAAAAABvMiXCMJAIAAACAISQRAAAAgA2SCMdIIgAAAAAYQhIBAAAA2DDxymqHSCIAAAAAGEISAQAAANhgToRjJBEAAAAADCGJAAAAAGwwJcIxkggAAAAAhtBEAAAAADCE25kAAAAAGy7cz+QQSQQAAAAAQ0giAAAAABs84tUxkggAAAAAhpBEAAAAADaYEuEYSQQAAAAAQ0giAAAAABsuIopwhCQCAAAAgCEkEQAAAIAN5kQ4RhIBAAAAwBCSCAAAAMAG74lwjCQCAAAAgCEkEQAAAIANFyZFOEQSAQAAAMAQkggAAADABkGEYyQRAAAAAAwhiQAAAABsMCfCMZIIAAAAAIbQRAAAAAA2TKb8W4yYN2+e6tatKy8vL3l5eSk4OFjr1q3Lsp3FYlFISIhMJpNWrVplty4hIUGhoaEqUaKE/Pz8NHz4cN24ccPwNeJ2JgAAAKAQKF++vCZPnqzq1avLYrFo8eLF6tKli/bv36+HHnrIut3MmTNlyqZDycjIUGhoqAICArRjxw6dPn1aYWFhKlq0qCZNmmSoFpIIAAAAoBDo3LmzOnbsqOrVq+vBBx/UxIkT5eHhoV27dlm3iYuL0/Tp0/Xhhx9m2X/jxo06fPiwPvnkE9WvX18hISGaMGGC5s6dq/T0dEO10EQAAAAANlzycUlLS1NqaqrdkpaW5rDGjIwMLV26VJcvX1ZwcLAk6cqVK+rVq5fmzp2rgICALPvs3LlTderUkb+/v3Wsffv2Sk1N1aFDhwxfIwAAAABOEB0dLW9vb7slOjr6ttvHx8fLw8NDZrNZAwcO1MqVKxUUFCRJevnll9W0aVN16dIl230TExPtGghJ1p8TExMN1c2cCAAAAMBGdvMJ8kpUVJQiIyPtxsxm8223r1GjhuLi4pSSkqLly5crPDxc27Zt07Fjx7Rlyxbt378/r0uWRBMBAAAAOI3ZbL5j0/BXbm5uqlatmiSpUaNGio2N1axZs1S8eHEdP35cPj4+dtt3795djz/+uLZu3aqAgADt2bPHbv2ZM2ckKdvbn+6E25kAAAAAG6Z8XO5WZmam0tLS9Nprr+nAgQOKi4uzLpI0Y8YMLVy4UJIUHBys+Ph4JSUlWfePiYmRl5eX9ZaonCKJAAAAAAqBqKgohYSEKDAwUBcvXtSSJUu0detWbdiwQQEBAdmmCYGBgapcubIkqV27dgoKClKfPn00ZcoUJSYmatSoUYqIiDCUhkg0EQAAAIAdl3ycE2FEUlKSwsLCdPr0aXl7e6tu3brasGGD2rZtm6P9XV1dtWbNGg0aNEjBwcFyd3dXeHi4xo8fb7gWmggAAACgEPjggw8MbW+xWLKMVaxYUWvXrr3rWmgiAAAAABsFM4coWJhYDQAAAMAQkggAAADARgGdElGgkEQAAAAAMIQkAgAAALCRn2+sLqxIIgAAAAAYQhIBAAAA2OBbdse4RgAAAAAMIYkAAAAAbDAnwjGSCAAAAACG0EQAAAAAMITbmQAAAAAb3MzkGEkEAAAAAENIIgAAAAAbTKx2jCQCAAAAgCEkEQAAAIANvmV3jGsEAAAAwBCSCAAAAMAGcyIcI4kAAAAAYAhJBAAAAGCDHMIxkggAAAAAhpBEAAAAADaYEuEYSQQAAAAAQ0giAAAAABsuzIpwiCQCAAAAgCEkEQAAAIAN5kQ4RhIBAAAAwBCSCAAAAMCGiTkRDpFEAAAAADCEJAIAAACwwZwIx0giAAAAABhCEwEAAADAEG5nAgAAAGzwsjnHSCIAAAAAGEISAQAAANhgYrVjJBEAAAAADCmwTcSZM2c0fvx4Z5cBAACA+4zJlH9LYVVgm4jExESNGzfO2WUAAAAA+AunzYk4cODAHdcfPXo0nyoBAAAA/sfE05kccloTUb9+fZlMJlkslizrbo2bCnPGAwAAANyjnNZE+Pr6asqUKWrdunW26w8dOqTOnTvnc1UAAAC437nwPbZDTmsiGjVqpFOnTqlixYrZrr9w4UK2KQUAAAAA53JaEzFw4EBdvnz5tusDAwO1cOHCfKwIAAAAYE5ETjitiXjqqafuuL5kyZIKDw/Pp2oAAAAA5BRvrAYAAABs8GwfxwrseyIAAAAAFEwkEQAAAIAN5kQ4RhIBAAAAwBCSCAAAAMAG74lwzOlJxPr16/Xdd99Zf547d67q16+vXr166fz5806sDAAAAEB2nN5EDB8+XKmpqZKk+Ph4DRs2TB07dtSJEycUGRnp5OoAAAAA/JXTb2c6ceKEgoKCJEkrVqxQp06dNGnSJO3bt08dO3Z0cnUAAAC43zCx2jGnJxFubm66cuWKJGnTpk1q166dJMnX19eaUAAAAAAoOJyeRDz22GOKjIxUs2bNtGfPHi1btkyS9PPPP6t8+fJOrg4AAAD3G14255jTk4g5c+aoSJEiWr58uebNm6dy5cpJktatW6cOHTo4uTrc7155rq2u7p+jqa90z3b9qjmDdHX/HHVuUddufPqIp/X9pyN0YfcM7Vr6Wn6UCgDZ+njhe+oX9ozaNn9Yndo+rqhhQ5Rw8oTdNv/3R4KiXnlRndo8pnZPPKLRr0Uq+c9zdtss/mCBBj7fW62bNVKHFo/m50cAUAA5PYkIDAzUmjVrsozPmDHDCdUA/9MoKFB9uzfTgZ//yHb9kN4tZbHcfv+Pvtqlh+tUVO3q5fKoQgBwbP++WHX7xz9VM6iOMjJu6N25s/Ty4Bf0yRerVbx4CV29ekUvR/RXtQdraNb8DyVJ7897R6++HKEFiz6Ti8vN7xtv3Liulq3b6aE69fTNV1868yMBeY4gwjGnJxH79u1TfHy89eevvvpKXbt21euvv6709HQnVob7mXtxNy2c9Kz+PeEzXUi9mmV93QfL6aU+rTRw7CfZ7j9synIt+Hy7TvzxZ16XCgB39PY776pj56dUpWo1VX+wpl4fO1FnEk/r6JHDkqT4H/cr8fT/aeSYiapa7UFVrfagRo6bpJ+OHNLe2N3W4/QdMFg9eoerarXqzvoowH1v3rx5qlu3rry8vOTl5aXg4GCtW7dOkpScnKwhQ4aoRo0aKl68uAIDA/Xiiy8qJSXF7hgJCQkKDQ1ViRIl5Ofnp+HDh+vGjRuGa3F6EzFgwAD9/PPPkqRff/1VPXv2VIkSJfTFF19oxIgRTq4O96uZUT20/tuD+u/uo1nWFS9WVIuin9XQyZ/rzJ8XnVAdAPx9ly/d/L3l5eUtSUpPT5fJZFJRNzfrNm5uZrm4uOhA3D6n1Ag4m4vJlG+LEeXLl9fkyZO1d+9e/fDDD2rVqpW6dOmiQ4cO6dSpUzp16pSmTZumgwcPatGiRVq/fr369u1r3T8jI0OhoaFKT0/Xjh07tHjxYi1atEhvvPGG8WtkeI9c9vPPP6t+/fqSpC+++ELNmzfXkiVLtGjRIq1YscLh/mlpaUpNTbVbLJkZeVw17mX/aN9I9WtW0Oh3Vme7fsqw7tr14wmt2Rqf7XoAKKgyMzM1e/pbqlOvgar8/0ThoTr1VKxYcc17Z7quXbuqq1evaO7MqcrIyNCf5846uWIAtjp37qyOHTuqevXqevDBBzVx4kR5eHho165dql27tlasWKHOnTuratWqatWqlSZOnKivv/7amjRs3LhRhw8f1ieffKL69esrJCREEyZM0Ny5cw3fAeT0JsJisSgzM1PSzUe83no3RIUKFXTu3Lk77SpJio6Olre3t91y48zePK0Z967y/j6aOry7nhu5SGnpWaO90CfqqMUjD2r41OVOqA4A7s7bb72pX4//onGTplnHSpb01YS33tb327ep7eMPq0OLR3Xp4kU9WDPIOh8CuN+Y8nHJ7gvxtLQ0hzVmZGRo6dKlunz5soKDg7PdJiUlRV5eXipS5OY06J07d6pOnTry9/e3btO+fXulpqbq0KFDBq5QAZhY3bhxY7355ptq06aNtm3bpnnz5km6+RI62w94O1FRUVnebO33+Kt5UivufQ1qBcq/lJd2Lvnf/w0VKeKqxxpW1cAezfXe8u9UpfwDStw+1W6/z6b10/f7j6v9C7Pyu2QAyJG333pTO77bpjnvLpaff4DdukcebabPv1qvCxfOy9XVVZ6eXnqyfXOVLRfipGqB+0d0dLTGjRtnNzZmzBiNHTs22+3j4+MVHBysa9euycPDQytXrrS+uNnWuXPnNGHCBPXv3986lpiYmOXv61s/JyYmGqrb6U3EzJkz1bt3b61atUojR45UtWrVJEnLly9X06ZNHe5vNptlNpvtxkwurnlSK+59/91zVI2enmg39u64f+noiTOavihGf164pPeXf2e3fu/ykRoxfYW+2XYwP0sFgByxWCyaMWWitm/drHcWLFLZcrd/B5OPT0lJ0t7YXTqfnKzHmrfMrzKBgiUfH8+U3Rfif/3b1laNGjUUFxenlJQULV++XOHh4dq2bZtdI5GamqrQ0FAFBQXdthm5W05vIurWrWv3dKZbpk6dKldXmgHkr0tX0nT4+Gm7sctX05Wcctk6nt1k6t9Pn9dvp/73JKYqFR6QR3Gz/B/wUnFzUdV98OZjXo/8mqjrN5izAyD/TH9rgjatX6vo6e+oRIkS1nkOHh6eMhcrJkn6ZvVKVaxcRSVLltTBAz9q1vRoPdMrTIGVKluPk5h4ShdTUnQm8bQyMjP0y9EjkqRyFQJVooR7/n8w4B6R3Rfid+Lm5mb90r1Ro0aKjY3VrFmztGDBAknSxYsX1aFDB3l6emrlypUqWrSodd+AgADt2bPH7nhnzpyxrjPC6U3E7RT7/7/YgMJo3hu91bzx/x6DuHtZlCSpRsc3lHA62VllAbgPrVq+TJI0ZMCzduOvj3lTHTs/JUlK+O2EFsydodSUFAWULaew5/qrR+9wu+0/mD9H69Z8Zf35ud5PS5Jmz1+oho0fycNPAOQ/UyF6U0RmZqZ1DkVqaqrat28vs9ms1atXZ/l7Ojg4WBMnTlRSUpL8/PwkSTExMfLy8sr2lqg7MVksd3pdVt7LyMjQjBkz9PnnnyshISHLzPDkZON/cBVvMDi3ygOAAiFh+0xnlwAAuaq0Z4H9Llu7j6c43iiXNKnqneNto6KiFBISosDAQF28eFFLlizRW2+9pQ0bNqhJkyZq166drly5opUrV8rd/X8JYenSpeXq6qqMjAzVr19fZcuW1ZQpU5SYmKg+ffqoX79+mjRpkqG6nf7YhXHjxuntt99Wjx49lJKSosjISHXr1k0uLi55dg8XAAAAcDsmU/4tRiQlJSksLEw1atRQ69atFRsbqw0bNqht27bat2+fdu/erfj4eFWrVk1lypSxLr///rskydXVVWvWrJGrq6uCg4P1r3/9S2FhYRo/frzxa+TsJKJq1aqaPXu2QkND5enpqbi4OOvYrl27tGTJEsPHJIkAcK8hiQBwrynIScSeX/MviXikSs6TiILE6UlEYmKi6tSpI0ny8PCwvpq7U6dO+uabb5xZGgAAAO5D+fmeiMLK6U1E+fLldfr0zafeVK1aVRs3bpQkxcbGGpqpDgAAACB/OL2JeOqpp7R582ZJ0pAhQzR69GhVr15dYWFhev75551cHQAAAO47RBEOOf1mtMmTJ1v/3aNHDwUGBmrnzp2qXr26Onfu7MTKAAAAAGTH6U3EXwUHBys4ONjZZQAAAAC4Dac0EatXr87xtk8++WQeVgIAAADYK0wvm3MWpzQRXbt2zdF2JpNJGRkZeVsMAAAAAEOc0kRkZmY647QAAACAQ0ZfAnc/cvrTmQAAAAAULk5rIrZs2aKgoCClpqZmWZeSkqKHHnpI27dvd0JlAAAAuJ/xhFfHnNZEzJw5Uy+88IK8vLyyrPP29taAAQM0Y8YMJ1QGAAAA4E6c1kT8+OOP6tChw23Xt2vXTnv37s3HigAAAAARReSA05qIM2fOqGjRorddX6RIEZ09ezYfKwIAAACQE05rIsqVK6eDBw/edv2BAwdUpkyZfKwIAAAAuPmeiPz6T2HltCaiY8eOGj16tK5du5Zl3dWrVzVmzBh16tTJCZUBAAAAuBOTxWKxOOPEZ86cUcOGDeXq6qrBgwerRo0akqSffvpJc+fOVUZGhvbt2yd/f3/Dxy7eYHBulwsATpWwfaazSwCAXFXa0ymvK8uRuISL+Xau+oGe+Xau3OS0//b8/f21Y8cODRo0SFFRUbrVy5hMJrVv315z5879Ww0EAAAAgLzl1BawYsWKWrt2rc6fP69jx47JYrGoevXqKlmypDPLAgAAwH2s8M5UyD8FIkcqWbKkHn74YWeXAQAAACAHCkQTAQAAABQYRBEOOe3pTAAAAAAKJ5IIAAAAwEZhfn9DfiGJAAAAAGAITQQAAAAAQ7idCQAAALBh4m4mh0giAAAAABhCEgEAAADYIIhwjCQCAAAAgCEkEQAAAIAtogiHSCIAAAAAGEISAQAAANjgZXOOkUQAAAAAMIQkAgAAALDBeyIcI4kAAAAAYAhJBAAAAGCDIMIxkggAAAAAhpBEAAAAALaIIhwiiQAAAABgCEkEAAAAYIP3RDhGEgEAAADAEJIIAAAAwAbviXCMJAIAAACAITQRAAAAAAzhdiYAAADABnczOUYSAQAAAMAQkggAAADAFlGEQyQRAAAAAAwhiQAAAABs8LI5x0giAAAAABhCEgEAAADY4GVzjpFEAAAAADCEJAIAAACwQRDhGEkEAAAAAENIIgAAAABbRBEOkUQAAAAAMIQkAgAAALDBeyIcI4kAAAAACoF58+apbt268vLykpeXl4KDg7Vu3Trr+mvXrikiIkKlSpWSh4eHunfvrjNnztgdIyEhQaGhoSpRooT8/Pw0fPhw3bhxw3AtNBEAAACADZMp/xYjypcvr8mTJ2vv3r364Ycf1KpVK3Xp0kWHDh2SJL388sv6+uuv9cUXX2jbtm06deqUunXrZt0/IyNDoaGhSk9P144dO7R48WItWrRIb7zxhvFrZLFYLIb3KuCKNxjs7BIAIFclbJ/p7BIAIFeV9iy4d9WfOHct385V+YFid7W/r6+vpk6dqqefflqlS5fWkiVL9PTTT0uSfvrpJ9WqVUs7d+7Uo48+qnXr1qlTp046deqU/P39JUnz58/Xq6++qrNnz8rNzS3H5yWJAAAAAGyY8nFJS0tTamqq3ZKWluawxoyMDC1dulSXL19WcHCw9u7dq+vXr6tNmzbWbWrWrKnAwEDt3LlTkrRz507VqVPH2kBIUvv27ZWammpNM3KKJgIAAABwkujoaHl7e9st0dHRt90+Pj5eHh4eMpvNGjhwoFauXKmgoCAlJibKzc1NPj4+dtv7+/srMTFRkpSYmGjXQNxaf2udEQU3RwIAAACcIR8fzhQVFaXIyEi7MbPZfNvta9Soobi4OKWkpGj58uUKDw/Xtm3b8rrMLGgiAAAAACcxm813bBr+ys3NTdWqVZMkNWrUSLGxsZo1a5Z69Oih9PR0XbhwwS6NOHPmjAICAiRJAQEB2rNnj93xbj296dY2OcXtTAAAAEAhlZmZqbS0NDVq1EhFixbV5s2breuOHj2qhIQEBQcHS5KCg4MVHx+vpKQk6zYxMTHy8vJSUFCQofOSRAAAAAA2CurL5qKiohQSEqLAwEBdvHhRS5Ys0datW7VhwwZ5e3urb9++ioyMlK+vr7y8vDRkyBAFBwfr0UcflSS1a9dOQUFB6tOnj6ZMmaLExESNGjVKERERhtIQiSYCAAAAKBSSkpIUFham06dPy9vbW3Xr1tWGDRvUtm1bSdKMGTPk4uKi7t27Ky0tTe3bt9d//vMf6/6urq5as2aNBg0apODgYLm7uys8PFzjx483XAvviQCAQoD3RAC41xTk90QkJDt+xGpuCfQ1lgAUFMyJAAAAAGBIwW0BAQAAACcomDMiChaSCAAAAACGkEQAAAAANkxEEQ6RRAAAAAAwhCQCAAAAsEMU4QhJBAAAAABDSCIAAAAAG8yJcIwkAgAAAIAhJBEAAACADYIIx0giAAAAABhCEgEAAADYYE6EYyQRAAAAAAwhiQAAAABsmJgV4RBJBAAAAABDaCIAAAAAGMLtTAAAAIAt7mZyiCQCAAAAgCEkEQAAAIANggjHSCIAAAAAGEISAQAAANjgZXOOkUQAAAAAMIQkAgAAALDBy+YcI4kAAAAAYAhJBAAAAGCLIMIhkggAAAAAhpBEAAAAADYIIhwjiQAAAABgCEkEAAAAYIP3RDhGEgEAAADAEJIIAAAAwAbviXCMJAIAAACAISQRAAAAgA3mRDhGEgEAAADAEJoIAAAAAIbQRAAAAAAwhCYCAAAAgCFMrAYAAABsMLHaMZIIAAAAAIaQRAAAAAA2eNmcYyQRAAAAAAwhiQAAAABsMCfCMZIIAAAAAIaQRAAAAAA2CCIcI4kAAAAAYAhJBAAAAGCLKMIhkggAAAAAhpBEAAAAADZ4T4RjJBEAAAAADCGJAAAAAGzwngjHSCIAAAAAGEISAQAAANggiHCMJAIAAACAISQRAAAAgC2iCIdIIgAAAAAYQhMBAAAAwBCaCAAAAMCGKR//Y0R0dLQefvhheXp6ys/PT127dtXRo0fttklMTFSfPn0UEBAgd3d3NWzYUCtWrLDbJjk5Wb1795aXl5d8fHzUt29fXbp0yVAtNBEAAABAIbBt2zZFRERo165diomJ0fXr19WuXTtdvnzZuk1YWJiOHj2q1atXKz4+Xt26ddMzzzyj/fv3W7fp3bu3Dh06pJiYGK1Zs0bbt29X//79DdVislgsllz7ZAVE8QaDnV0CAOSqhO0znV0CAOSq0p4F9/k+127k37mK3cVlOHv2rPz8/LRt2zY1b95ckuTh4aF58+apT58+1u1KlSqlt956S/369dORI0cUFBSk2NhYNW7cWJK0fv16dezYUX/88YfKli2bo3OTRAAAAABOkpaWptTUVLslLS0tR/umpKRIknx9fa1jTZs21bJly5ScnKzMzEwtXbpU165dU4sWLSRJO3fulI+Pj7WBkKQ2bdrIxcVFu3fvznHdBbcFvAtX989xdgm4D6SlpSk6OlpRUVEym83OLgcA7hq/14Cb7iYdMGrsm9EaN26c3diYMWM0duzYO+6XmZmpoUOHqlmzZqpdu7Z1/PPPP1ePHj1UqlQpFSlSRCVKlNDKlStVrVo1STfnTPj5+dkdq0iRIvL19VViYmKO6yaJAP6mtLQ0jRs3LsffFgBAQcfvNSD/RUVFKSUlxW6JiopyuF9ERIQOHjyopUuX2o2PHj1aFy5c0KZNm/TDDz8oMjJSzzzzjOLj43O17nsyiQAAAAAKA7PZbDj5Gzx4sHVCdPny5a3jx48f15w5c3Tw4EE99NBDkqR69erp22+/1dy5czV//nwFBAQoKSnJ7ng3btxQcnKyAgICclwDSQQAAABQCFgsFg0ePFgrV67Uli1bVLlyZbv1V65ckSS5uNj/ie/q6qrMzExJUnBwsC5cuKC9e/da12/ZskWZmZlq0qRJjmshiQAAAAAKgYiICC1ZskRfffWVPD09rXMYvL29Vbx4cdWsWVPVqlXTgAEDNG3aNJUqVUqrVq2yPspVkmrVqqUOHTrohRde0Pz583X9+nUNHjxYPXv2zPGTmSSSCOBvM5vNGjNmDJMPAdwz+L0GFGzz5s1TSkqKWrRooTJlyliXZcuWSZKKFi2qtWvXqnTp0urcubPq1q2rjz76SIsXL1bHjh2tx/n0009Vs2ZNtW7dWh07dtRjjz2md99911At9+R7IgAAAADkHZIIAAAAAIbQRAAAAAAwhCYCAAAAgCE0EYAkk8mkVatWObsMAMg1/F4DkJdoInDPS0xM1JAhQ1SlShWZzWZVqFBBnTt31ubNm51dmqSbz3x+4403VKZMGRUvXlxt2rTRL7/84uyyABRgBf332pdffql27dqpVKlSMplMiouLc3ZJAHIZTQTuaSdPnlSjRo20ZcsWTZ06VfHx8Vq/fr1atmypiIgIZ5cnSZoyZYpmz56t+fPna/fu3XJ3d1f79u117do1Z5cGoAAqDL/XLl++rMcee0xvvfWWs0sBkFcswD0sJCTEUq5cOculS5eyrDt//rz135IsK1eutP48YsQIS/Xq1S3Fixe3VK5c2TJq1ChLenq6dX1cXJylRYsWFg8PD4unp6elYcOGltjYWIvFYrGcPHnS0qlTJ4uPj4+lRIkSlqCgIMs333yTbX2ZmZmWgIAAy9SpU61jFy5csJjNZstnn312l58ewL2ooP9es3XixAmLJMv+/fv/9ucFUDDxxmrcs5KTk7V+/XpNnDhR7u7uWdb7+Pjcdl9PT08tWrRIZcuWVXx8vF544QV5enpqxIgRkqTevXurQYMGmjdvnlxdXRUXF6eiRYtKuvk2yfT0dG3fvl3u7u46fPiwPDw8sj3PiRMnlJiYqDZt2ljHvL291aRJE+3cuVM9e/a8iysA4F5TGH6vAbg/0ETgnnXs2DFZLBbVrFnT8L6jRo2y/rtSpUp65ZVXtHTpUuv/2CYkJGj48OHWY1evXt26fUJCgrp37646depIkqpUqXLb89x6Xb2/v7/duL+/v3UdANxSGH6vAbg/MCcC9yzLXbyMfdmyZWrWrJkCAgLk4eGhUaNGKSEhwbo+MjJS/fr1U5s2bTR58mQdP37cuu7FF1/Um2++qWbNmmnMmDE6cODAXX0OALiF32sACgqaCNyzqlevLpPJpJ9++snQfjt37lTv3r3VsWNHrVmzRvv379fIkSOVnp5u3Wbs2LE6dOiQQkNDtWXLFgUFBWnlypWSpH79+unXX39Vnz59FB8fr8aNG+udd97J9lwBAQGSpDNnztiNnzlzxroOAG4pDL/XANwnnDslA8hbHTp0MDwBcdq0aZYqVarYbdu3b1+Lt7f3bc/Ts2dPS+fOnbNd99prr1nq1KmT7bpbE6unTZtmHUtJSWFiNYDbKui/12wxsRq4d5FE4J42d+5cZWRk6JFHHtGKFSv0yy+/6MiRI5o9e7aCg4Oz3ad69epKSEjQ0qVLdfz4cc2ePdv6bZwkXb16VYMHD9bWrVv122+/6fvvv1dsbKxq1aolSRo6dKg2bNigEydOaN++ffrvf/9rXfdXJpNJQ4cO1ZtvvqnVq1crPj5eYWFhKlu2rLp27Zrr1wNA4VfQf69JNyeAx8XF6fDhw5Kko0ePKi4ujrlewL3E2V0MkNdOnTpliYiIsFSsWNHi5uZmKVeunOXJJ5+0/Pe//7Vuo788CnH48OGWUqVKWTw8PCw9evSwzJgxw/qNXVpamqVnz56WChUqWNzc3Cxly5a1DB482HL16lWLxWKxDB482FK1alWL2Wy2lC5d2tKnTx/LuXPnbltfZmamZfTo0RZ/f3+L2Wy2tG7d2nL06NG8uBQA7hEF/ffawoULLZKyLGPGjMmDqwHAGUwWy13M0gIAAABw3+F2JgAAAACG0EQAAAAAMIQmAgAAAIAhNBEAAAAADKGJAAAAAGAITQQAAAAAQ2giAAAAABhCEwEAAADAEJoIAChgnn32WXXt2tX6c4sWLTR06NB8r2Pr1q0ymUy6cOFCvp8bAFCw0UQAQA49++yzMplMMplMcnNzU7Vq1TR+/HjduHEjT8/75ZdfasKECTnalj/8AQD5oYizCwCAwqRDhw5auHCh0tLStHbtWkVERKho0aKKioqy2y49PV1ubm65ck5fX99cOQ4AALmFJAIADDCbzQoICFDFihU1aNAgtWnTRqtXr7begjRx4kSVLVtWNWrUkCT9/vvveuaZZ+Tj4yNfX1916dJFJ0+etB4vIyNDkZGR8vHxUalSpTRixAhZLBa7c/71dqa0tDS9+uqrqlChgsxms6pVq6YPPvhAJ0+eVMuWLSVJJUuWlMlk0rPPPitJyszMVHR0tCpXrqzixYurXr16Wr58ud151q5dqwcffFDFixdXy5Yt7eoEAMAWTQQA3IXixYsrPT1dkrR582YdPXpUMTExWrNmja5fv6727dvL09NT3377rb7//nt5eHioQ4cO1n2mT5+uRYsW6cMPP9R3332n5ORkrVy58o7nDAsL02effabZs2fryJEjWrBggTw8PFShQgWtWLFCknT06FGdPn1as2bNkiRFR0fro48+0vz583Xo0CG9/PLL+te//qVt27ZJutnsdOvWTZ07d1ZcXJz69eun1157La8uGwCgkON2JgD4GywWizZv3qwNGzZoyJAhOnv2rNzd3fX+++9bb2P65JNPlJmZqffff18mk0mStHDhQvn4+Gjr1q1q166dZs6cqaioKHXr1k2SNH/+fG3YsOG25/3555/1+eefKyYmRm3atJEkValSxbr+1q1Pfn5+8vHxkXQzuZg0aZI2bdqk4OBg6z7fffedFixYoCeeeELz5s1T1apVNX36dElSjRo1FB8fr7feeisXrxoA4F5BEwEABqxZs0YeHh66fv26MjMz1atXL40dO1YRERGqU6eO3TyIH3/8UceOHZOnp6fdMa5du6bjx48rJSVFp0+fVpMmTazrihQposaNG2e5pemWuLg4ubq66oknnshxzceOHdOVK1fUtm1bu/H09HQ1aNBAknTkyBG7OiRZGw4AAP6KJgIADGjZsqXmzZsnNzc3lS1bVkWK/O/XqLu7u922ly5dUqNGjfTpp59mOU7p0qX/1vmLFy9ueJ9Lly5Jkr755huVK1fObp3ZbP5bdQAA7m80EQBggLu7u6pVq5ajbRs2bKhly5bJz89PXl5e2W5TpkwZ7d69W82bN5ck3bhxQ3v37lXDhg2z3b5OnTrKzMzUtm3brLcz2bqVhGRkZFjHgoKCZDablZCQcNsEo1atWlq9erXd2K5duxx/SADAfYmJ1QCQR3r37q0HHnhAXbp00bfffqsTJ05o69atevHFF/XHH39Ikl566SVNnjxZq1at0k8//aR///vfd3zHQ6VKlRQeHq7nn39eq1atsh7z888/lyRVrFhRJpNJa9as0dmzZ3Xp0iV5enrqlVde0csvv6zFixfr+PHj2rdvn9555x0tXrxYkjRw4ED98ssvGj58uI4ePaolS5Zo0aJFeX2JAACFFE0EAOSREiVKaPv27QoMDFS3bt1Uq1Yt9e3bV9euXbMmE8OGDVOfPn0UHh6u4OBgeXp66qmnnrrjcefNm6enn35a//73v1WzZk298MILunz5siSpXLlyGjdunF577TX5+/tr8ODBkqQJEyZo9OjRio6OVq1atdShQwd98803qly5siQpMDBQK1as0KpVq1SvXj3Nnz9fkyZNysOrAwAozEyW283eAwAAAIBskEQAAAAAMIQmAgAAAIAhNBEAAAAADKGJAAAAAGAITQQAAAAAQ2giAAAAABhCEwEAAADAEJoIAAAAAIbQRAAAAAAwhCYCAAAAgCE0EQAAAAAM+X8nSkLPlY6/xwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 ROC AUC: 0.6039\n",
            "Fold 2 Precision: 0.6377\n",
            "Fold 2 Recall: 0.4016\n",
            "Fold 2 F1-Score: 0.4929\n",
            "Average Validation Accuracy: 0.5606\n",
            "Variance of Validation Accuracy: 0.0000\n",
            "Average Test Accuracy: 0.5759\n",
            "Variance of Test Accuracy: 0.0000\n",
            "Average ROC AUC: 0.6074\n",
            "Average Precision: 0.6006\n",
            "Average Recall: 0.4877\n",
            "Average F1-Score: 0.5308\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,645\u001b[0m (315.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,645</span> (315.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,881\u001b[0m (105.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,881</span> (105.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m53,764\u001b[0m (210.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,764</span> (210.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uujWvez105h-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}