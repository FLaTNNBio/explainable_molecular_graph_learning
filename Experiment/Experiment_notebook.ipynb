{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtVle8RHNqON"
      },
      "source": [
        "# Installation and Common Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLTzxU_DNzLi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  !pip install pysmiles\n",
        "  !pip install git+https://github.com/VenkateshwaranB/stellargraph.git\n",
        "  !pip install rdkit\n",
        "  !pip install torch_geometric\n",
        "  !pip install datasets\n",
        "  !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcuv1AHaNqOR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from stellargraph import datasets\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv8jU7ilW-6t"
      },
      "outputs": [],
      "source": [
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you run on colab"
      ],
      "metadata": {
        "id": "LwlFi6nUSOyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2IcVbx3jwCr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C1O3XD3xibZ"
      },
      "source": [
        "Common Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the HIV Dataset"
      ],
      "metadata": {
        "id": "i1ACg7kHSR34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjfW3Bz-qAr-"
      },
      "outputs": [],
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "\n",
        "#qua sta anche clintox e altri vedere bene documntazione però il codice funziona anche con uun proprio dataset, però il clintox ha la y diversa dalla nostra quindi in molti casi conviene avere a che fare con db propri, dove settiamo noi la y per bene e usiamo lo script SMILEs-MOL-OGB\n",
        "all_data = dataset[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "modify the path in order to load the file"
      ],
      "metadata": {
        "id": "o2wNm_oISWgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4sCu2-XWLin"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch_geometric\n",
        "# Load Data object from file\n",
        "with open('/content/drive/MyDrive/clintox.pt', 'rb') as f:\n",
        "    all_data = pickle.load(f)\n",
        "\n",
        "# Now loaded_data contains the Data object\n",
        "print(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQS4IfKzcC9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import stellargraph as sg\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from stellargraph.data import BiasedRandomWalk\n",
        "\n",
        "def create_networkx_graph(edge_index, edge_attr, x, y, num_nodes):\n",
        "    # Create an empty NetworkX graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with features\n",
        "    for i in range(num_nodes):\n",
        "        node_features = {f\"feat_{j}\": x[i, j].item() for j in range(x.shape[1])}\n",
        "        G.add_node(i, features=node_features)\n",
        "\n",
        "    # Add edges with attributes\n",
        "    for j in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[:, j].tolist()\n",
        "        edge_attributes = {f\"attr_{k}\": edge_attr[j, k].item() for k in range(edge_attr.shape[1])}\n",
        "        G.add_edge(src, dst, attributes=edge_attributes)\n",
        "\n",
        "    return G, y.item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names=None):\n",
        "    \"\"\"\n",
        "    Plots the confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (list or array): True labels.\n",
        "    y_pred (list or array): Predicted labels.\n",
        "    class_names (list): List of class names. If None, integer labels are used.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# y_true = [0, 1, 1, 0, 1, 0]\n",
        "# y_pred = [0, 1, 0, 0, 1, 1]\n",
        "# class_names = ['Class 0', 'Class 1']\n",
        "# plot_confusion_matrix(y_true, y_pred, class_names)\n"
      ],
      "metadata": {
        "id": "e_Ix2PETlKT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZBWOStOXUU8"
      },
      "source": [
        "#Node2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxkjURhnXcIv"
      },
      "source": [
        "Construction of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I_OdnXOCqE8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import copy\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "label_0_indices = []\n",
        "label_1_indices = []\n",
        "dfs = []\n",
        "\n",
        "for i in range(len(all_data)):\n",
        "  if all_data[i].y == 0:\n",
        "    label_0_indices.append(i)\n",
        "  elif all_data[i].y == 1:\n",
        "    label_1_indices.append(i)\n",
        "\n",
        "for iter in range(1):\n",
        "  print('iter:', iter)\n",
        "\n",
        "  min_size_dataset = min(len(label_0_indices), len(label_1_indices))\n",
        "\n",
        "  random.shuffle(label_0_indices)\n",
        "  random.shuffle(label_1_indices)\n",
        "  label_0_indices = label_0_indices[:min_size_dataset]\n",
        "  label_1_indices = label_1_indices[:min_size_dataset]\n",
        "  print('size 0 data:' , len(label_0_indices))\n",
        "  print('size 1 data:', len(label_1_indices))\n",
        "\n",
        "  balanced_indices = label_0_indices + label_1_indices\n",
        "  print('size total dataset:' , len(balanced_indices))\n",
        "\n",
        "  balanced_data = [all_data[i] for i in balanced_indices]\n",
        "  print(len(balanced_data))\n",
        "\n",
        "  df = pd.DataFrame(columns = ['emb','label'])\n",
        "\n",
        "  for dtb in balanced_data:\n",
        "    graph,y = create_networkx_graph(dtb.edge_index, dtb.edge_attr, dtb.x, dtb.y, dtb.num_nodes)\n",
        "        # Convert node features to DataFrame\n",
        "    node_features_dict = {node: graph.nodes[node]['features'] for node in graph.nodes}\n",
        "    node_df = pd.DataFrame.from_dict(node_features_dict, orient='index')\n",
        "\n",
        "        # Convert edge attributes to DataFrame\n",
        "    edge_features_dict = {(src, dst): data['attributes'] for src, dst, data in graph.edges(data=True)}\n",
        "    edge_df = pd.DataFrame.from_dict(edge_features_dict, orient='index')\n",
        "\n",
        "        # Create 'source', 'target' columns in the edge DataFrame\n",
        "    edge_df['source'] = [edge[0] for edge in edge_df.index]\n",
        "    edge_df['target'] = [edge[1] for edge in edge_df.index]\n",
        "\n",
        "        # Create a StellarGraph from the NetworkX graph and DataFrames\n",
        "    Gs = sg.StellarGraph(nodes=node_df, edges=edge_df)\n",
        "\n",
        "    rw = BiasedRandomWalk(Gs)\n",
        "\n",
        "    walks = rw.run(\n",
        "        nodes=list(Gs.nodes()),  # root nodes\n",
        "        length=100,  # maximum length of a random walk\n",
        "        n=10,  # number of random walks per root node\n",
        "        p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
        "        q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
        "        weighted=True,\n",
        "        seed = 42\n",
        "    )\n",
        "\n",
        "    str_walks = [[str(n) for n in walk] for walk in walks]\n",
        "    model = Word2Vec(str_walks, window=5, min_count=0, sg=1, workers=2)\n",
        "\n",
        "        # Retrieve node embeddings and corresponding subjects\n",
        "    node_ids = model.wv.index_to_key  # list of node IDs\n",
        "    node_embeddings = (\n",
        "        model.wv.vectors\n",
        "    )  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
        "\n",
        "    df.loc[len(df)] = [node_embeddings, y]\n",
        "  #df.to_pickle('/content/drive/MyDrive/emb_bbbp_node2vec_data_' + str(iter + 1) + '.pkl')\n",
        "  #print(df)\n",
        "  dfs.append(df)\n",
        "  #se va tutto bene poi questo break si toglie e si fa per tuti i kfold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Udl-HSNsLnu"
      },
      "source": [
        "Creation of the Model for classification on the Embedding data given from node2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMg83xsGuhn7"
      },
      "outputs": [],
      "source": [
        "# Define a function to create and compile your model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def create_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(222, 100)),\n",
        "    layers.Dropout(0.3),  # Adding dropout for regularization\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),  # Adding dropout for regularization\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.3),  # Adding dropout for regularization\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVeC_w4UsjDq"
      },
      "source": [
        "5 k-fold on 5 datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AvjMtzYbuJxl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize dictionary to store best models and their performances\n",
        "best_models = {}\n",
        "\n",
        "for df_idx, df in enumerate(dfs):\n",
        "    best_accuracy = 0\n",
        "    best_model = None\n",
        "\n",
        "    # Prepare data\n",
        "    X_emb = pad_sequences(df['emb'], dtype='float32', padding='post')\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    for fold_idx, (train_index, val_index) in enumerate(kf.split(X_emb)):\n",
        "        X_emb_train, X_emb_val = X_emb[train_index], X_emb[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        model = create_model()\n",
        "        print(len((y_train.reshape(-1, 1))))\n",
        "        history = model.fit(X_emb_train, y_train.reshape(-1, 1), epochs=100, batch_size=32, validation_data=(X_emb_val, y_val.reshape(-1, 1)))\n",
        "\n",
        "        # Evaluate model performance\n",
        "        _, accuracy = model.evaluate(X_emb_val, y_val)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model\n",
        "\n",
        "    # Save the best model for this dataset\n",
        "    best_models[f'df_{df_idx}'] = best_model\n",
        "    print(f\"Best accuracy for dataset {df_idx}: {best_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFwkcgJmoUv3"
      },
      "outputs": [],
      "source": [
        "for idx, (name, model) in enumerate(best_models.items()):\n",
        "    model.save(f'/content/drive/MyDrive/best_model_bbbp{name}.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DIvTuGoETnp"
      },
      "source": [
        "load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9QKaH5dESs-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "best_models = []\n",
        "for cnt in range(5):\n",
        "  best_models.append(load_model('/content/drive/MyDrive/best_model_bbbpdf_' + str(cnt) + '.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LqXD5WXxTc"
      },
      "source": [
        "Evaluating the performances of the best models using ROC\n",
        "fare file log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EVM5KRu---_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open a file to write the results, modify path if you are in local\n",
        "with open(\"/content/drive/MyDrive/metrics_results_bbbp_models.txt\", \"w\") as file:\n",
        "\n",
        "    # Loop through best models\n",
        "    for idx, model in enumerate(best_models):\n",
        "        file.write(f\"\\nModel {idx + 1}\\n\")\n",
        "\n",
        "        df = dfs[idx]\n",
        "        X_emb = pad_sequences(df['emb'], dtype='float32', padding='post')\n",
        "        y = np.array(df['label'])\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_emb_train, X_emb_test, y_train, y_test = train_test_split(X_emb, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        # Pad sequences if necessary\n",
        "        X_emb_train = pad_sequences(X_emb_train, dtype='float32', padding='post')\n",
        "        X_emb_test = pad_sequences(X_emb_test, dtype='float32', padding='post')\n",
        "\n",
        "        # Reshape labels if necessary\n",
        "        y_train_reshaped = y_train.reshape(-1, 1)\n",
        "        y_test_reshaped = y_test.reshape(-1, 1)\n",
        "\n",
        "        # predict probabilities for test set\n",
        "        yhat_probs = model.predict(X_emb_test, verbose=0)\n",
        "        # predict crisp classes for test set\n",
        "        y_classes = np.argmax(yhat_probs, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_classes)\n",
        "        precision = precision_score(y_test, y_classes, labels=[1] , average = 'weighted')\n",
        "        recall = recall_score(y_test, y_classes, labels=[1], average = 'weighted')\n",
        "        f1 = f1_score(y_test, y_classes, labels=[1], average = 'weighted')\n",
        "        conf_matrix = confusion_matrix(y_test, y_classes)\n",
        "\n",
        "        file.write(\"Accuracy: {}\\n\".format(accuracy))\n",
        "        file.write(\"Precision: {}\\n\".format(precision))\n",
        "        file.write(\"Recall: {}\\n\".format(recall))\n",
        "        file.write(\"F1 Score: {}\\n\".format(f1))\n",
        "\n",
        "        # Calculate ROC AUC\n",
        "        roc_auc = roc_auc_score(y_test, y_classes)\n",
        "        file.write(\"ROC AUC: {}\\n\".format(roc_auc))\n",
        "\n",
        "        # Plot ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_classes)\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC)')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig(\"ROC_Model{}.png\".format(idx + 1))  # Save ROC curve plot\n",
        "        plt.close()\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plot_confusion_matrix(y_test, y_classes, class_names=['Class 0', 'Class 1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOsT4DIMB35f"
      },
      "source": [
        "Integrated Gradients - on the classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb4mR4N3mhhi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def integrated_gradients(input_data, model):\n",
        "    \"\"\"\n",
        "    Calculate integrated gradients for a given input_data and model\n",
        "    \"\"\"\n",
        "    # Define the baseline as all zeros\n",
        "    baseline = np.zeros_like(input_data)\n",
        "\n",
        "    # Create a linear interpolation path from baseline to the actual input\n",
        "    steps = 50\n",
        "    interpolated_points = [baseline + (i/steps) * (input_data - baseline) for i in range(steps+1)]\n",
        "\n",
        "    # Convert to numpy array\n",
        "    interpolated_points = np.array(interpolated_points)\n",
        "\n",
        "    # Convert to TensorFlow tensor\n",
        "    interpolated_points = tf.convert_to_tensor(interpolated_points, dtype=tf.float32)\n",
        "\n",
        "    # Get model predictions for the interpolated points\n",
        "    preds = model.predict(interpolated_points)\n",
        "\n",
        "    # Create a GradientTape to record operations for automatic differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_points)\n",
        "        predictions = model(interpolated_points)\n",
        "\n",
        "    # Calculate gradients with respect to input\n",
        "    gradients = tape.gradient(predictions, interpolated_points)\n",
        "\n",
        "    # Define the integrated gradients\n",
        "    integrated_grads = np.mean(gradients.numpy(), axis=0) * (input_data - baseline)\n",
        "\n",
        "    # Sum along the path to approximate the integral\n",
        "    integrated_grads = np.sum(integrated_grads, axis=1)\n",
        "\n",
        "    return integrated_grads\n",
        "\n",
        "# Example usage\n",
        "index_to_explain = 4  # Choose an index to explain\n",
        "embedding_to_explain = X_emb[index_to_explain]\n",
        "integrated_grads_result = integrated_gradients(embedding_to_explain, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1FqpJ0Q-5bR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an array of indices to use as x-axis\n",
        "indices = np.arange(len(integrated_grads_result))\n",
        "\n",
        "# Filter out non-zero values and their corresponding indices\n",
        "non_zero_indices = [i for i, val in enumerate(integrated_grads_result) if val != 0]\n",
        "non_zero_values = [val for val in integrated_grads_result if val != 0]\n",
        "\n",
        "# Plot non-zero integrated gradients\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(non_zero_indices, non_zero_values, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Integrated Gradients (Non-zero)')\n",
        "plt.title('Non-zero Integrated Gradients for Explaining Model Prediction')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SDNE"
      ],
      "metadata": {
        "id": "-Bs0fWiRsIlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset creation for SDNE Experiment"
      ],
      "metadata": {
        "id": "qzDLTttsTF5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch import nn, optim\n",
        "\n",
        "# Assuming 'all_data' is already defined and contains the graph data\n",
        "\n",
        "def create_networkx_graph(edge_index, edge_attr, x, y, num_nodes):\n",
        "    # Create an empty NetworkX graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with features\n",
        "    for i in range(num_nodes):\n",
        "        node_features = {f\"feat_{j}\": x[i, j].item() for j in range(x.shape[1])}\n",
        "        G.add_node(i, **node_features)\n",
        "\n",
        "    # Add edges with attributes\n",
        "    for j in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[:, j].tolist()\n",
        "        edge_attributes = {f\"attr_{k}\": edge_attr[j, k].item() for k in range(edge_attr.shape[1])}\n",
        "        G.add_edge(src, dst, **edge_attributes)\n",
        "\n",
        "    return G, y.item()\n",
        "\n",
        "class SDNE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, alpha=1e-5, beta=5):\n",
        "        super(SDNE, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # Encode\n",
        "        y = self.encoder(x)\n",
        "        # Decode\n",
        "        x_hat = self.decoder(y)\n",
        "        return x_hat, y\n",
        "\n",
        "    def loss_function(self, x, x_hat, y, adj):\n",
        "        # Reconstruction loss\n",
        "        mse = nn.MSELoss()\n",
        "        L_1st = mse(x_hat, x)\n",
        "\n",
        "        # Laplacian regularization\n",
        "        L_2nd = torch.sum(adj * torch.norm(y.unsqueeze(1) - y, dim=2))\n",
        "\n",
        "        return L_1st + self.alpha * L_1st + self.beta * L_2nd\n",
        "\n",
        "def train_sdne(graph, hidden_dims=[128, 64], epochs=100, lr=0.01):\n",
        "    # Create adjacency matrix\n",
        "    adj = nx.adjacency_matrix(graph).todense()\n",
        "    adj = torch.tensor(adj, dtype=torch.float32)\n",
        "\n",
        "    # Get node features\n",
        "    node_features = np.array([list(graph.nodes[i].values()) for i in range(graph.number_of_nodes())])\n",
        "\n",
        "    # Check if node features are empty\n",
        "    if node_features.shape[1] == 0:\n",
        "        raise ValueError(\"Node features are empty. Ensure nodes have features.\")\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    node_features = scaler.fit_transform(node_features)\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float32)\n",
        "\n",
        "    # Initialize model\n",
        "    input_dim = node_features.shape[1]\n",
        "    model = SDNE(input_dim, hidden_dims)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, y = model(node_features, adj)\n",
        "        loss = model.loss_function(node_features, x_hat, y, adj)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #if (epoch + 1) % 10 == 0:\n",
        "            #print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return model, y\n",
        "\n",
        "def balance_dataset(all_data, seed=42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    label_0_indices = [i for i, data in enumerate(all_data) if data.y.item() == 0]\n",
        "    label_1_indices = [i for i, data in enumerate(all_data) if data.y.item() == 1]\n",
        "\n",
        "    min_size_dataset = min(len(label_0_indices), len(label_1_indices))\n",
        "\n",
        "    random.shuffle(label_0_indices)\n",
        "    random.shuffle(label_1_indices)\n",
        "\n",
        "    label_0_indices = label_0_indices[:min_size_dataset]\n",
        "    label_1_indices = label_1_indices[:min_size_dataset]\n",
        "    print(len(label_0_indices))\n",
        "    print(len(label_1_indices))\n",
        "\n",
        "    balanced_indices = label_0_indices + label_1_indices\n",
        "    random.shuffle(balanced_indices)\n",
        "\n",
        "    balanced_data = [all_data[i] for i in balanced_indices]\n",
        "\n",
        "    return balanced_data\n",
        "\n",
        "# Generate and save embeddings for the balanced datasets\n",
        "if __name__ == \"__main__\":\n",
        "    all_graph_embeddings = []\n",
        "\n",
        "    for i in range(1):  # Loop to generate 5 different datasets\n",
        "        print(f'Generating dataset {i + 1}')\n",
        "        balanced_data = balance_dataset(all_data)\n",
        "\n",
        "        for data in balanced_data:\n",
        "            num_nodes = data.num_nodes\n",
        "            edge_index = data.edge_index\n",
        "            edge_attr = data.edge_attr\n",
        "            x = data.x\n",
        "            y = data.y\n",
        "\n",
        "            # Create graph\n",
        "            G, y_label = create_networkx_graph(edge_index, edge_attr, x, y, num_nodes)\n",
        "\n",
        "            # Train SDNE\n",
        "            model, embeddings = train_sdne(G, epochs=100)\n",
        "\n",
        "            # Collect node embeddings and aggregate them to obtain a graph-level embedding\n",
        "            embeddings_np = embeddings.detach().numpy()\n",
        "            graph_embedding = np.mean(embeddings_np, axis=0)  # Averaging node embeddings\n",
        "            all_graph_embeddings.append((graph_embedding.tolist(), y_label))\n",
        "\n",
        "    # Convert graph-level embeddings to DataFrame\n",
        "    df_graph_embeddings = pd.DataFrame(all_graph_embeddings, columns=[\"embedding\", \"label\"])\n",
        "\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(df_graph_embeddings)\n"
      ],
      "metadata": {
        "id": "xxLnsdjK-Y2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import ast\n",
        "\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    embeddings = []\n",
        "    labels = df['label'].tolist()\n",
        "\n",
        "    for emb in df['embedding']:\n",
        "        if isinstance(emb, str):\n",
        "            # Convert string representation of list back to list\n",
        "            emb = ast.literal_eval(emb)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    X = np.array(embeddings)\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "X, y = extract_features_labels(df_graph_embeddings)\n",
        "\n",
        "# Check if data is loaded correctly\n",
        "if X.shape[0] == 0 or X.shape[1] == 0:\n",
        "    raise ValueError(\"No valid embeddings found. Please check the DataFrame for formatting issues.\")\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Perform 5-fold cross-validation on the entire dataset\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "roc_aucs = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "histories = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f'Fold {fold_no}')\n",
        "\n",
        "    X_train_val, X_test = X[train_index], X[test_index]\n",
        "    y_train_val, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Perform an additional split to get a validation set from the training+validation set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val)\n",
        "\n",
        "    # Create and train the neural network\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = create_model(input_dim)\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f'Fold {fold_no} Validation Accuracy: {val_accuracy:.4f}')\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    histories.append(history.history)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Fold {fold_no} Test Accuracy: {test_accuracy:.4f}')\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Predict probabilities and calculate metrics for the test set\n",
        "    y_test_pred_proba = model.predict(X_test).flatten()\n",
        "    y_test_pred = (y_test_pred_proba > 0.5).astype(\"int32\")\n",
        "\n",
        "    roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
        "    precision = precision_score(y_test, y_test_pred)\n",
        "    recall = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f'Fold {fold_no} ROC-AUC: {roc_auc:.4f}')\n",
        "    print(f'Fold {fold_no} Precision: {precision:.4f}')\n",
        "    print(f'Fold {fold_no} Recall: {recall:.4f}')\n",
        "    print(f'Fold {fold_no} F1-Score: {f1:.4f}')\n",
        "\n",
        "    roc_aucs.append(roc_auc)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Plot confusion matrix for each fold\n",
        "    plot_confusion_matrix(y_test, y_test_pred, class_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "# Calculate and print the average validation accuracy and variance\n",
        "average_val_accuracy = np.mean(val_accuracies)\n",
        "variance_val_accuracy = np.var(val_accuracies)\n",
        "print(f'Average Validation Accuracy: {average_val_accuracy:.4f}')\n",
        "print(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}')\n",
        "\n",
        "# Calculate and print the average test accuracy and variance\n",
        "average_test_accuracy = np.mean(test_accuracies)\n",
        "variance_test_accuracy = np.var(test_accuracies)\n",
        "print(f'Average Test Accuracy: {average_test_accuracy:.4f}')\n",
        "print(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}')\n",
        "\n",
        "# Calculate and print the average ROC-AUC, Precision, Recall, and F1-Score\n",
        "average_roc_auc = np.mean(roc_aucs)\n",
        "average_precision = np.mean(precisions)\n",
        "average_recall = np.mean(recalls)\n",
        "average_f1_score = np.mean(f1_scores)\n",
        "\n",
        "print(f'Average ROC-AUC: {average_roc_auc:.4f}')\n",
        "print(f'Average Precision: {average_precision:.4f}')\n",
        "print(f'Average Recall: {average_recall:.4f}')\n",
        "print(f'Average F1-Score: {average_f1_score:.4f}')\n",
        "\n",
        "# Save the average validation accuracy, variance, test accuracy, and metrics to a file\n",
        "with open(\"model_accuracies.txt\", \"w\") as f:\n",
        "    f.write(f'Average Validation Accuracy: {average_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Average Test Accuracy: {average_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Average ROC-AUC: {average_roc_auc:.4f}\\n')\n",
        "    f.write(f'Average Precision: {average_precision:.4f}\\n')\n",
        "    f.write(f'Average Recall: {average_recall:.4f}\\n')\n",
        "    f.write(f'Average F1-Score: {average_f1_score:.4f}\\n')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Save training history for each fold (optional)\n",
        "with open(\"training_histories.txt\", \"w\") as f:\n",
        "    for i, history in enumerate(histories):\n",
        "        f.write(f'\\nFold {i + 1} History:\\n')\n",
        "        f.write(str(history))\n"
      ],
      "metadata": {
        "id": "EAqVMiFSQA_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HOPE"
      ],
      "metadata": {
        "id": "oNeNbrv3xjgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset creation for HOPE Experiment"
      ],
      "metadata": {
        "id": "a9AV-8VNTfS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "def create_networkx_graph(edge_index, edge_attr, x, y, num_nodes):\n",
        "    # Create an empty NetworkX graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with features\n",
        "    for i in range(num_nodes):\n",
        "        node_features = {f\"feat_{j}\": x[i, j].item() for j in range(x.shape[1])}\n",
        "        G.add_node(i, **node_features)\n",
        "\n",
        "    # Add edges with attributes\n",
        "    for j in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[:, j].tolist()\n",
        "        edge_attributes = {f\"attr_{k}\": edge_attr[j, k].item() for k in range(edge_attr.shape[1])}\n",
        "        G.add_edge(src, dst, **edge_attributes)\n",
        "\n",
        "    return G, y.item()\n",
        "\n",
        "def balance_dataset(all_data, seed=42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    label_0_indices = [i for i, data in enumerate(all_data) if data.y.item() == 0]\n",
        "    label_1_indices = [i for i, data in enumerate(all_data) if data.y.item() == 1]\n",
        "\n",
        "    min_size_dataset = min(len(label_0_indices), len(label_1_indices))\n",
        "\n",
        "    random.shuffle(label_0_indices)\n",
        "    random.shuffle(label_1_indices)\n",
        "\n",
        "    label_0_indices = label_0_indices[:min_size_dataset]\n",
        "    label_1_indices = label_1_indices[:min_size_dataset]\n",
        "    print(len(label_0_indices))\n",
        "    print(len(label_1_indices))\n",
        "\n",
        "    balanced_indices = label_0_indices + label_1_indices\n",
        "    random.shuffle(balanced_indices)\n",
        "\n",
        "    balanced_data = [all_data[i] for i in balanced_indices]\n",
        "\n",
        "    return balanced_data\n",
        "\n",
        "def compute_hope_embedding(G, d=128, beta=0.01):\n",
        "    # Create adjacency matrix\n",
        "    A = nx.to_numpy_array(G)\n",
        "\n",
        "    # Compute the Katz similarity matrix\n",
        "    I = np.eye(A.shape[0])\n",
        "    S = np.linalg.inv(I - beta * A) - I\n",
        "\n",
        "    # Ensure k is valid for SVD\n",
        "    k = min(d // 2, min(S.shape) - 1)\n",
        "\n",
        "    if k <= 0:\n",
        "        # If k is invalid, return trivial embeddings\n",
        "        return np.zeros((S.shape[0], d))\n",
        "\n",
        "    # Compute SVD\n",
        "    U, s, Vt = svds(S, k=k)\n",
        "    S_sqrt = np.diag(np.sqrt(s))\n",
        "    X1 = np.dot(U, S_sqrt)\n",
        "    X2 = np.dot(Vt.T, S_sqrt)\n",
        "    X = np.concatenate((X1, X2), axis=1)\n",
        "\n",
        "    # Ensure the final embedding dimension matches d\n",
        "    if X.shape[1] < d:\n",
        "        X = np.pad(X, ((0, 0), (0, d - X.shape[1])), 'constant')\n",
        "    elif X.shape[1] > d:\n",
        "        X = X[:, :d]\n",
        "\n",
        "    return X\n",
        "\n",
        "# Generate and save embeddings for the balanced datasets\n",
        "if __name__ == \"__main__\":\n",
        "    all_graph_embeddings = []\n",
        "\n",
        "    for i in range(1):  # Loop to generate embeddings from 5 different balanced datasets\n",
        "        print(f'Generating embeddings for dataset {i + 1}')\n",
        "        balanced_data = balance_dataset(all_data)\n",
        "\n",
        "        for data in balanced_data:\n",
        "            num_nodes = data.num_nodes\n",
        "            edge_index = data.edge_index\n",
        "            edge_attr = data.edge_attr\n",
        "            x = data.x\n",
        "            y = data.y\n",
        "\n",
        "            # Create graph\n",
        "            G, y_label = create_networkx_graph(edge_index, edge_attr, x, y, num_nodes)\n",
        "\n",
        "            # Compute HOPE embeddings\n",
        "            embeddings = compute_hope_embedding(G, d=128, beta=0.01)\n",
        "\n",
        "            # Collect node embeddings and aggregate them to obtain a graph-level embedding\n",
        "            graph_embedding = np.mean(embeddings, axis=0)  # Averaging node embeddings\n",
        "            all_graph_embeddings.append((graph_embedding.tolist(), y_label))\n",
        "\n",
        "    # Convert graph-level embeddings to DataFrame\n",
        "    df_graph_embeddings = pd.DataFrame(all_graph_embeddings, columns=[\"embedding\", \"label\"])\n",
        "\n",
        "    # Save the DataFrame to a CSV file (optional)\n",
        "    df_graph_embeddings.to_csv(\"graph_embeddings.csv\", index=False)\n",
        "\n",
        "    # Print the DataFrame\n",
        "    print(df_graph_embeddings)\n"
      ],
      "metadata": {
        "id": "oEJnUWgaxksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import ast\n",
        "\n",
        "\n",
        "# Extract features and labels\n",
        "def extract_features_labels(df):\n",
        "    embeddings = []\n",
        "    labels = df['label'].tolist()\n",
        "\n",
        "    for emb in df['embedding']:\n",
        "        if isinstance(emb, str):\n",
        "            # Convert string representation of list back to list\n",
        "            emb = ast.literal_eval(emb)\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    X = np.array(embeddings)\n",
        "    y = np.array(labels)\n",
        "    return X, y\n",
        "\n",
        "# Assuming df_graph_embeddings is already defined and loaded\n",
        "X, y = extract_features_labels(df_graph_embeddings)\n",
        "\n",
        "# Check if data is loaded correctly\n",
        "if X.shape[0] == 0 or X.shape[1] == 0:\n",
        "    raise ValueError(\"No valid embeddings found. Please check the DataFrame for formatting issues.\")\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(input_dim):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Perform 5-fold cross-validation on the entire dataset\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_no = 1\n",
        "val_accuracies = []\n",
        "test_accuracies = []\n",
        "histories = []\n",
        "\n",
        "roc_aucs = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(f'Fold {fold_no}')\n",
        "\n",
        "    X_train_val, X_test = X[train_index], X[test_index]\n",
        "    y_train_val, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Perform an additional split to get a validation set from the training+validation set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val)\n",
        "\n",
        "    # Create and train the neural network\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = create_model(input_dim)\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f'Fold {fold_no} Validation Accuracy: {val_accuracy:.4f}')\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    histories.append(history.history)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Fold {fold_no} Test Accuracy: {test_accuracy:.4f}')\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred_proba = model.predict(X_test).flatten()\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    # Compute and store metrics\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    roc_aucs.append(roc_auc)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Plot confusion matrix for each fold\n",
        "    plot_confusion_matrix(y_test, y_test_pred, class_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    print(f'Fold {fold_no} ROC AUC: {roc_auc:.4f}')\n",
        "    print(f'Fold {fold_no} Precision: {precision:.4f}')\n",
        "    print(f'Fold {fold_no} Recall: {recall:.4f}')\n",
        "    print(f'Fold {fold_no} F1-Score: {f1:.4f}')\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "# Calculate and print the average metrics\n",
        "average_val_accuracy = np.mean(val_accuracies)\n",
        "variance_val_accuracy = np.var(val_accuracies)\n",
        "print(f'Average Validation Accuracy: {average_val_accuracy:.4f}')\n",
        "print(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}')\n",
        "\n",
        "average_test_accuracy = np.mean(test_accuracies)\n",
        "variance_test_accuracy = np.var(test_accuracies)\n",
        "print(f'Average Test Accuracy: {average_test_accuracy:.4f}')\n",
        "print(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}')\n",
        "\n",
        "average_roc_auc = np.mean(roc_aucs)\n",
        "average_precision = np.mean(precisions)\n",
        "average_recall = np.mean(recalls)\n",
        "average_f1 = np.mean(f1_scores)\n",
        "\n",
        "print(f'Average ROC AUC: {average_roc_auc:.4f}')\n",
        "print(f'Average Precision: {average_precision:.4f}')\n",
        "print(f'Average Recall: {average_recall:.4f}')\n",
        "print(f'Average F1-Score: {average_f1:.4f}')\n",
        "\n",
        "# Save the average metrics to a file\n",
        "with open(\"model_accuracies.txt\", \"w\") as f:\n",
        "    f.write(f'Average Validation Accuracy: {average_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Validation Accuracy: {variance_val_accuracy:.4f}\\n')\n",
        "    f.write(f'Average Test Accuracy: {average_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Variance of Test Accuracy: {variance_test_accuracy:.4f}\\n')\n",
        "    f.write(f'Average ROC AUC: {average_roc_auc:.4f}\\n')\n",
        "    f.write(f'Average Precision: {average_precision:.4f}\\n')\n",
        "    f.write(f'Average Recall: {average_recall:.4f}\\n')\n",
        "    f.write(f'Average F1-Score: {average_f1:.4f}\\n')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Save training history for each fold (optional)\n",
        "with open(\"training_histories.txt\", \"w\") as f:\n",
        "    for i, history in enumerate(histories):\n",
        "        f.write(f'\\nFold {i + 1} History:\\n')\n",
        "        f.write(str(history))\n"
      ],
      "metadata": {
        "id": "B6Zb2A9Q1QJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}